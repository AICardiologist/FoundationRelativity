\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[hidelinks]{hyperref}
\usepackage{parskip}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows.meta,calc}
\usepackage{enumitem}
\usepackage{xcolor}

\setcounter{secnumdepth}{3}

% Standard macros
\newcommand{\BISH}{\mathrm{BISH}}
\newcommand{\LPO}{\mathrm{LPO}}
\newcommand{\WLPO}{\mathrm{WLPO}}
\newcommand{\LLPO}{\mathrm{LLPO}}
\newcommand{\BMC}{\mathrm{BMC}}
\newcommand{\MP}{\mathrm{MP}}
\newcommand{\FT}{\mathrm{FT}}
\newcommand{\CC}{\mathrm{CC}}
\newcommand{\DC}{\mathrm{DC}_\omega}
\newcommand{\CRM}{\mathrm{CRM}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\Lean}{\textsc{Lean~4}}
\newcommand{\Mathlib}{\textsc{Mathlib4}}

\title{\textbf{Bishop's Unfinished Revolution:\\[4pt]
Constructive Reverse Mathematics, Human Perseverance, and AI-Assisted Discovery}\\[8pt]
\large Paper~29 in the Constructive Reverse Mathematics and Physics Series}
\author{Paul Chun-Kit Lee\thanks{New York University. \texttt{dr.paul.c.lee@gmail.com}.
This paper documents work conducted using AI systems (primarily Anthropic's Claude) as collaborative tools; see \S\ref{sec:methodology} for methodology. Throughout this paper, ``we'' refers to the human--AI collaborative unit; ``I'' refers to the human researcher alone.}}
\date{February 2026}

\begin{document}
\maketitle

% ====================================================================
\begin{abstract}
In 1967, Errett Bishop launched a revolution in constructive mathematics. The mainstream responded with hostility, then silence. Sixty years later, constructive reverse mathematics~--- the program Bishop's work made possible~--- remains marginalized in American mathematics, with no institutional home in the United States. This paper tells the story of that marginalization, of the mathematicians who carried the revolution forward despite it, and of one outsider who entered the struggle~--- fascinated not by the theorems alone but by the human perseverance behind them. The question that emerged~--- what logical principles does physics actually require?~--- is Bishop's question applied to mathematical physics, inherited marginalization and all. The path forward required learning unfamiliar mathematics, enduring years of skepticism and silence, and sustaining faith in a vision of scientific beauty that made no institutional sense. AI did not supply the vision, the persistence, or the willingness to be laughed at. What AI could do~--- eventually, after generations of improvement~--- was execute: iterating against the \Lean{} proof assistant fast enough to test ideas in hours rather than months, and serving as an interlocutor that gradually learned to argue back. The result is 28 papers calibrating approximately fifty physical propositions against the constructive omniscience hierarchy across eleven domains of mathematical physics, formalized and mechanically type-checked in over 23,000 lines of \Lean{} code. These results have not been validated by domain experts in constructive mathematics or mathematical physics; we regard them as preliminary and expect that specialists will identify errors, alternative interpretations, and gaps we have missed. Notably, post hoc analysis of filesystem timestamps reveals that the AI's formalization \emph{quality}~--- measured by axiom audit thoroughness~--- improved measurably over the sprint, even within a single model generation: the AI learned the program's methodological norms in real time. But the experience suggests that the decisive ingredients in foundational research at disciplinary intersections remain irreducibly human~--- curiosity, conviction, and the refusal to stop~--- even when the tools that make execution possible are not.
\end{abstract}

% ====================================================================
\section{Bishop's Unfinished Revolution}
\label{sec:revolution}

Constructive mathematics has a history of punishing its champions. In 1928, Hilbert had Brouwer expelled from the editorial board of \emph{Mathematische Annalen}; Brouwer abandoned his intuitionist program and withdrew into silence \cite{vanDalen2005}. The most creative decade of constructive mathematics was ended not by mathematical refutation but by political action.

Forty years later, Errett Bishop arrived and the cycle repeated. His 1967 \emph{Foundations of Constructive Analysis} \cite{Bishop1967} reconstructed substantial portions of real analysis, measure theory, and functional analysis without the law of excluded middle or the axiom of choice~--- proving that constructive mathematics was not a philosophical curiosity but a working mathematics. The reaction was hostile. Colleagues dismissed the program as philosophy masquerading as mathematics; Bishop pushed back hard, diagnosing a ``debasement of meaning'' in classical existence proofs. According to Bridges \cite{Bridges2009}, the hostility eventually caused Bishop to become ``almost completely withdrawn from mathematics.'' He died in 1983, at fifty-four, having planted seeds that would not flower for decades.

Others carried the revolution forward. Bridges and Richman \cite{BridgesRichman1987} systematized Bishop's framework; Ishihara \cite{Ishihara1992,Ishihara2006} developed constructive reverse mathematics~--- the systematic calibration of logical principles against mathematical theorems, asking not ``is this theorem true?'' but ``what logical principle does this theorem cost?'' Bauer's ``Five Stages of Accepting Constructive Mathematics'' \cite{Bauer2017}, published in the \emph{Bulletin of the AMS} and awarded the 2022 Conant Prize, brought this tradition to a mainstream audience~--- yet produced no institutional shift. No US mathematics department created a position in constructive mathematics; no graduate program added CRM to its curriculum.

Constructive reverse mathematics remains marginalized in American mathematics today. It has a small, dedicated community~--- concentrated in New Zealand, Japan, and parts of Europe~--- and the 2023 \emph{Handbook of Constructive Mathematics} \cite{BridgesIshihara2023} testifies to the program's depth and continued vitality. But it has no institutional home in the United States. Bishop's revolution is unfinished not because it failed, but because the establishment decided it was not worth finishing.

% ====================================================================
\section{How I Got Here}
\label{sec:intro}

I am a cardiologist, not a mathematician. As medicine became increasingly bureaucratic, I found myself reading about scientists~--- not their theorems but their lives. Einstein in the patent office, Noether denied a salary, Boltzmann defending atomism until it broke him. I saw human struggles that mirrored my own, and the formulas stopped being abstract~--- each one a scar or a triumph of somebody's working life.

Through Bauer's ``Five Stages'' \cite{Bauer2017} I found Bishop, and his story stopped me cold. Bishop's \emph{Foundations} became my real textbook. The question that emerged~--- what logical principles does physics actually require?~--- is Bishop's question applied to mathematical physics. It requires the constructive omniscience hierarchy\footnote{The hierarchy, from weakest to strongest: $\BISH$ (no omniscience~--- purely constructive); $\LLPO$ (given a binary sequence with at most one~1, decide which half it is in); $\WLPO$ (decide whether a binary sequence is identically zero); $\LPO$ (decide whether a binary sequence contains a~1). Each principle is strictly stronger than the previous. Physical theorems are ``calibrated'' by identifying the weakest principle sufficient to prove them.} ($\BISH$, $\LLPO$, $\WLPO$, $\LPO$, and related principles), mathematical physics, and formal verification simultaneously. It belongs to no established discipline. It inherits Bishop's marginalization.

Once the calibration approach worked, it was like finding a key that opened door after door. The Ising model, the uncertainty principle, Bell's theorem, Noether's conservation laws~--- each revealed logical structure that the physics literature takes for granted but that constructive analysis makes precise. I could not stop. The lesson I had absorbed from the scientists I admired~--- Bishop, Boltzmann, Noether~--- was that you persevere. AI cannot do that. AI cannot sustain a years-long pursuit of a question that makes no institutional sense. That part is irreducibly human.

What AI could do was improve. The question came first, years before any AI tool could help. Early chatbots hallucinated badly; I pushed through, learning \Lean{} the hard way, receiving blunt feedback on the Zulip forum for beginner mistakes. Good scientists in past eras endured worse. And then, quite suddenly, everything clicked: the AI could write \Lean{} that compiled, the compiler could verify what I had designed on paper, and the conceptual framework could be populated in hours rather than months. The final month's burst of approximately 14 papers reflects not a sudden lowering of standards but the convergence of a mature conceptual framework~--- dozens of pre-designed calibrations awaiting formalization~--- and an AI system (Opus~4.6) whose \Lean{} capability had finally crossed the threshold.

Community engagement yielded mixed results. A clinician claiming novel results in constructive reverse mathematics does not fit established credibility patterns. But two insights from Lean Zulip participants proved important: the recommendation to audit constructive axiom usage rigorously (which led to the \texttt{\#print axioms} certification methodology), and the suggestion to write human-readable proofs alongside the \Lean{} code. My motivation was curiosity, not career~--- no grants, no tenure case, no publication targets. The \Lean{} compiler does not evaluate credentials; it evaluates proofs. That became the program's claim to credibility. The tools Bishop never had~--- a proof assistant that mechanically certifies constructive content, and AI systems that can execute formalizations at scale~--- now exist.

% ====================================================================
\section{The Scientific Question}
\label{sec:science}

The program's driving question~--- what is the role of logic in physical reality?~--- is not new. Hilbert's sixth problem (1900) asked for the axiomatization of physics \cite{Hilbert1900}. Quantum logic \cite{BirkhoffvonNeumann1936} proposed that quantum mechanics requires a non-classical logical structure. The topos-theoretic approach to physics \cite{DoringIsham2008} attempted to reformulate physical theories within non-classical logical frameworks. Each of these programs proposed a logical structure and asked physics to adopt it.

The present program takes a different approach. Rather than proposing a new logic for physics, it takes standard physics as given and asks: where does classical logic actually enter? The \Lean{} proof assistant enforces this discipline mechanically. A constructive proof compiles without classical axioms; a classical proof triggers the appearance of \texttt{Classical.choice} or equivalent in the \texttt{\#print axioms} certificate. By formalizing standard results in mathematical physics and examining which logical principles appear in their proofs, the program maps the \emph{logical stratigraphy} of physical theories.

This approach takes seriously Tao's argument \cite{Tao2025} that AI tools can legitimately assist mathematical research provided results are independently verified~--- the \Lean{} compiler serves as exactly that independent verifier. But the program also highlights an \emph{interdisciplinary barrier} that Tao's framework does not address: the question belongs to no established field. It requires constructive mathematics, mathematical physics, and formal verification simultaneously. The human researcher must see the connection~--- must have the taste to recognize that the thermodynamic limit and the geodesic incompleteness theorem share a logical spine. AI cannot supply that vision. But once the vision exists, AI systems are useful precisely because they carry vocabulary from all three fields simultaneously, and can be directed to articulate connections that the human has intuited but not yet formalized.

The method is best understood through an example. Consider the bidual space of a Banach space~--- a standard construction in functional analysis that physicists use routinely without examining its logical content. Over constructive logic, the existence of a non-reflexive Banach space~--- a gap between a space and its bidual~--- turns out to be equivalent to $\WLPO$, the Weak Limited Principle of Omniscience \cite{Lee2026a}. This is not an artificial construction; it reflects a genuine logical cost hidden in a mathematical object that analysts have used for over a century. To make the methodology concrete, here is the actual \Lean{} formalization of this result:

\begin{quote}
\small\ttfamily
\noindent\textrm{--\,\textit{Definitions (Paper~2, Basic.lean)}}\\
\textbf{def} WLPO : Prop :=\\
\quad $\forall\;(\alpha : \NN \to \mathrm{Bool}),\;(\forall\, n,\;\alpha\, n = \mathrm{false}) \;\lor\; \lnot\,(\forall\, n,\;\alpha\, n = \mathrm{false})$\\[4pt]
\textbf{def} BidualGapStrong : Prop :=\\
\quad $\exists\;(X : \mathrm{Type})\;[\mathrm{NormedAddCommGroup}\;X]\;[\mathrm{NormedSpace}\;\RR\;X]\;[\mathrm{CompleteSpace}\;X],$\\
\quad\quad $\mathrm{DualIsBanach}\;X \;\land\; \lnot\;\mathrm{Surjective}\;(\mathrm{inclusionInDoubleDual}\;\RR\;X)$\\[4pt]
\textrm{--\,\textit{Main equivalence (WLPO\_to\_Gap\_HB.lean)}}\\
\textbf{theorem} gap\_equiv\_wlpo : BidualGapStrong $\leftrightarrow$ WLPO\\[4pt]
\textrm{--\,\textit{Axiom certificate}}\\
\textrm{--\,\texttt{\#print axioms} gap\_equiv\_wlpo}\\
\textrm{--\,[propext, Classical.choice, Quot.sound]}
\end{quote}

\noindent The theorem states that the existence of a non-reflexive Banach space~--- a gap between a space and its bidual~--- is logically equivalent to $\WLPO$, the Weak Limited Principle of Omniscience. The axiom certificate shows three axioms: \texttt{propext} and \texttt{Quot.sound} are structural axioms present in all \Lean{} proofs; \texttt{Classical.choice} enters through \Mathlib{}'s construction of $\RR$, not through the proof's logical content. This is the pattern throughout the program: the compiler certifies what the proof uses; the three-tier methodology (\S\ref{sec:classical}) determines whether the classical axiom is genuine or inherited.

The program began here and extended the same methodology across eleven domains of physics. The specific findings~--- observable-dependent cost, structural identities invisible to informal methods, a recurring $\LPO$ equivalence governing completed limits in five independent domains~--- are developed fully in the companion papers \cite{Lee2026_P10,Lee2026_P12}. The episodes in \S\ref{sec:episodes} illustrate how these findings emerged through the collaboration.

% ====================================================================
\section{The \Lean{} Proof Assistant as Scientific Instrument}
\label{sec:methodology}

\Lean{} is not merely a proof checker. In this program, it served as a scientific instrument~--- the tool that turned subjective claims about constructive content into objective, machine-checkable certificates. Every theorem compiles. The \texttt{\#print axioms} command certifies which logical principles each proof employs. These are guarantees that do not depend on trust in either the human or the AI. The compiler does not evaluate credentials, institutional affiliation, or publication record. It evaluates proofs. For a cardiologist claiming novel results in constructive reverse mathematics, this was not a convenience~--- it was the difference between making claims and producing evidence.

\Lean{} also served as an honesty enforcer. When either party made an incorrect mathematical claim, the attempt to formalize it would fail, forcing correction. Several results changed character during formalization: claims that seemed obvious on paper required additional axioms, while results expected to require strong principles turned out to be constructively provable. These surprises~--- revealed by the compiler, not by either collaborator~--- are among the program's most informative findings. AI-generated proofs, in particular, could be \emph{plausibly wrong}~--- correct-sounding lemma names, reasonable structure, sensible intermediate steps, but containing subtle errors that would survive informal review. \Lean{} caught them immediately. Without this independent guarantee, a program of this volume would be epistemically suspect regardless of who produced it.

Bishop never had this tool. He conducted constructive reverse mathematics with pencil and paper, establishing the constructive content of his results by force of argument alone. His colleagues were not equipped to verify those arguments~--- and many refused to engage with them at all. \Lean{} mechanizes what Bishop had to do by sheer intellectual force: it certifies that a proof uses only the logical principles claimed, and it does so without requiring the reviewer to understand or even accept the constructive framework. This is the tool that turns constructive reverse mathematics from a philosophical stance into an experimental science. Without \Lean{}, this program would be a collection of claims about logical cost. With \Lean{}, it is a collection of certificates.

\label{sec:classical}
One methodological caveat requires frank acknowledgment. \Mathlib{}, the mathematical library for \Lean{}, imports \texttt{Classical.choice} at the library level, so \texttt{\#print axioms} shows classical axioms for \emph{all} theorems involving $\RR$. This means the axiom checker cannot, by itself, distinguish genuine classical content from inherited library infrastructure. This is standard practice in constructive reverse mathematics~--- Bridges and Richman \cite{BridgesRichman1987}, Simpson \cite{Simpson2009}, and Ishihara \cite{Ishihara2006} all worked within classical metatheories. The program addresses this through three certification levels, detailed in Paper~10 \cite{Lee2026_P10}. \emph{Level~1 (mechanically certified)}: \texttt{\#print axioms} shows no \texttt{Classical.choice}~--- rare for \Mathlib{}-based proofs but achievable for some pure $\BISH$ results operating over $\NN$ or finite types. \emph{Level~2 (structurally verified)}: \texttt{Classical.choice} appears but enters only through \Mathlib{} infrastructure ($\RR$ as a Cauchy completion, decidable instances on finite sets); the proof's logical structure uses no case splits on undecidable propositions, and human--AI audit confirms the classical axiom is eliminable in principle. This is the most common certification level in the program. \emph{Level~3 (intentionally classical)}: the proof genuinely requires classical logic~--- for instance, the reverse direction of a $\BMC \equiv \LPO$ equivalence, where the classical content is the \emph{finding}, not contamination. A fully constructive \Lean{} library for CRM does not yet exist; building one is a major infrastructure project beyond this series' scope.

% ====================================================================
\section{Episodes of Collaborative Discovery and Debate}
\label{sec:episodes}

The collaboration's character is best illustrated not by its successes but by the debates that shaped the program's conclusions. In most cases, the human researcher drove the interpretive direction~--- pushing back when the AI under-claimed or over-corrected, and supplying the taste for what matters. The AI's value was as a fast interlocutor who could be argued with, not as an oracle who saw what the human could not. We describe three representative episodes.

\subsection{The Noether theorem: from ``thin'' to ``universal template''}
\label{sec:noether}

The calibration of Noether's theorem (Paper~15) revealed that conservation laws have a biphasic logical structure: the local differential identity ($\partial_\mu J^\mu = 0$) is fully constructive ($\BISH$), while the global conserved charge ($Q = \int J^0 \, d^3x$ over unbounded domains) costs $\LPO$ when the integrand is sign-definite, via bounded monotone convergence \cite{Lee2026k}.

During the program's internal review, the AI initially assessed this result as ``thin''~--- merely adding another row to the calibration table without proving a tight equivalence. I pushed back, and the moment of recognition~--- when the AI saw that Noether's theorem was not just another row in the table but the template for \emph{every} conservation law in physics~--- was one of the program's most exciting. Energy, momentum, angular momentum, and electric charge all inherit the same logical split. Every conserved quantity in nature carries the biphasic structure. The AI had nearly talked us out of seeing it.

This episode illustrates both the value and the limitation of AI-assisted critical review. The AI's initial assessment was wrong, its reversal was correct, but the reversal required human pressure. A more reliable AI critic would have recognized the universality on first examination.

\subsection{Newton versus Lagrange: a genuine surprise}
\label{sec:newtonlagrange}

The calibration of classical mechanics (Paper~28) produced the kind of result that makes you set down your coffee and stare. Newton's equations (ODE formulation) are $\BISH$~--- solving a system of ordinary differential equations on a compact interval requires only constructive reasoning. But Lagrange's variational formulation~--- asserting that the physical trajectory minimizes the action functional~--- requires the Fan Theorem, because it involves optimization over a continuum of paths \cite{Lee2026x}.

Every physics student learns that Newton and Lagrange are equivalent. They are not. The compiler said so, and neither of us had predicted it. The \Lean{} formalization makes the stratification precise:

\begin{quote}
\small\ttfamily
\noindent\textrm{--\,\textit{Newton ($\BISH$): unique solution, no Fan Theorem}}\\
\textbf{theorem} el\_unique\_solution\_N2 (p : HOParams) (hcfl : p.k $<$ 8 $*$ p.m) :\\
\quad $\exists!\; q : \RR,\; (8 \cdot p.m - p.k) \cdot q = 4 \cdot p.m \cdot (p.A + p.B)$\\[4pt]
\textrm{--\,\textit{Lagrange ($\FT$): minimizer existence $\Leftrightarrow$ Fan Theorem}}\\
\textbf{theorem} minimizer\_iff\_ft :\\
\quad $(\forall\, f,\;\mathrm{ContinuousOn}\;f\;[0,1] \to \exists\, x \in [0,1],\;\forall\, y \in [0,1],\;f(x) \le f(y))$\\
\quad $\leftrightarrow\;\mathrm{FanTheorem}$
\end{quote}

\noindent Solving the equations of motion (Newton) is constructive; asserting that the physical trajectory \emph{minimizes} the action functional (Lagrange) requires the Fan Theorem, because it involves optimization over a continuum of paths. The \emph{equations} are the physical content; the \emph{optimization interpretation} is a framing whose existence claim carries additional logical cost. The AI noted that this challenges the widespread assumption of formulation equivalence and provides a logical explanation for why numerical ODE solvers (Newton) and variational integrators (Lagrange) have different computational characters.

The encoding construction was designed by me; the analysis of its implications was collaborative; and the shared recognition that we had found something genuinely unexpected~--- not just another confirmation of the pattern, but a result that rearranges how you think about classical mechanics~--- was one of the high points of the entire program.

\subsection{When to stop: the diminishing returns debate}
\label{sec:stop}

After Paper~23 completed the calibration of the Fan Theorem, the AI argued that the program had reached the natural end of its exploratory phase. Every major constructive principle had at least one physical instantiation. Additional calibrations would confirm the pattern without extending it. I had planned further papers and initially resisted this assessment.

The AI's argument was straightforward: the marginal return on new table entries was low. Each would say some variation of ``finite truncation is $\BISH$, completed limit is $\LPO$, exact threshold is $\LLPO$.'' The pattern was established; more instances would not change the story. The genuinely hard open problems~--- formulation-invariance as a metatheorem, constructive finite-dimensional quantum mechanics, QFT renormalization~--- were each years of work with uncertain payoff.

Stopping was hard. But the AI was right: the story was told. The pattern was established. What remained was not exploration but communication~--- the harder and less thrilling task of explaining what we had found. I accepted the assessment, and the program shifted from exploration to consolidation.

% ====================================================================
\section{Distribution of Intellectual Labor}
\label{sec:labor}

The episodes above illustrate the collaboration in action; this section summarizes the pattern.

\textbf{The lonely part: asking the question.} The program exists because a human asked a question that AI systems would not have generated independently~--- a question requiring disciplinary trespass across constructive mathematics, mathematical physics, and formal verification simultaneously. The architectural decisions (targeting the omniscience hierarchy, using \Lean{} and \Mathlib{}, selecting which theories to calibrate) and the creative encoding constructions (embedding binary sequences into physical parameters so that a property's truth value tracks a logical principle) were predominantly human throughout. None of the AI systems would have formulated this program unprompted.

\textbf{The collaborative part: developing the ideas.} Once a target was specified, the human directed the interpretive work and the AI helped articulate it. In every episode described in \S\ref{sec:episodes}, the key insight~--- the Noether universality, the Newton--Lagrange stratification, the decision to stop~--- was either originated or rescued by human judgment. The AI's role was to serve as a fast interlocutor: I would see a pattern; the AI would help me articulate it precisely and test it against the program's other results.

\textbf{The fast part: the compiler loop.} Over 23,000 lines of verified \Lean{} code were produced primarily through Claude Code. The iterative loop~--- write a proof term, get rejected with a type error, adjust, resubmit, repeat dozens of times, and then suddenly the green checkmark~--- completed in minutes what would have taken hours by hand~--- though the underlying mathematical methodology, developed over decades by the constructive mathematics community, was the essential prerequisite that no amount of AI speed could substitute for. In the program's final week, the human--AI pair averaged roughly one complete formalization per two hours of wall-clock time~--- an order-of-magnitude acceleration compared to the early papers, which each required weeks. The acceleration reflects three stacked effects: mechanical scaffolding reuse (project infrastructure copied verbatim), pattern instantiation (encoding constructions adapted from earlier calibrations), and~--- most notably~--- the AI's increasing mastery of the program's methodological standards.

\textbf{Quantifying the learning curve.} Filesystem timestamps for the 19~formalizations produced during the final sprint (Papers~8--28, all using Opus~4.6, February~6--11) permit a post hoc analysis of the collaboration's productivity and quality evolution. To control for template reuse, we computed \emph{effective novel lines}~--- total source lines minus verbatim-shared code inherited from earlier papers~--- for each formalization, and measured \emph{axiom audit density} (the number of \texttt{\#print axioms} calls per 100~source lines) as a proxy for methodological rigour. Table~\ref{tab:learning} summarises the results grouped into chronological thirds.

\begin{table}[ht]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
& \textbf{Early} & \textbf{Middle} & \textbf{Late} \\
& (Papers 8--16) & (Papers 17--23) & (Papers 24--28) \\
\midrule
Papers ($n$) & 8 & 6 & 5 \\
Total source lines & 7,361 & 5,770 & 5,064 \\
Effective novelty & 82\% & 87\% & 75\% \\
Median wall-clock (min) & 37 & 21 & 36 \\
Novel lines / min & 16.5 & 38.7 & 23.4 \\
Axiom audits / 100 lines & 1.2 & 2.5 & 2.3 \\
\bottomrule
\end{tabular}
\caption{Within-sprint learning metrics for 19~formalizations (all Opus~4.6, Feb~6--11, 2026). \emph{Effective novelty} = fraction of source lines not inherited verbatim from earlier papers. \emph{Novel lines/min} = effective novel lines divided by wall-clock span (first file creation to last modification). \emph{Axiom audits/100 lines} = \texttt{\#print axioms} calls per 100 source lines.}
\label{tab:learning}
\end{table}

Two findings emerge. First, raw coding speed did not improve monotonically~--- the middle third was fastest (38.7~novel lines/min) while the final third slowed (23.4), because later papers tackled harder formalizations (ergodic theory in Paper~25, variational mechanics in Paper~28). Productivity variance is dominated by problem difficulty, not cumulative experience ($R^2 < 0.01$ for effective speed vs.\ sequence number).

Second, \emph{formalization quality} did improve measurably. Axiom audit density~--- the number of \texttt{\#print axioms} certificates per 100~lines of source code~--- doubled from the early papers (1.2) to the middle and late papers (2.5 and 2.3, respectively; linear trend $R^2 = 0.40$; with $n = 19$ this is suggestive rather than conclusive, and replication across independent projects would be needed to establish the finding with conventional statistical confidence). The change is not merely quantitative. Paper~8's main module contains a single bare \texttt{\#print axioms} call with no annotation. By Paper~28, the \texttt{Stratification.lean} module includes a 15-line methodological note~--- embedded in the theorem's docstring~--- explaining \emph{why} \texttt{Classical.choice} appears and how the constructive stratification is established despite it. Paper~25 independently developed a 395-line Type-level workaround for the problem that Prop-level reverse directions in a classical proof assistant are trivially dischargeable via \texttt{Classical.choice}~--- a methodological innovation the human had not requested. The AI did not merely get faster; it learned what this program's central methodological concern \emph{is}, and began addressing it proactively.

\textbf{The honest part: learning to argue.} The AI developed the capacity to push back. The shift from sycophantic agreement (early models) to genuine pushback (later models) was one of the most significant changes across AI generations. The ``when to stop'' assessment (\S\ref{sec:stop}) is the clearest example~--- the AI providing judgment I accepted reluctantly, because it was right and I did not want it to be. AI-assisted critical review supplements but does not replace expert peer review.

\textbf{What the AI could not do.} The boundaries matter as much as the achievements. The AI could not originate the core question, could not evaluate whether a \Lean{} encoding faithfully represents the intended physics, and required human correction in multiple documented cases. A constructive mathematician or a physicist might find errors we missed~--- and I would welcome those objections, because the conversation matters more than being right.

% ====================================================================
\section{Evolution Across AI Generations}
\label{sec:evolution}

The program spanned five generations of Claude models. Each brought qualitative changes to the collaboration, summarized in Table~\ref{tab:generations}.

\begin{table}[ht]
\centering
\small
\begin{tabular}{@{}llllll@{}}
\toprule
\textbf{Generation} & \textbf{Period} & \textbf{Lean} & \textbf{Math} & \textbf{Critical} & \textbf{Mode} \\
\midrule
Claude 3.5 Sonnet & Mid-2024 & None & Textbook & Minimal & Directive \\
Late 3.5 / early Opus & Late 2024--Early 2025 & Basic & Working & Weak & Assisted \\
Opus 4 / Sonnet 4 & Mid-2025 & Productive & Collaborative & Emerging & Partnership \\
Sonnet 4.5 / Opus 4.5 & Late 2025 & Strong & Synthesizing & Substantive & Interpretive \\
Opus 4.6 & Early 2026 & Very strong & Coherent & Mature & Architectural \\
\bottomrule
\end{tabular}
\caption{AI capability by generation. ``Lean'' = \Lean{} formalization capability. ``Math'' = mathematical contribution level. ``Critical'' = capacity for honest critical evaluation. ``Mode'' = collaboration character.}
\label{tab:generations}
\end{table}

The arc described in \S\ref{sec:intro}~--- from early frustration through gradual improvement to the breakthrough~--- is reflected quantitatively in the table. The key transitions were: from no \Lean{} capability at all (mid-2024) to basic but slow formalization (early 2025) to rapid compiler-driven iteration at 10--20$\times$ speed (mid-2025) to a capable executor in early 2026. Throughout, the human supplied the vision, the architectural decisions, and the taste for what mattered; the AI supplied speed, vocabulary, and~--- increasingly~--- the capacity to push back when prompted. The shift from sycophantic agreement to useful friction was welcome, but it was friction in service of a human-directed program, not independent judgment. Importantly, the Opus~4.6 column in Table~\ref{tab:generations} is not static: within the final sprint, the AI's axiom audit density doubled and its documentation of methodological caveats matured qualitatively (Table~\ref{tab:learning}), suggesting that in-context learning\footnote{Each formalization session used Claude Code with access to the project's existing \texttt{.lean} files. The AI could read previously completed formalizations in the same repository, absorbing patterns, naming conventions, and methodological practices from earlier work. This is ``in-context learning'' in the broad sense~--- not single-session prompt learning, but cumulative norm absorption across sessions mediated by the shared codebase.} within a single model generation can be substantial when the codebase provides consistent norms to absorb.

% ====================================================================
\section{What the Program Found}
\label{sec:results}

The headline finding, across all 28~papers: every empirically accessible prediction~--- everything a finite laboratory can measure~--- is constructively derivable ($\BISH$). Non-constructive principles enter exclusively at idealization steps where physicists assert the existence of completed infinite quantities. This pattern holds without exception across eleven domains of mathematical physics and approximately fifty calibrated propositions.

Several findings deserve individual mention. The logical cost of $E = mc^2$ depends not on the algebra but on the noun: when ``$E$'' denotes the energy of a nuclear reaction (finite, bounded), the equation is $\BISH$; when it denotes the total energy of an infinite gravitational field, the existence claim costs $\LPO$. The classification is finer than system-level~--- it classifies \emph{questions}, not systems. The same logical equivalence ($\BMC \equiv \LPO$) governs completed limits in five independent domains. And CRM detects structural identities invisible to informal methods: Bell nonlocality and Kochen-Specker contextuality~--- physically unrelated no-go theorems treated separately for decades~--- turn out to share identical logical structure at the $\LLPO$ level, with the intermediate value theorem as the common mechanism \cite{Lee2026w}.

Whether this idealization--omniscience correlation reflects something about nature, about how humans build mathematical models, or both, remains open. The stronger metaphysical claim (``nature is constructive'') may be unresolvable by the methods here. The substantial empirical claim~--- that idealization is where classical logic enters, without exception~--- survived every test applied across 28~papers. The hypothesis is falsifiable: a single physical prediction requiring non-constructive logic would refute it. The full calibration table, the constructive hierarchy, and the interpretive framework are developed in the companion papers \cite{Lee2026_P10,Lee2026_P12}.

% ====================================================================
\section{Limitations and Open Questions}
\label{sec:limits}

We want to be honest about the boundaries. The compiler certifies that the proofs are correct; it does not certify that our encoding choices faithfully represent the physics. A constructive mathematician might find subtle errors; a physicist might argue we missed the real point. We do not claim that all fifty calibrations are novel to the CRM literature; some may formalize results known informally to specialists. The contribution is the systematic \Lean{}-verified survey across physics, not individual equivalences. This program exists because of the mathematical foundations built over decades by the constructive mathematics community~--- Bridges, Richman, Ishihara, Bauer, and others~--- whose rigour and persistence in the face of institutional indifference made the questions we ask here possible. Any errors in our application of their methods to physics are ours alone, and we would be grateful for correction. The \texttt{Classical.choice} methodology (\S\ref{sec:classical}) is our best approximation within current tooling~--- a fully constructive \Lean{} library would strengthen the claims. And AI-assisted critical review is not a substitute for expert peer review.

A word about volume. Twenty-eight papers in a month invites suspicion, and rightly so. The honest explanation is not speed but reuse: once the encoding framework was established~--- embedding binary sequences into physical parameters so that a theorem's truth value tracks an omniscience principle~--- each new domain required adapting the construction, not reinventing it. The questions piled up faster than we could resist them: if the pattern holds for the Ising model, does it hold for black holes? For Noether's theorem? For the Standard Model? We followed the thread because we were curious, not because we were racing. In hindsight, a more focused program~--- fewer domains, deeper analysis, more engagement with existing CRM results in each~--- would have been stronger. We chose breadth because the working hypothesis demanded testing across many domains, but we recognize that breadth came at the cost of depth, and we expect that specialists will find errors a more careful program would have avoided.

The program's core finding~--- that constructive logical cost correlates with physical idealization across every domain examined~--- is either a deep observation about the relationship between logic and physical reality, or an artifact of how humans build mathematical models, or both. The formal results are mechanically checked; their interpretation remains open. But I confess to a suspicion~--- unproven, perhaps unprovable~--- that the pattern is trying to tell us something. The \Lean{} code is on Zenodo. The claims are falsifiable: a single physical prediction whose empirical content requires non-constructive logic would refute the working hypothesis. We expect and encourage researchers~--- particularly in the Lean and CRM communities~--- to examine the results carefully and tell us what we got wrong. Their judgment, not ours, should be regarded as definitive.

This collaboration is itself an object of study. It is not the mode of research I would have chosen. It is the mode that was available, and it worked better than I had any right to expect. Whatever its limitations, it produced something neither party could have produced alone.

% ====================================================================
\section{Conclusion}
\label{sec:conclusion}

Twenty-eight papers, over 23,000 lines of verified \Lean{} code, approximately fifty calibrated propositions across eleven domains. The human provided the question, the vision, the taste for what mattered, the encoding constructions, and the stubborn refusal to stop when the world was silent. The AI provided speed~--- formalization capacity, vocabulary across fields, and increasingly useful friction when directed to critique. The \Lean{} compiler served as the independent referee that made the collaboration epistemically credible. None of this would have existed without the human conviction that the question was worth asking; none of it would have been executable at this scale without the AI.

We believe the question was worth asking and the answers were worth finding. Whether the correlation between constructive strength and physical idealization reflects something about nature, or about mathematics, or about the human impulse to build cathedrals over cellars, remains open. But the map is drawn, the compiler has certified it, and the territory it reveals is, we think, beautiful.

This program began as one person's stubborn curiosity and became, through an improbable partnership with a tool that did not exist two years ago, something that, if the community finds it sound, may contribute to the tradition Bishop began. Bishop pursued his constructive vision alone and paid a steep personal price; fifty years later, the tools he lacked finally exist, and the question he opened can be pressed further than he could have dreamed. If this program inspires even one reader to pick up Bishop's \emph{Foundations} and ask what their own field's theorems actually cost, the struggle will have been worth it.

% ====================================================================
\section*{Acknowledgments}

This program was conducted using Claude (Anthropic) across five model generations, with Claude Code as the primary \Lean{} formalization tool. Additional interactions with other AI systems (OpenAI, xAI, Google DeepMind) provided supplementary critique and alternative perspectives at various points.

I am grateful beyond what formal acknowledgments can express. To the developers of \Lean{} and \Mathlib{}, who built the instrument that made this work possible. To the Lean Zulip participants who offered constructive criticism when it would have been easier to ignore an outsider~--- their suggestions materially improved the program's methodology, and their engagement meant more than they know. To Errett Bishop, whose 1967 \emph{Foundations} planted the seed that grew, over many years, into this program. He showed that constructive mathematics was not merely a philosophical stance but a working mathematical practice, and he paid a professional price for that demonstration. If this program vindicates any part of his vision~--- that the constructive content of mathematics is worth extracting and examining~--- then it owes more to Bishop's lonely persistence than to any tool or technique developed since. And to the AI systems that served as patient, tireless, occasionally maddening interlocutors throughout~--- imperfect partners in an imperfect collaboration, but partners nonetheless.

The struggle was real. The joy was greater.

% ====================================================================
\begin{thebibliography}{99}

\bibitem{BirkhoffvonNeumann1936}
Birkhoff, G. and von Neumann, J. ``The logic of quantum mechanics.'' \emph{Annals of Mathematics} 37(4) (1936): 823--843.

\bibitem{Bauer2017}
Bauer, A. ``Five stages of accepting constructive mathematics.'' \emph{Bulletin of the American Mathematical Society} 54(3) (2017): 481--498.

\bibitem{Bishop1967}
Bishop, E. \emph{Foundations of Constructive Analysis}. McGraw-Hill, New York, 1967.

\bibitem{BridgesRichman1987}
Bridges, D. and Richman, F. \emph{Varieties of Constructive Mathematics}. Cambridge University Press, 1987.

\bibitem{Bridges2009}
Bridges, D. ``Errett Bishop.'' \emph{Personal memorial and mathematical tribute}. Available at \texttt{dsbridges.com/errett-bishop}, accessed 2026.

\bibitem{BridgesIshihara2023}
Bridges, D., Ishihara, H., Rathjen, M., and Schwichtenberg, H., eds. \emph{Handbook of Constructive Mathematics}. Encyclopedia of Mathematics and its Applications. Cambridge University Press, 2023.

\bibitem{Cubitt2015}
Cubitt, T.~S., Perez-Garcia, D., and Wolf, M.~M. ``Undecidability of the spectral gap.'' \emph{Nature} 528 (2015): 207--211.

\bibitem{deMoura2021}
de~Moura, L., Kong, S., Avigad, J., van~Doorn, F., and von~Raumer, J. ``The Lean~4 theorem prover and programming language.'' In \emph{CADE-28}, Lecture Notes in Computer Science 12699, pp.~625--635. Springer, 2021.

\bibitem{DoringIsham2008}
D\"oring, A. and Isham, C.~J. ```What is a thing?': Topos theory in the foundations of physics.'' In \emph{New Structures for Physics}, Lecture Notes in Physics 813, Springer, 2008.

\bibitem{Gisin2020}
Gisin, N. ``Mathematical languages shape our understanding of time in physics.'' \emph{Nature Physics} 16 (2020): 114--116.

\bibitem{Hilbert1900}
Hilbert, D. ``Mathematical problems.'' Lecture delivered before the International Congress of Mathematicians, Paris, 1900. English translation in \emph{Bulletin of the AMS} 8 (1902): 437--479.

\bibitem{Ishihara1992}
Ishihara, H. ``Continuity properties in constructive mathematics.'' \emph{Journal of Symbolic Logic} 57 (1992): 557--565.

\bibitem{Ishihara2006}
Ishihara, H. ``Reverse mathematics in Bishop's constructive mathematics.'' \emph{Philosophia Scientiae}, Cahier sp\'ecial 6 (2006): 43--59.

\bibitem{Simpson2009}
Simpson, S.~G. \emph{Subsystems of Second Order Arithmetic}. 2nd ed. Cambridge University Press, 2009.

\bibitem{Tao2025}
Tao, T. ``Machine assisted proof.'' \emph{Notices of the American Mathematical Society} 72(1) (2025): 11--25.

\bibitem{vanDalen2005}
van~Dalen, D. \emph{Mystic, Geometer, and Intuitionist: The Life of L.E.J.\ Brouwer}. Vol.~2: \emph{Hope and Disillusion}. Clarendon Press, Oxford, 2005.

% Program papers
\bibitem{Lee2026a}
Lee, P.~C.-K. ``Constructive reverse mathematics of the bidual gap: WLPO equivalence for Banach space non-reflexivity.'' 2026. Lean~4 formalization archived at Zenodo. DOI: 10.5281/zenodo.17107493. Paper~2.

\bibitem{Lee2026f}
Lee, P.~C.-K. ``Axiom calibration for quantum spectra.'' 2026. Lean~4 formalization archived at Zenodo. DOI: 10.5281/zenodo.17059483. Paper~4.

\bibitem{Lee2026_P5}
Lee, P.~C.-K. ``The event horizon as a logical boundary: Schwarzschild curvature and constructive calibration.'' 2026. Paper~5.

\bibitem{Lee2026g}
Lee, P.~C.-K. ``Constructive reverse mathematics for the Heisenberg uncertainty principle.'' 2026. Lean~4 formalization archived at Zenodo. DOI: 10.5281/zenodo.18519836. Paper~6.

\bibitem{Lee2026b}
Lee, P.~C.-K. ``The physical bidual gap: WLPO and non-reflexivity of trace-class operators.'' 2026. Lean~4 formalization archived at Zenodo. DOI: 10.5281/zenodo.18527559. Paper~7.

\bibitem{Lee2026c}
Lee, P.~C.-K. ``The constructive cost of the thermodynamic limit: LPO equivalence and BISH dispensability in the 1D Ising model.'' 2026. Lean~4 formalization archived at Zenodo. DOI: 10.5281/zenodo.18516813. Paper~8.

\bibitem{Lee2026e}
Lee, P.~C.-K. ``Formulation-invariance of the logical cost of the thermodynamic limit: A combinatorial proof for the 1D Ising model.'' 2026. Lean~4 formalization archived at Zenodo. DOI: 10.5281/zenodo.18517570. Paper~9.

\bibitem{Lee2026_P10}
Lee, P.~C.-K. ``The logical geography of mathematical physics: Constructive calibration from density matrices to the choice axis.'' 2026. Paper~10 (technical synthesis).

\bibitem{Lee2026h}
Lee, P.~C.-K. ``The constructive cost of quantum entanglement: Tsirelson bound and Bell state entropy in Lean~4.'' 2026. Lean~4 formalization archived at Zenodo. DOI: 10.5281/zenodo.18527676. Paper~11.

\bibitem{Lee2026_P12}
Lee, P.~C.-K. ``The map and the territory: A constructive history of mathematical physics.'' 2026. Paper~12 (historical companion).

\bibitem{Lee2026i}
Lee, P.~C.-K. ``The event horizon as a logical boundary.'' 2026. DOI: 10.5281/zenodo.18529007. Paper~13.

\bibitem{Lee2026j}
Lee, P.~C.-K. ``The measurement problem as a logical artefact.'' 2026. DOI: 10.5281/zenodo.18569068. Paper~14.

\bibitem{Lee2026k}
Lee, P.~C.-K. ``Noether's theorem and the logical cost of global conservation laws.'' 2026. DOI: 10.5281/zenodo.18572494. Paper~15.

\bibitem{Lee2026l}
Lee, P.~C.-K. ``The Born rule as a logical artefact.'' 2026. DOI: 10.5281/zenodo.18575377. Paper~16.

\bibitem{Lee2026m}
Lee, P.~C.-K. ``The Bekenstein-Hawking formula and the constructive cost of quantum gravity.'' 2026. DOI: 10.5281/zenodo.18597306. Paper~17.

\bibitem{Lee2026n}
Lee, P.~C.-K. ``The fermion mass hierarchy and the Scaffolding Principle.'' 2026. DOI: 10.5281/zenodo.18600243. Paper~18.

\bibitem{Lee2026o}
Lee, P.~C.-K. ``WKB tunneling and LLPO.'' 2026. DOI: 10.5281/zenodo.18602596. Paper~19.

\bibitem{Lee2026p}
Lee, P.~C.-K. ``Observable-dependent logical cost: 1D Ising magnetization and WLPO.'' 2026. DOI: 10.5281/zenodo.18603079. Paper~20.

\bibitem{Lee2026q}
Lee, P.~C.-K. ``Bell nonlocality and LLPO.'' 2026. DOI: 10.5281/zenodo.18603251. Paper~21.

\bibitem{Lee2026r}
Lee, P.~C.-K. ``Markov's Principle and radioactive decay.'' 2026. DOI: 10.5281/zenodo.18603503. Paper~22.

\bibitem{Lee2026s}
Lee, P.~C.-K. ``The Fan Theorem and the constructive cost of optimization.'' 2026. DOI: 10.5281/zenodo.18604312. Paper~23.

\bibitem{Lee2026t}
Lee, P.~C.-K. ``LLPO equivalence via Kochen-Specker contextuality.'' 2026. DOI: 10.5281/zenodo.18604317. Paper~24.

\bibitem{Lee2026u}
Lee, P.~C.-K. ``The choice axis in constructive reverse mathematics: calibrating ergodic theorems and laws of large numbers.'' 2026. DOI: 10.5281/zenodo.18615453. Paper~25.

\bibitem{Lee2026v}
Lee, P.~C.-K. ``Bidual gap detection is WLPO-complete: G\"odel sequences and explicit reductions.'' 2026. DOI: 10.5281/zenodo.18615457. Paper~26.

\bibitem{Lee2026w}
Lee, P.~C.-K. ``The logical cost of root-finding: LLPO, the IVT, and Bell angle optimization.'' 2026. DOI: 10.5281/zenodo.18615459. Paper~27.

\bibitem{Lee2026x}
Lee, P.~C.-K. ``Newton vs.\ Lagrange vs.\ Hamilton: constructive stratification of classical mechanics.'' 2026. DOI: 10.5281/zenodo.18616620. Paper~28.

\end{thebibliography}

\end{document}
