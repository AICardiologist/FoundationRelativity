% ==========================================================================
%  The Logical Constitution of Physical Reality
%  Constructive Reverse Mathematics of Mathematical Physics
%  Monograph — Papers 1–42
% ==========================================================================

\documentclass[11pt,a4paper]{report}

% ---- Page geometry ----
\usepackage[margin=1in]{geometry}

% ---- Fonts ----
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% ---- Math ----
\usepackage{amsmath,amssymb,mathtools}
\usepackage{amsthm}

% ---- Language ----
\usepackage[greek,american]{babel}

% ---- Symbols ----
\usepackage{stmaryrd}

% ---- Lists ----
\usepackage{enumitem}

% ---- Tables ----
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}

% ---- Graphics / Diagrams ----
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric,calc,
                 decorations.pathmorphing,decorations.pathreplacing,patterns,fit}
\usepackage{graphicx}

% ---- Listings (Lean 4) ----
\usepackage{listings}
\usepackage[x11names,table]{xcolor}

% ---- Boxes ----
\usepackage{mdframed}

% ---- Chapter summary box ----
\newmdenv[
  linewidth=1.2pt,
  linecolor=blue!40,
  backgroundcolor=blue!3,
  roundcorner=4pt,
  innertopmargin=8pt,
  innerbottommargin=8pt,
  innerleftmargin=10pt,
  innerrightmargin=10pt,
  skipabove=\bigskipamount,
  skipbelow=\medskipamount,
  frametitle={\textsc{Chapter Summary}},
  frametitlefont=\small\bfseries\sffamily,
  frametitlebackgroundcolor=blue!10,
  frametitleaboveskip=4pt,
  frametitlebelowskip=4pt,
]{chapterbox}

% ---- URLs ----
\usepackage{url}

% ---- Cross-references (hyperref before cleveref) ----
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{cleveref}

% ==========================================================================
%  Theorem environments
% ==========================================================================
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% ==========================================================================
%  Custom macros — omniscience principles & notation
% ==========================================================================
\newcommand{\BISH}{\mathrm{BISH}}
\newcommand{\LLPO}{\mathrm{LLPO}}
\newcommand{\WLPO}{\mathrm{WLPO}}
\newcommand{\LPO}{\mathrm{LPO}}
\newcommand{\BMC}{\mathrm{BMC}}
\newcommand{\FT}{\mathrm{FT}}
\newcommand{\DC}{\mathrm{DC}}
\newcommand{\CCax}{\mathrm{CC}}
\newcommand{\MP}{\mathrm{MP}}
\newcommand{\LEM}{\mathrm{LEM}}
\newcommand{\CRM}{\mathrm{CRM}}
\newcommand{\ZFC}{\mathrm{ZFC}}
\newcommand{\BI}{\mathrm{BI}}
\newcommand{\AC}{\mathrm{AC}}

% Number sets
\newcommand{\NN}{\mathbb{N}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\CC}{\mathbb{C}}

% Physics
\newcommand{\Hil}{\mathcal{H}}
\newcommand{\AdS}{\mathrm{AdS}}
\newcommand{\CFT}{\mathrm{CFT}}
\newcommand{\RT}{\mathrm{RT}}
\newcommand{\QES}{\mathrm{QES}}

% Tools
\newcommand{\Lean}{\textsc{Lean~4}}
\newcommand{\Mathlib}{\textsc{Mathlib4}}
\newcommand{\leanok}{\textsf{\small \textcolor{green!70!black}{\checkmark}}}
% \leanRepo removed

% Star ratings for significance guide
\newcommand{\FiveStar}{$\bigstar\bigstar\bigstar\bigstar\bigstar$}
\newcommand{\FourStar}{$\bigstar\bigstar\bigstar\bigstar$}
\newcommand{\ThreeStar}{$\bigstar\bigstar\bigstar$}
\newcommand{\TwoStar}{$\bigstar\bigstar$}
\newcommand{\OneStar}{$\bigstar$}

% ==========================================================================
%  Lean listing style
% ==========================================================================
\definecolor{lean-kw}{RGB}{0,0,180}
\definecolor{lean-cm}{RGB}{0,128,0}
\definecolor{lean-str}{RGB}{163,21,21}
\definecolor{lean-bg}{RGB}{248,248,248}

\lstdefinelanguage{lean4}{
  keywords={theorem, lemma, def, class, instance, import, open, variable,
            noncomputable, section, namespace, end, where, let, have, show,
            intro, obtain, use, exact, rw, simp, apply, by, fun, match, if,
            then, else, do, return, axiom, abbrev, sorry},
  sensitive=true,
  morecomment=[l]{--},
  morecomment=[s]{/-}{-/},
  morestring=[b]",
}

\lstset{
  language=lean4,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{lean-kw}\bfseries,
  commentstyle=\color{lean-cm}\itshape,
  stringstyle=\color{lean-str},
  backgroundcolor=\color{lean-bg},
  frame=single, framerule=0.5pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=2, showstringspaces=false,
  numbers=left, numberstyle=\tiny\color{gray}, numbersep=5pt,
  xleftmargin=15pt, captionpos=b,
}

% ==========================================================================
%  Title
% ==========================================================================
\title{%
  \textbf{The Logical Constitution of Physical Reality}\\[6pt]
  {\Large Constructive Reverse Mathematics of Mathematical Physics}\\[4pt]
  {\large A Monograph in 42 Papers}%
}

\author{%
  Paul Chun-Kit Lee\thanks{Individual paper DOIs on Zenodo.}\\
  NYU Langone Hospital---Brooklyn, New York University, New York
}

\date{February 2026}

% ==========================================================================
\begin{document}
% ==========================================================================

\maketitle

% ------------------------------------------------------------------
%  Dedication
% ------------------------------------------------------------------
\clearpage
\thispagestyle{empty}
\vspace*{\fill}
\begin{center}
\textit{To my wife Mimi}
\end{center}
\vspace*{\fill}

% ------------------------------------------------------------------
%  Epigraph
% ------------------------------------------------------------------
\clearpage
\thispagestyle{empty}
\vspace*{\fill}
\begin{center}
{\large
\foreignlanguage{greek}{%
>En >arq\~h| \~hn <o L'ogos,\\[4pt]
ka`i <o L'ogos \~hn pr`os t`on Je'on,\\[4pt]
ka`i Je`os \~hn <o L'ogos.%
}}
\end{center}
\vspace{1cm}
\begin{center}
\normalsize\textit{In the beginning was the Word,\\
and the Word was with God,\\
and the Word was God.}\\[6pt]
--- John 1:1
\end{center}
\vspace*{\fill}

% ------------------------------------------------------------------
%  Abstract
% ------------------------------------------------------------------
\begin{abstract}
We prove that the logical resources required for all empirical predictions in known physics are exactly $\BISH+\LPO$: Bishop's constructive mathematics augmented by the Limited Principle of Omniscience. This characterization is established by systematic axiom calibration across 42~papers spanning the Standard Model (QED, QCD, electroweak theory), general relativity, statistical mechanics, quantum information theory, the AdS/CFT correspondence, and the cosmological constant problem. All calibrations are formally verified in approximately 35{,}000 lines of \Lean\ proof code.

Three foundational results underpin the characterization. First, Fekete's Subadditive Lemma---the mathematical engine of phase transitions---is equivalent to $\LPO$ over $\BISH$, establishing that $\LPO$ is physically instantiated. Second, the Fan Theorem (compactness) is dispensable. Third, Dependent Choice is dispensable. A conservation metatheorem explains the pattern: empirical predictions are finite compositions of computable functions ($\BISH$), and the only idealizations exceeding finite computation are completed limits ($\LPO$ via the Bounded Monotone Convergence equivalence).

The undecidability arc completes the picture: every known physical undecidability result is Turing--Weihrauch equivalent to $\LPO$, traceable to a single ancestor (Wang tiling). A refined analysis reveals that generic intensive observables without promise gaps can reach $\Sigma^0_2$---but this Platonic tier is empirically inaccessible due to finite experimental precision. The diagnostic phase (Papers~41--42) applies the framework to the AdS/CFT correspondence and the cosmological constant problem, dissolving the $10^{120}$ discrepancy as a regulator artifact and identifying the holographic dictionary as an axiom-preserving map.

\medskip\noindent
\textbf{Keywords:} constructive mathematics, reverse mathematics, formal verification, foundations of physics, Bishop's constructive analysis, omniscience principles, \Lean
\end{abstract}

\tableofcontents
\listoffigures

% ======================================================================
%  PREFACE
% ======================================================================
\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface}

\section*{The Cost of Being Right Too Early}

The intellectual history of constructive mathematics is usually told as a sequence of ideas. But behind the ideas were human beings, and the personal cost of this program is a story rarely told outside the specialist community. It deserves telling, because it is part of what the formulas mean.

In 1907, L.\,E.\,J.\ Brouwer submitted his doctoral thesis \emph{Over de grondslagen der wiskunde}~\cite{Brouwer1907} to the University of Amsterdam and launched intuitionism---a radical program asserting that mathematics is a human mental construction, that infinite objects exist only through the processes that generate them, and that the Law of Excluded Middle ($\LEM$: for every proposition~$P$, either $P$ or $\neg P$) is not a valid principle of reasoning about infinite collections. Brouwer rejected completed infinities, non-constructive existence proofs, and the Axiom of Choice. His program was philosophically deep but mathematically constrained: by adopting principles that are \emph{classically false} (such as the continuity of all real-valued functions), Brouwer created a mathematics incompatible with the classical tradition. Most mathematicians rejected it---not because the philosophy was wrong, but because the mathematics was too alien to adopt. And Brouwer paid for his vision. His \emph{Grundlagenstreit} with David Hilbert---the foundational dispute over the legitimacy of non-constructive methods---escalated through the 1920s from a philosophical disagreement into a campaign of professional destruction. In~1928, Hilbert arranged to have Brouwer expelled from the editorial board of \emph{Mathematische Annalen}, then the most prestigious journal in mathematics. The board was dissolved and reconstituted without Brouwer; Einstein and Carath\'eodory declined reappointment in protest, but their protest changed nothing. Brouwer wrote to a confidant: ``All my life's work has been wrested from me and I am left in fear, shame, and mistrust, and suffering the persecution of my baiting torturers''~\cite{vanDalen2005}. The man who had seen more clearly than anyone that infinite reasoning requires justification spent his remaining decades in isolation---withdrawn from mathematical life, consumed by fears of persecution and financial ruin. He died in 1966, aged~85, struck by a car near his home in Blaricum. His insight into the foundations had been correct; the profession that rejected him never acknowledged this during his lifetime.

Sixty years after Brouwer's thesis, Errett Bishop published \emph{Foundations of Constructive Analysis}~\cite{Bishop1967} and demonstrated that Brouwer's philosophical concerns could be addressed without his non-classical axioms. Bishop's constructive mathematics ($\BISH$) simply \emph{declines} to use the Law of Excluded Middle and the Axiom of Choice without assuming their negations. Every theorem proved in $\BISH$ is automatically valid in classical mathematics, in intuitionism, and in recursive mathematics. Bishop showed that substantial portions of analysis---measure theory, Banach space theory, spectral theory---could be developed constructively, providing explicit algorithms wherever classical mathematics merely asserts existence. The promise was real: a foundation for mathematics in which every existence proof comes with a construction, every function is computable, and every proof provides an algorithm. But Bishop's fate was quieter than Brouwer's and no less painful. Abraham Robinson conceded that ``even those who are not willing to accept Bishop's basic philosophy must be impressed with the great analytical power displayed in his work.'' Bishop was invited to address the International Congress of Mathematicians in 1966 and to deliver the American Mathematical Society's Colloquium Lectures in 1973---four hour-long talks he titled \emph{Schizophrenia in Contemporary Mathematics}, a pointed diagnosis of what he saw as the profession's refusal to confront its own non-constructive habits. But these honors were exceptions. When Bishop presented constructive mathematics at departments across the United States, the reception was hostile. He encountered not disagreement but dismissal---the particular cruelty of being told that your life's work is not mathematics at all. By the late 1970s, Bishop had become almost completely withdrawn from mathematical life. He died of cancer in 1983, aged~54. His program was still dismissed by the mainstream. The recognition that he had been right would come decades later, and it would come too late for him.

The pattern of vision punished is not unique to constructive mathematics. Galileo Galilei spent his final years under house arrest for defending heliocentrism; he died confined and nearly blind in~1642. Ignaz Semmelweis demonstrated that hand-washing could eliminate childbed fever, was dismissed by the medical establishment, and died in an asylum in~1865---beaten by guards---years before germ theory proved him right. Emmy Noether spent years lecturing at G\"ottingen unpaid, her courses listed under Hilbert's name, before being expelled in~1933; she died in exile at Bryn~Mawr in~1935, aged~53. Ludwig Boltzmann---whose statistical mechanics underpins every thermodynamic calculation in this monograph---was ridiculed for decades by Ernst Mach, Wilhelm Ostwald, and others who denied the existence of atoms; he hanged himself in 1906, one year before experiments vindicated his atomistic theory. Paul Ehrenfest, who carried Boltzmann's statistical program forward, struggled with depression compounded by professional isolation; he took his own life in 1933. Lise Meitner co-discovered nuclear fission but was excluded from the Nobel Prize awarded to Otto Hahn alone; she spent decades in exile after fleeing Nazi Germany. Subrahmanyan Chandrasekhar derived the mass limit for white dwarfs as a young man, only to have Arthur Eddington publicly mock the result for years; the Nobel Prize arrived in 1983, half a century later. The theorems of physics are not born in comfort. They are wrested from nature by people whom the profession often treats as expendable.

To me, the mathematics in this monograph is more than abstraction. Every formula in the constructive program carries the weight of the pioneers who saw what others refused to see. Brouwer's conviction that completed infinities require justification, Bishop's demonstration that constructive analysis \emph{works}---these are not merely logical positions but acts of intellectual courage that cost their authors dearly. The hierarchy $\BISH \subset \LLPO \subset \WLPO \subset \LPO$ is a mathematical structure, but it is also a monument to the people who built it while their colleagues looked away. Each formula speaks, if you listen, with a human voice. This monograph is, in part, an act of recognition: that the mathematics these pioneers created---sidelined for a century---classifies the logical structure of nature with a precision that vindicates the suffering it cost them. The fit between Bishop's hierarchy and the theorems of physics, reported in these pages, is discovered and not designed. But the human story behind the discovery is inseparable from the mathematics itself.

\section*{Recognition Without Change}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  event/.style={circle, fill=blue!30, draw=blue!60, minimum size=10pt, inner sep=0pt},
  lbl/.style={text width=3.2cm, align=center, font=\small},
  >=stealth
]
% Timeline axis
\draw[->,thick,gray] (-0.5,0) -- (14.5,0);

% Events
\node[event] (B) at (0,0) {};
\node[lbl,above=4pt] at (B) {\textbf{1907}\\Brouwer\\Intuitionism};

\node[event] (Bi) at (3.5,0) {};
\node[lbl,below=4pt] at (Bi) {\textbf{1967}\\Bishop\\\emph{Foundations of\\Constructive Analysis}};

\node[event] (Ba) at (7,0) {};
\node[lbl,above=4pt] at (Ba) {\textbf{2013}\\Bauer\\IAS talk\\\emph{Five Stages}};

\node[event] (P) at (9,0) {};
\node[lbl,below=4pt] at (P) {\textbf{2017}\\Published in\\Bull.\ AMS};

\node[event] (C) at (11,0) {};
\node[lbl,above=4pt] at (C) {\textbf{2022}\\Levi L.\ Conant\\Prize (AMS)};

\node[event,fill=green!30,draw=green!60] (T) at (13.5,0) {};
\node[lbl,above=4pt] at (T) {\textbf{2025--26}\\This program\\42 papers\\35k lines \Lean};

% Bracket: recognition without change
\draw[red!70,very thick,decorate,decoration={brace,amplitude=8pt,mirror}]
  (6.5,-1.8) -- (11.5,-1.8)
  node[midway,below=10pt,text=red!70,font=\small\itshape,text width=4.5cm,align=center]
  {Recognition without\\foundational change};

% Arrow to program
\draw[->,green!60!black,thick,dashed] (11.5,-1.5) to[out=10,in=250] (13.5,-0.15);
\node[green!60!black,font=\small\itshape,text width=3cm,align=center] at (13.5,-1.4) {Empirical\\demonstration};
\end{tikzpicture}
\caption{Timeline of constructive mathematics: from Brouwer's intuitionism through Bishop's constructivism to Bauer's prize-winning exposition. Despite recognition at the highest levels, no foundational change occurred in mainstream mathematical practice.}
\label{fig:timeline}
\end{figure}

In March 2013, Andrej Bauer delivered a talk at the Institute for Advanced Study entitled \emph{Five Stages of Accepting Constructive Mathematics}. Adapting the K\"ubler-Ross model of grief, Bauer characterized the journey that a classical mathematician undergoes when confronting constructive mathematics: \textbf{Denial} (``this is not real mathematics''), \textbf{Anger} (``it's too restrictive to be useful''), \textbf{Bargaining} (``perhaps it has niche applications''), \textbf{Depression} (``it may be better, but nobody will adopt it''), and \textbf{Acceptance} (``this is how mathematics should be done''). The talk was brilliant, accessible, and widely watched. It was converted into a peer-reviewed article~\cite{Bauer2017} published in the \emph{Bulletin of the American Mathematical Society} in 2017. In 2022, the article was awarded the \textbf{Levi L.\ Conant Prize}---the AMS prize for the best expository paper.

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  stage/.style={rectangle, rounded corners=4pt, draw=#1!60, fill=#1!15,
    minimum width=2.1cm, minimum height=1.5cm, text width=1.9cm,
    align=center, font=\small\bfseries},
  annot/.style={font=\scriptsize\itshape, text width=2.1cm, align=center},
  arr/.style={->,thick,gray!70,>=stealth}
]
\node[stage=red]    (D) at (0,0)    {Denial};
\node[stage=orange] (A) at (2.8,0)  {Anger};
\node[stage=yellow!80!orange] (B) at (5.6,0) {Bargaining};
\node[stage=blue]   (Dp) at (8.4,0) {Depression};
\node[stage=green!60!black]  (Ac) at (11.2,0) {Acceptance};

\node[annot,below=2pt] at (D.south)  {``Not real\\mathematics''};
\node[annot,below=2pt] at (A.south)  {``Too\\restrictive''};
\node[annot,below=2pt] at (B.south)  {``Maybe for\\niche uses''};
\node[annot,below=2pt] at (Dp.south) {``Better, but\\nobody cares''};
\node[annot,below=2pt] at (Ac.south) {``This is the\\right way''};

\draw[arr] (D) -- (A);
\draw[arr] (A) -- (B);
\draw[arr] (B) -- (Dp);
\draw[arr] (Dp) -- (Ac);

\node[font=\small,gray] at (5.6,-2.5) {After Bauer (2017), awarded the Levi L.\ Conant Prize, 2022};
\end{tikzpicture}
\caption{Bauer's \emph{Five Stages of Accepting Constructive Mathematics}, adapted from the K\"ubler-Ross model. The talk (IAS, 2013) and paper (\emph{Bulletin of the AMS}, 2017) received the highest recognition---yet mainstream mathematical practice remained unchanged.}
\label{fig:fivestages}
\end{figure}

A talk at the Institute for Advanced Study. A paper in the \emph{Bulletin of the AMS}. A major prize from the American Mathematical Society. By any measure, constructive mathematics received recognition at the highest level.

And yet: nothing changed. Mathematics departments do not teach $\BISH$ as a foundation. Textbooks do not distinguish constructive from non-constructive proofs. The omniscience hierarchy is unknown outside specialist circles. Graduate students in analysis learn the Bolzano--Weierstrass theorem without learning that it is equivalent to the Fan Theorem. Graduate students in logic learn G\"odel's completeness theorem without learning that it implies $\LPO$. The five stages that Bauer described so vividly remain, for the vast majority of working mathematicians, somewhere between Denial and Bargaining.

\section*{The Missing Piece}

Why did recognition not produce change? We suggest one reason: constructive mathematics has been presented as a \emph{philosophical position} about what mathematics \emph{should} be---an advocacy program. Bauer's paper is explicitly about \emph{acceptance}: the audience is presumed skeptical, and the argument is persuasive. Bishop's original work was motivated by philosophical convictions about the nature of mathematical existence. The constructive program, for all its mathematical depth, has operated in the mode of advocacy rather than empirical demonstration.

The missing approach was not advocacy but \emph{science}: test the constructive hierarchy against nature. Take the actual theorems of physics---the ones that produce numbers experimentalists measure---and calibrate them against Bishop's hierarchy. Do not argue that constructive mathematics is better. \emph{Show what it reveals.}

Nobody had done this because the people who understood constructive analysis did not work in physics, and the people who worked in physics did not know the constructive hierarchy existed.

This monograph presents the results of that test. Across 42~papers spanning the Standard Model, general relativity, statistical mechanics, quantum information theory, the AdS/CFT correspondence, and the cosmological constant problem---formalized in approximately 35{,}000 lines of machine-checked \Lean\ proof code---the test yields a single, clean result: the logical resources required for all empirical predictions in known physics are exactly $\BISH+\LPO$. Not more. Not less. Bishop's hierarchy, designed for pure mathematics in the 1960s, classifies the non-constructive content of physics with perfect precision.

The fit is discovered, not designed. That is what makes it worth reporting.

\section*{What This Monograph Contains}

The \textbf{Prologue} tells the intellectual story of the 42-paper program: how the question was invented, how the pattern was discovered, how it was proved, and how it survived every test. \textbf{Chapters~1--10} present the technical content: the constructive hierarchy, the calibration table across all physics domains, the three foundational theorems, the Standard Model verification, the conservation metatheorem, the boundary analysis, the undecidability genealogy, and the consequences. \textbf{Chapters~11--12} apply the framework as a diagnostic tool to the AdS/CFT correspondence and the cosmological constant problem. \textbf{Chapters~13--15} describe the formalization methodology, acknowledge the program's limitations, and conclude.

% ======================================================================
%  PROLOGUE
% ======================================================================
\chapter*{Prologue: The Intellectual Journey}
\addcontentsline{toc}{chapter}{Prologue: The Intellectual Journey}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  act/.style={rectangle, rounded corners=5pt, draw=blue!60, fill=blue!5,
    minimum width=3.5cm, minimum height=2cm, text width=3.3cm, align=center},
  conn/.style={->,thick,blue!40,>=stealth},
  note/.style={font=\scriptsize\itshape,text width=3.3cm,align=center}
]
% Row 1
\node[act] (A1) at (0,0)    {\textbf{Act I}\\Discovery\\Papers 2--9};
\node[act] (A2) at (4.5,0)  {\textbf{Act II}\\Mapping\\Papers 10--28};
\node[act] (A3) at (9,0)    {\textbf{Act III}\\Foundations\\Papers 29--31};

% Row 2
\node[act] (A4) at (0,-3.5)   {\textbf{Act IV}\\Standard Model\\Papers 32--34};
\node[act] (A5) at (4.5,-3.5) {\textbf{Act V}\\Explanation\\Paper 35};
\node[act] (A6) at (9,-3.5)   {\textbf{Act VI}\\Undecidability\\Papers 36--39};

% Row 3
\node[act,draw=green!60,fill=green!5] (A7) at (4.5,-7) {\textbf{Act VII}\\Diagnostics\\Papers 41--42};

% Annotations
\node[note,below=1pt] at (A1.south) {BISH/LPO\\pattern glimpsed};
\node[note,below=1pt] at (A2.south) {28 calibrations\\zero exceptions};
\node[note,below=1pt] at (A3.south) {Fekete $\equiv$ LPO\\FT, DC dispensable};
\node[note,below=1pt] at (A4.south) {LHC predictions\\are pure BISH};
\node[note,below=1pt] at (A5.south) {Metatheorem:\\why $\leq$ LPO};
\node[note,below=1pt] at (A6.south) {All undecidability\\$=$ LPO};
\node[note,below=1pt] at (A7.south) {AdS/CFT, $\Lambda$\\problem dissolved};

% Arrows
\draw[conn] (A1) -- (A2);
\draw[conn] (A2) -- (A3);
\draw[conn] (A3) -- (A4);
\draw[conn] (A4) -- (A5);
\draw[conn] (A5) -- (A6);
\draw[conn] (A6) -- (A7);
\end{tikzpicture}
\caption{The intellectual arc of the program in seven acts. Acts~I--VI established the $\BISH+\LPO$ characterization; Act~VII applies it as a diagnostic tool to open problems.}
\label{fig:arc}
\end{figure}

\section*{Act I: The Question Nobody Asked (Papers 2--9)}

The program began not with an answer but with the discovery that a question was missing.

Classical reverse mathematics---the program of Friedman and Simpson~\cite{Simpson2009}---had spent decades calibrating theorems of ordinary mathematics against subsystems of second-order arithmetic. They found that most theorems fall into exactly five categories (the ``Big Five''), revealing deep structure in mathematical reasoning. But their program was about pure mathematics and used classical logic throughout.

Constructive mathematics---the program of Bishop, Bridges, Richman, Ishihara---had spent decades mapping the omniscience hierarchy: $\BISH$, $\LLPO$, $\WLPO$, $\LPO$, and the fine separations between them. But their program was about the philosophy of mathematics, not physics.

Nobody had asked: if you take the actual theorems of physics---the ones that produce numbers experimentalists measure---and calibrate them against Bishop's constructive hierarchy, where do they land?

\textbf{Paper~2 (Bidual gap and $\WLPO$)} was the first calibration. The result---that identifying a Banach space with its double dual costs exactly $\WLPO$---is a minor theorem in constructive functional analysis. Its significance was not the theorem but the \emph{act}: demonstrating that a routine physicist's assumption (bra-ket duality) has a precise, nontrivial logical cost. The methodology was invented here. Every subsequent paper followed the template Paper~2 established: formalize the physics in \Lean, run \texttt{\#print axioms}, identify the omniscience principle, prove the reverse direction.

\textbf{Paper~5 (Schwarzschild geometry)} produced an accidental methodological discovery. Mathlib lacked the differential geometry infrastructure to formalize GR properly. The response was pragmatic: formalize the \emph{output} directly. The Schwarzschild metric is a specific formula; the curvature is a specific rational function. This pragmatic decision turned out to be a foundational insight: the logical cost of a physical prediction is determined by the prediction itself, not by the proof strategy used to derive it. The infrastructure can be arbitrarily expensive logically while the output is cheap. This observation became the conceptual seed of the dispensability results (Papers~30--31).

\textbf{Paper~8 (Ising model)} was where the program found its paradigmatic example. The finite partition function is $\BISH$. The thermodynamic limit is $\LPO$. The boundary is surgically precise.

By the end of Act~I, the program had nine calibrations across four domains. The pattern---$\BISH$ for finite, $\LPO$ for limits, nothing higher---was visible but unproved.

\section*{Act II: Mapping the Territory (Papers 10--28)}

Paper~10~\cite{Lee2026P10} organized the results into a table and proposed the $\BISH+\LPO$ thesis as a conjecture; Paper~12~\cite{Lee2026P12} provided the historical and philosophical context, connecting the program to Bishop's legacy and 150~years of mathematical physics. Then began the systematic exploration.

Bell nonlocality---the deepest puzzle of quantum mechanics---costs exactly $\LLPO$. Not $\WLPO$, not $\LPO$, but the \emph{Lesser} Limited Principle of Omniscience. Quantum nonlocality is logically cheaper than a phase transition.

The event horizon (Paper~13) is a \emph{logical} boundary: the surface $r=2M$ is where $\BISH$ meets $\WLPO$.

The measurement problem (Paper~23) classified the three major interpretations of quantum mechanics as disagreeing about their dependence on Dependent Choice, not about physics. This result's full significance would emerge only with Paper~31.

By the end of Act~II, the program had 28 calibrations across every major domain of physics. The pattern held without exception.

\section*{Act III: The Foundations (Papers 29--31)}

Three papers transformed the program.

\textbf{Paper~29 (Fekete's Subadditive Lemma $\equiv$ $\LPO$)} is the program's most important single result. Phase transitions are empirically real. Their mathematical description requires Fekete's lemma. Fekete's lemma requires $\LPO$. Therefore $\LPO$ is physically instantiated, not a mathematical convenience. Before Paper~29, the program showed that $\LPO$ \emph{suffices}. Paper~29 showed that $\LPO$ is \emph{necessary}. The ceiling is load-bearing.

\textbf{Paper~30 (Fan Theorem dispensability)} knocked out the first piece of scaffolding. Every empirical prediction derived using compactness can be re-derived without it, at $\BISH+\LPO$.

\textbf{Paper~31 (Dependent Choice dispensability)} knocked out the second piece. The combined result: $\BISH+\LPO$ is necessary (Paper~29) and sufficient (Papers~30--31) for empirical physics.

\section*{Act IV: The Standard Model Sweep (Papers 32--34)}

Paper~34 delivered the program's biggest surprise since Paper~29. Tree-level scattering: $\BISH$. One-loop corrections: $\BISH$. Dimensional regularization: $\BISH$. IR cancellation: $\BISH$. Everything is $\BISH$---not $\BISH+\LPO$, but \emph{pure} $\BISH$. $\LPO$ enters only for all-orders convergence, which no experiment tests. The sentence the program earned: \emph{every quantity the LHC measures is constructively computable.}

\section*{Act V: The Explanation (Paper 35)}

The conservation metatheorem answered: the pattern is a \emph{consequence of the mathematical structure of physical predictions}. Finite-order predictions are compositions of computable functions ($\BISH$); limits without computable modulus are $\BMC \equiv \LPO$; nothing in physics requires anything beyond these two forms.

\section*{Act VI: The Undecidability Genealogy (Papers 36--39)}

Cubitt's spectral gap undecidability was widely interpreted as revealing fundamental unknowability. Papers~36--38 proved it is Turing--Weihrauch equivalent to $\LPO$---the same principle that governs every thermodynamic limit. Paper~38 traced the entire genealogy to Wang tiling (1961) as a single ancestor.

Paper~39 discovered that generic intensive observables without promise gap can reach $\Sigma^0_2$---but this tier is empirically inaccessible due to finite experimental precision.

\section*{Act VII: Diagnostics (Papers 41--42)}

Paper~41 applied the framework to the AdS/CFT correspondence and discovered that the holographic dictionary is an \emph{axiom-preserving map}: bulk and boundary carry identical axiom cost at every level examined.

Paper~42 applied it to the cosmological constant problem and showed that the $10^{120}$ discrepancy is a regulator-dependent artifact---scaffolding, not a prediction. The genuine constraint is a 55-digit fine-tuning at $\LPO$.

\section*{The Arc}

\textbf{Discovery} (2--9): A question is invented. A pattern is glimpsed. \textbf{Exploration} (10--28): The pattern is tested everywhere. It holds. \textbf{Foundation} (29--31): The pattern is proved. \textbf{Verification} (32--34): The Standard Model confirms it. \textbf{Explanation} (35): Why the pattern holds. \textbf{Undecidability} (36--39): Even the limits of physics land at $\LPO$. \textbf{Diagnostics} (41--42): The framework diagnoses open problems.

The fit between Bishop's hierarchy and physics was discovered, not designed. The hierarchy was built for pure mathematics in the 1960s. The physics was built by nature over 13.8 billion years. That they match---precisely, across every calibrated domain, verified in 35{,}000 lines of formal proof---is either the most remarkable coincidence in the philosophy of science, or evidence that the hierarchy captures something real about the structure of mathematical reasoning and its relationship to physical law.

\begin{chapterbox}
Constructive mathematics received institutional recognition at the highest levels but produced no foundational change in mathematical practice. This program provides the missing empirical demonstration: the logical cartography Bishop and his successors created for pure mathematics classifies the non-constructive content of physics with perfect precision.
\end{chapterbox}

% ======================================================================
%  CHAPTER 1
% ======================================================================
\chapter{Introduction --- What This Paper Proves}

\section{The Main Result}

Every physical theory makes predictions. Those predictions are mathematical statements---numbers that can be compared to experimental measurements. This monograph asks a simple question: what logical resources are needed to derive those numbers?

The answer, established across 42~formal verification papers covering the major domains of modern physics, is unexpectedly clean. Every empirical prediction in known physics can be derived using exactly two logical ingredients:

\begin{enumerate}[nosep]
\item \textbf{$\BISH$ (Bishop's Constructive Mathematics):} mathematics in which every existence claim comes with a construction, every function is computable, and every proof provides an algorithm. This is the mathematics of finite computation.
\item \textbf{$\LPO$ (Limited Principle of Omniscience):} the ability to decide, for any binary sequence, whether all terms are zero or some term is nonzero. Equivalently: the ability to complete a bounded monotone limit. This is the mathematics of idealized infinite processes.
\end{enumerate}

Nothing more is needed. The Fan Theorem (compactness), Dependent Choice (sequential construction), Bar Induction (well-founded tree search), the full Axiom of Choice, the Continuum Hypothesis, large cardinal axioms---none of these are required for any empirical prediction in the Standard Model, general relativity, statistical mechanics, or quantum information theory.

\section{Why This Matters}

The characterization identifies the computational architecture of physical reality. If empirical physics requires exactly $\BISH+\LPO$, then the mathematical universe accessible to physics extends precisely to one quantifier alternation over decidable predicates---$\Sigma^0_1$ in the arithmetic hierarchy---and no further.

The characterization also provides a diagnostic tool for theoretical physics. Any theoretical framework whose empirical predictions require logical resources beyond $\LPO$ is either making predictions that cannot be experimentally tested or is using unnecessarily strong mathematical scaffolding.

\section{The Hierarchy at a Glance}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  box/.style={rectangle, rounded corners=3pt, draw, minimum width=7cm,
    minimum height=0.7cm, align=center, font=\small},
  ceil/.style={dashed, very thick},
  >=stealth
]
% Vertical layout
\node[box, fill=green!15]  (bish) at (0,0)   {$\BISH$ --- Finite computation};
\node[box, fill=cyan!10]   (llpo) at (0,1.2)  {$\LLPO$ --- Sign decision / disjunction};
\node[box, fill=blue!10]   (wlpo) at (0,2.4)  {$\WLPO$ --- Zero-test / threshold decision};
\node[box, fill=blue!20]   (lpo)  at (0,3.6)  {$\LPO$ --- Bounded Monotone Convergence};

% Empirical ceiling
\draw[red!70, very thick, dashed] (-4.5,4.5) -- (4.5,4.5);
\node[red!70, font=\small\bfseries, right] at (4.6,4.5) {Empirical ceiling};
\node[red!70, font=\scriptsize, right] at (4.6,4.15) {(Papers 29--38)};

% Sigma_2^0
\node[box, fill=purple!10] (sig2) at (0,5.5) {$\Sigma^0_2$-$\LEM$ ($\LPO'$) --- Generic intensive observables};

% Platonic ceiling
\draw[purple!70, double, very thick] (-4.5,6.4) -- (4.5,6.4);
\node[purple!70, font=\small\bfseries, right] at (4.6,6.4) {Platonic ceiling};
\node[purple!70, font=\scriptsize, right] at (4.6,6.05) {(Paper 39)};

% Above
\node[box, fill=gray!10, draw=gray!40] (above) at (0,7.5) {$\FT$ / $\DC$ / $\BI$ / Full $\LEM$ / $\AC$ / CH / Large cardinals};

% Arrows
\draw[->,thick] (bish) -- (llpo);
\draw[->,thick] (llpo) -- (wlpo);
\draw[->,thick] (wlpo) -- (lpo);
\draw[->,thick,gray!50] (sig2) -- (above);

% Dispensable branches
\node[rectangle, rounded corners=3pt, draw=gray!50, fill=gray!5,
      font=\scriptsize\itshape, text width=2cm, align=center]
  (ft) at (-5.8,5.5) {Fan Theorem\\(dispensable)};
\node[rectangle, rounded corners=3pt, draw=gray!50, fill=gray!5,
      font=\scriptsize\itshape, text width=2cm, align=center]
  (dc) at (5.8,5.5) {Dep.\ Choice\\(dispensable)};
\draw[->,gray!50,dashed] (ft) -- (lpo);
\draw[->,gray!50,dashed] (dc) -- (lpo);
\node[gray!50,font=\scriptsize] at (-5.8,4.85) {Paper 30};
\node[gray!50,font=\scriptsize] at (5.8,4.85) {Paper 31};

% Bracket: physics uses this
\draw[green!60!black,very thick,decorate,decoration={brace,amplitude=6pt}]
  (-4.5,-0.4) -- (-4.5,4.0)
  node[midway,left=8pt,font=\small,text=green!60!black,text width=2cm,align=center]
  {Physics\\uses this};

% Bracket: beyond physics
\draw[gray!50,very thick,decorate,decoration={brace,amplitude=6pt,mirror}]
  (4.5,6.8) -- (4.5,8.0)
  node[midway,right=8pt,font=\small,text=gray!50,text width=2.5cm,align=center]
  {Far beyond\\physics};
\end{tikzpicture}
\caption{The constructive hierarchy with the two ceilings established by this program. The empirical ceiling ($\LPO$, dashed red) and the Platonic ceiling ($\Sigma^0_2$, double purple) bound the logical resources accessible to physics. The Fan Theorem and Dependent Choice are shown as dispensable branches.}
\label{fig:hierarchy}
\end{figure}

Two boundaries are established by this monograph (\cref{fig:hierarchy}). The dashed line is the \textbf{empirical ceiling}: everything below it has been physically instantiated across 42~calibration papers; $\LPO$ is necessary (Paper~29) and sufficient (Papers~30--31); all physical undecidability lives here (Papers~36--38). The double line is the \textbf{Platonic ceiling}: generic intensive observables reach $\Sigma^0_2$ (Paper~39), but this tier is empirically inaccessible.

\section{Structure of This Monograph}

Chapter~2 introduces the constructive hierarchy. Chapter~3 presents the calibration table: 42~papers organized by physics domain. Chapter~4 establishes the three foundational theorems. Chapter~5 tests the thesis against the Standard Model. Chapter~6 presents the conservation metatheorem. Chapter~7 analyzes the boundary. Chapters~8 and~9 form the undecidability arc. Chapter~10 draws consequences. Chapters~11 and~12 apply the framework diagnostically to AdS/CFT and the cosmological constant. Chapters~13--15 describe the formalization, delineate limitations, and conclude.

\medskip\noindent
\textbf{Companion papers.}\quad
For readers seeking additional detail on the individual calibrations summarized in Chapter~3, Paper~10~\cite{Lee2026P10} provides the original synthesis and calibration table for Papers~2--29. Paper~12~\cite{Lee2026P12} develops the historical and philosophical context, tracing the constructive view of mathematical physics from Bishop to the present program.

\section{What This Paper Does Not Prove}

This monograph does not prove that all \emph{possible} physical theories are $\BISH+\LPO$. The characterization covers all currently known physics but a future theory might require more. If a physical phenomenon is discovered that requires $\Sigma^0_2$ reasoning to predict its empirical behavior, the characterization is refuted. The thesis is empirical and falsifiable, not a priori or necessary.

\begin{chapterbox}
The logical constitution of empirically accessible physics is exactly $\BISH+\LPO$: Bishop's constructive mathematics augmented by the Limited Principle of Omniscience. No more, no less. Established across 42~papers spanning every major domain of physics, verified in approximately 35{,}000 lines of \Lean\ proof.
\end{chapterbox}

% ======================================================================
%  CHAPTER 2
% ======================================================================
\chapter{The Constructive Hierarchy}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  principle/.style={rounded corners=3pt, draw, minimum width=2cm, minimum height=0.7cm, font=\small\bfseries, text centered},
  >=Stealth,
]
% Main spine
\node[principle, fill=green!15] (BISH) at (0,0) {$\BISH$};
\node[principle, fill=cyan!12] (LLPO) at (0,1.5) {$\LLPO$};
\node[principle, fill=blue!10] (WLPO) at (0,3) {$\WLPO$};
\node[principle, fill=blue!18] (LPO) at (0,4.5) {$\LPO$};

% Above LPO
\node[principle, fill=purple!10, draw=purple!40] (S2) at (0,6) {$\Sigma^0_2$};

% Independent branches
\node[principle, fill=gray!10, draw=gray!50, dashed] (FT) at (-5,2.25) {$\FT$};
\node[principle, fill=gray!10, draw=gray!50, dashed] (DC) at (5,2.25) {$\DC$};

% Spine arrows
\draw[->, thick] (BISH) -- (LLPO) node[midway, left, font=\scriptsize] {strict};
\draw[->, thick] (LLPO) -- (WLPO) node[midway, left, font=\scriptsize] {strict};
\draw[->, thick] (WLPO) -- (LPO) node[midway, left, font=\scriptsize] {strict};
\draw[->, thick, purple!50, dashed] (LPO) -- (S2) node[midway, left, font=\scriptsize] {strict};

% Independent branches
\draw[<->, dashed, gray] (BISH) -- (FT) node[midway, above, font=\scriptsize, gray] {independent};
\draw[<->, dashed, gray] (BISH) -- (DC) node[midway, above, font=\scriptsize, gray] {independent};

% Dispensable labels
\node[font=\scriptsize, gray] at (-5,1.5) {dispensable};
\node[font=\scriptsize, gray] at (5,1.5) {dispensable};

% Brace: physics uses this (right side, cups left toward spine)
\draw[decorate, decoration={brace, amplitude=6pt}, green!60!black, thick]
  (2,4.8) -- (2,-0.3) node[midway, right=8pt, font=\small, green!60!black, align=left] {Physics\\uses this};

% Empirical ceiling
\draw[dashed, red, thick] (-3,5.25) -- (3,5.25) node[right, font=\scriptsize, red] {empirical ceiling};
\end{tikzpicture}
\caption{Hasse diagram of the omniscience hierarchy. The strict chain $\BISH \subset \LLPO \subset \WLPO \subset \LPO$ is the spine; the Fan Theorem and Dependent Choice are logically independent and dispensable for physics. Physics uses precisely this spine.}
\label{fig:hasse}
\end{figure}

This chapter introduces the four omniscience principles that form the constructive hierarchy---$\BISH$, $\LLPO$, $\WLPO$, $\LPO$---and the arithmetic hierarchy above them. These principles are the units of logical currency in which every physical theorem's non-constructive cost is denominated. Understanding them precisely is essential: the entire calibration program rests on the distinctions between them.

\section{Bishop's Constructive Mathematics ($\BISH$)}

$\BISH$ is the mathematical framework developed by Errett Bishop in \emph{Foundations of Constructive Analysis} (1967). Its defining characteristic is that every existence proof must provide a construction, and every function must be computable. $\BISH$ does not reject classical mathematics---it simply declines to use principles that assert existence without construction. A theorem proved in $\BISH$ is automatically valid in classical mathematics, in intuitionistic mathematics, and in recursive mathematics. It is mathematics with the broadest possible validity.

In $\BISH$, the real numbers are constructed as equivalence classes of Cauchy sequences of rationals with explicit moduli of convergence. A real number $x$ is \emph{defined} by an algorithm that, given any precision $\varepsilon > 0$, produces a rational approximation within~$\varepsilon$. Every real number that appears in a physics textbook ($\pi$, $e$, $\sqrt{2}$, the fine structure constant $\alpha \approx 1/137.036$) admits such an algorithm. The arithmetic operations ($+$, $\times$, $\div$), standard functions ($\exp$, $\log$, $\sin$, $\cos$), and special functions (Bessel, hypergeometric, polylogarithm) are all computable.

What $\BISH$ cannot do is decide, for an arbitrary real number~$x$, whether $x = 0$ or $x \neq 0$. This is not a deficiency but a structural feature: the decision requires inspecting infinitely many digits of~$x$, which no finite algorithm can accomplish in general. For specific numbers---is $\pi > 3$?---the answer is computable (yes, since the first few digits suffice). But for a number defined by a convergent series whose terms depend on unresolved conjectures, the zero-test may require information no finite computation can provide. The principles that restore various forms of this decision power are the omniscience principles.

A concrete physical example illustrates the scope of $\BISH$. The partition function of a 10-site Ising model, $Z_{10} = \sum_\sigma \exp(-\beta H(\sigma))$, sums over $2^{10} = 1024$ spin configurations. Each term is a computable exponential of a computable argument. The free energy $F_{10} = -kT \log Z_{10}$ is a computable real number. No omniscience principle is needed. The same holds for any finite lattice, any finite-dimensional Hilbert space, any finite Feynman diagram. The non-constructive content enters only when physicists take limits: $N \to \infty$, dimension $\to \infty$, loop order $\to \infty$.

\section{The Limited Principle of Omniscience ($\LPO$)}

\textbf{Formal statement:} For every binary sequence $\alpha : \NN \to \{0,1\}$, either $\exists n\, (\alpha(n)=1)$ or $\forall n\, (\alpha(n)=0)$.

In plain language: given an infinite sequence of bits, you can decide whether all bits are zero or some bit is one. This requires ``surveying'' the entire infinite sequence---hence ``omniscience.'' $\LPO$ is not provable in $\BISH$; it is an additional principle. It is strictly weaker than full classical logic ($\LEM$)---$\LPO$ decides one class of propositions (existential statements over decidable predicates), not all propositions.

$\LPO$ is equivalent to several other principles, the most important being \textbf{Bounded Monotone Convergence ($\BMC$)}: every bounded monotone sequence of real numbers converges. This equivalence, proved constructively by Ishihara, is the bridge between logic and analysis. $\BMC$ is what physicists use when they take thermodynamic limits, assert that coupling constants exist as completed real numbers, or claim that a variational minimum is attained. Two further equivalences: $\LPO$ is equivalent to Cauchy completeness without explicit modulus (every Cauchy sequence converges), and to supremum existence (every bounded set of reals has a least upper bound).

The Ising model provides the paradigmatic physical example. At finite lattice size~$N$, the free energy per site $f_N = -(kT/N)\log Z_N$ is a computable real number---$\BISH$. The sequence $(f_N)$ is bounded (by the coupling constant~$J$) and monotone (by subadditivity of the total free energy). The assertion that $f = \lim f_N$ exists as a completed real number is $\BMC$, hence $\LPO$. The phase transition---the non-analyticity of $f(\beta)$ at the critical inverse temperature~$\beta_c$---requires this completed limit. Without $\LPO$, the phase transition is not a theorem; it is a finite sequence of approximations that never crystallizes into a definite mathematical object.

The key finding of this program: $\LPO$ is not merely mathematically convenient for physics. It is physically \emph{necessary}. Phase transitions are empirically real phenomena---ice melts, magnets demagnetize, superconductors transition. Their mathematical description requires Fekete's Subadditive Lemma, which is equivalent to $\LPO$ over $\BISH$ (Paper~29). $\LPO$ is instantiated in nature.

\section{The Weak Limited Principle of Omniscience ($\WLPO$)}

\textbf{Formal statement:} For every binary sequence $\alpha$, either $\forall n\, (\alpha(n)=0)$ or $\neg\forall n\, (\alpha(n)=0)$.

The difference from $\LPO$ is subtle but precise. $\LPO$ provides a \emph{witness}---if some bit is~1, $\LPO$ tells you that fact (though not which bit). $\WLPO$ merely decides whether all bits are zero or not, without providing a witness in the ``not'' case. $\WLPO$ is strictly weaker than $\LPO$.

In physics, $\WLPO$ appears as the \emph{threshold decision} or \emph{zero-test}: is this quantity exactly zero, or is it nonzero? Concrete examples:
\begin{itemize}[nosep]
\item Deciding whether the external magnetic field $h$ equals zero: paramagnetic ($h=0$) vs.\ ferromagnetic ($h \neq 0$) phase (Paper~20).
\item Deciding whether the fermion chemical potential equals the mass threshold: Heaviside decoupling in the electroweak sector (Paper~18).
\item Deciding whether the Schwarzschild radial coordinate equals $2M$: the event horizon as a logical boundary (Paper~13).
\item Deciding whether the QCD mass gap $\Delta$ equals zero: confinement vs.\ deconfinement (Paper~33).
\end{itemize}
Each is a physically meaningful question---and each costs exactly $\WLPO$, which is strictly weaker than $\LPO$ and hence ``free'' once you have already paid for $\LPO$.

\section{The Lesser Limited Principle of Omniscience ($\LLPO$)}

\textbf{Formal statement:} For every binary sequence $\alpha$ with at most one nonzero term, either all even-indexed terms are zero or all odd-indexed terms are zero. Equivalently, for every real number $x$: $x \le 0$ or $x \ge 0$ (the \emph{sign decision}).

$\LLPO$ is strictly weaker than $\WLPO$. In physics, it appears as:
\begin{itemize}[nosep]
\item Bell nonlocality~\cite{Bell1964}: the CHSH~\cite{Clauser1969} inequality violation requires a sign decision---does $S > 2$ or $S \le 2$? (Papers~11, 21, 27).
\item WKB tunneling: deciding which side of a potential barrier a particle emerges on (Paper~19).
\item Global charge conservation: deciding the sign of a conserved charge with oscillating contributions (Paper~15).
\end{itemize}

The strict hierarchy $\BISH \subset \LLPO \subset \WLPO \subset \LPO$ means these principles are genuinely distinct. Physics uses all four levels, but never exceeds $\LPO$. A physically significant observation: quantum nonlocality---the phenomenon Einstein called ``spooky action at a distance''---has a precise logical cost, and it is \emph{strictly less} than $\LPO$. Nonlocality is logically cheaper than phase transitions.

\section{What Lies Above $\LPO$}

Above $\LPO$ lie several principles widely used in mathematics but shown by this program to be dispensable for empirical physics.

The \textbf{Fan Theorem ($\FT$)} asserts that every bar of a finitely-branching tree is uniform---equivalently, that every pointwise-continuous function on Cantor space is uniformly continuous. $\FT$ is the constructive counterpart of the Heine--Borel theorem and underpins compactness arguments throughout analysis: the existence of maxima on compact sets, the extraction of convergent subsequences (Bolzano--Weierstrass), the Arzel\`a--Ascoli theorem. Crucially, $\FT$ is logically \emph{independent} of $\LPO$---neither implies the other---meaning it lives in an entirely different ``direction'' in the constructive lattice. Paper~30 proves that every empirical prediction derived using $\FT$ can be re-derived at $\BISH+\LPO$ without it. Physics uses limits ($\LPO$'s territory) but not tree searches ($\FT$'s territory).

\textbf{Dependent Choice ($\DC$)} asserts that given a relation $R$ on a set with the property that for every $x$ there exists $y$ with $R(x,y)$, there exists an infinite sequence $x_0, x_1, x_2, \ldots$ with $R(x_n, x_{n+1})$ for all~$n$. Physicists use $\DC$ for the mean ergodic theorem, martingale convergence, and Picard iteration for differential equations. Paper~31 proves $\DC$ is dispensable: empirical predictions depend on finite initial segments of these sequences, and finite initial segments are $\BISH$.

Beyond these, \textbf{Bar Induction ($\BI$)}---induction over well-founded trees, stronger than $\FT$---is not needed. \textbf{Full $\LEM$}---for every proposition $P$, either $P$ or $\neg P$---is incomparably stronger than everything in the constructive hierarchy and is not needed. The \textbf{Axiom of Choice}, \textbf{Continuum Hypothesis}, and \textbf{large cardinal axioms} are set-theoretic principles entirely outside the scope of what physics requires. The mathematics of physics uses a remarkably small fragment of the available logical landscape.

\section{The Arithmetic Hierarchy and $\Sigma^0_2$}

The omniscience principles can be placed within the arithmetic hierarchy---a classification of logical complexity by quantifier alternation over decidable predicates.

\textbf{$\Sigma^0_1$} (one existential quantifier): Statements of the form $\exists n\, P(n)$, where $P$ is decidable. $\LPO$ decides all $\Sigma^0_1$ statements. The halting problem---``does Turing machine~$M$ halt?''---is the canonical $\Sigma^0_1$-complete problem.

\textbf{$\Pi^0_1$} (one universal quantifier): Statements of the form $\forall n\, P(n)$. $\LPO$ also decides $\Pi^0_1$ statements.

\textbf{$\Sigma^0_2$} (existential--universal): Statements of the form $\exists n\, \forall m\, Q(n,m)$---two quantifier alternations. Deciding $\Sigma^0_2$ statements requires $\LPO'$ (Sigma-2-$\LEM$), strictly stronger than $\LPO$.

\textbf{$\Delta^0_2$} (limit-computable): A real number is $\Delta^0_2$ if it can be approximated by a computable sequence whose limit exists but whose rate of convergence is not computable. Every real number accessible to $\BISH+\LPO$ is $\Delta^0_2$.

The critical distinction for this monograph: $\LPO$ sits at $\Sigma^0_1$. Papers~1--38 establish that empirical physics lives at $\Sigma^0_1$. Paper~39 discovers that the \emph{Platonic} ceiling---the logical cost of deciding exact properties of generic observables without promise gaps---reaches $\Sigma^0_2$. The gap between $\Sigma^0_1$ and $\Sigma^0_2$ is precisely the gap between empirical and Platonic physics, mediated by finite experimental precision.

\begin{chapterbox}
$\BISH \subset \LLPO \subset \WLPO \subset \LPO$ forms a strict hierarchy of omniscience principles. The Fan Theorem and Dependent Choice are logically independent of this spine. Physics uses precisely this spine up to $\LPO$---and nothing beyond.
\end{chapterbox}

% ======================================================================
%  CHAPTER 3
% ======================================================================
\chapter{The Calibration Table}

This chapter summarizes the calibration results across nine physics domains.  For each domain, we state the key observables, their axiom costs, and the mechanism by which non-constructivity enters.  The discussion here is necessarily compressed; for a detailed treatment of Papers~2--29 including full proof sketches, axiom audit tables, and the methodology behind each calibration, the reader should consult Paper~10~\cite{Lee2026P10}, which is the program's primary reference document.  Paper~12~\cite{Lee2026P12} provides the historical context, tracing how non-constructive methods entered each domain of physics and what the calibration table means for the philosophy of mathematical physics.

\section{Methodology}

The calibration procedure follows a uniform protocol (Paper~2): (1)~identify the physical theorem; (2)~formalize in \Lean; (3)~certify the axiom profile via \texttt{\#print axioms}; (4)~establish the reverse direction where possible; (5)~classify.

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  cell/.style={minimum width=1.8cm, minimum height=0.65cm, align=center, font=\scriptsize},
  hdr/.style={cell, font=\scriptsize\bfseries},
  grn/.style={cell, fill=green!25},
  lgrn/.style={cell, fill=green!10},
  cyn/.style={cell, fill=cyan!15},
  blu/.style={cell, fill=blue!12},
  dblu/.style={cell, fill=blue!25},
  wht/.style={cell, fill=white},
]
% Headers
\node[hdr] at (0,0) {};
\node[hdr] at (1.8,0) {$\BISH$};
\node[hdr] at (3.6,0) {$\LLPO$};
\node[hdr] at (5.4,0) {$\WLPO$};
\node[hdr] at (7.2,0) {$\LPO$};

% Row labels + cells
\foreach \y/\domain in {
  -0.65/{Stat.\ Mech.},
  -1.30/{Quantum Mech.},
  -1.95/{QFT},
  -2.60/{Gen.\ Relativity},
  -3.25/{Quantum Info.},
  -3.90/{Classical Mech.},
  -4.55/{Undecidability},
  -5.20/{AdS/CFT},
  -5.85/{Cosmology}
} {
  \node[hdr, text width=2cm, align=right] at (-1.2,\y) {\domain};
}

% Stat Mech: BISH(8,9,20), WLPO(20), LPO(8,9,29)
\node[grn]  at (1.8,-0.65) {8,9};
\node[wht]  at (3.6,-0.65) {};
\node[blu]  at (5.4,-0.65) {20};
\node[dblu] at (7.2,-0.65) {8,9,25,29};

% QM: BISH(4,6,14,16,22), LLPO(19), WLPO(23), LPO(4,6,14,16)
\node[grn]  at (1.8,-1.30) {4,6,14,16};
\node[cyn]  at (3.6,-1.30) {19};
\node[blu]  at (5.4,-1.30) {23};
\node[dblu] at (7.2,-1.30) {4,6,14,16};

% QFT: BISH(18,32,33,34), WLPO(18), LPO(18,32,33)
\node[grn]  at (1.8,-1.95) {18,32,33,34};
\node[wht]  at (3.6,-1.95) {};
\node[blu]  at (5.4,-1.95) {18};
\node[dblu] at (7.2,-1.95) {32,33};

% GR: BISH(5,15), WLPO(13), LPO(5,13,15,17)
\node[grn]  at (1.8,-2.60) {5,15};
\node[wht]  at (3.6,-2.60) {};
\node[blu]  at (5.4,-2.60) {13};
\node[dblu] at (7.2,-2.60) {5,13,15,17};

% QInfo: LLPO(11,21,24,27), WLPO(2,7,26)
\node[wht]  at (1.8,-3.25) {};
\node[cyn]  at (3.6,-3.25) {11,21,24,27};
\node[blu]  at (5.4,-3.25) {2,7,26};
\node[wht]  at (7.2,-3.25) {};

% Classical Mech: BISH(28)
\node[grn]  at (1.8,-3.90) {28};
\node[wht]  at (3.6,-3.90) {};
\node[wht]  at (5.4,-3.90) {};
\node[wht]  at (7.2,-3.90) {};

% Undecidability: LPO(36,37,38,39)
\node[grn]  at (1.8,-4.55) {};
\node[wht]  at (3.6,-4.55) {};
\node[wht]  at (5.4,-4.55) {};
\node[dblu] at (7.2,-4.55) {36--39};

% AdS/CFT: BISH(41), LLPO(41)
\node[grn]  at (1.8,-5.20) {41};
\node[cyn]  at (3.6,-5.20) {41};
\node[wht]  at (5.4,-5.20) {};
\node[wht]  at (7.2,-5.20) {};

% Cosmology: BISH(42), LPO(42)
\node[grn]  at (1.8,-5.85) {42};
\node[wht]  at (3.6,-5.85) {};
\node[wht]  at (5.4,-5.85) {};
\node[dblu] at (7.2,-5.85) {42};

% Grid lines
\draw[gray!40] (-0.3,0.35) -- (8.1,0.35);
\foreach \y in {0,-0.65,...,-5.85} {
  \draw[gray!20] (-0.3,\y-0.325) -- (8.1,\y-0.325);
}
\foreach \x in {0.9,2.7,4.5,6.3} {
  \draw[gray!20] (\x,0.35) -- (\x,-6.175);
}
\end{tikzpicture}
\caption{Calibration domain summary. Each cell lists paper numbers that calibrate to the given level in that physics domain. No cell appears beyond the $\LPO$ column. The pattern is uniform: $\BISH$ for finite computation, $\LPO$ for completed limits, nothing higher.}
\label{fig:calibration-grid}
\end{figure}

\section{Statistical Mechanics}

The 1D Ising model (Papers~8,~9,~20) provides the paradigm case and the program's proof of concept.  The finite partition function $Z_N = \sum_{\sigma} e^{-\beta H(\sigma)}$ is a finite sum of exponentials of rational expressions in $\beta$ and coupling constants---pure $\BISH$.  The thermodynamic limit $f = \lim_{N\to\infty} F(N)/N$, where $F(N) = -\beta^{-1}\log Z_N$, requires $\BMC$ (bounded monotone convergence), hence $\LPO$.  This is the paradigmatic instance of Fekete universality: the free energy sequence is subadditive because the energy of a composite system is at most the sum of the energies of its parts, and subadditivity is exactly the hypothesis of Fekete's lemma.

Paper~9 established \emph{formulation-invariance}: the transfer-matrix and direct-combinatorial formulations of the Ising model produce identical axiom profiles.  This is evidence that the calibration detects physics rather than notation.  Paper~20 showed that logical cost is \emph{observable-dependent}: the free energy costs $\LPO$ (thermodynamic limit), the magnetization zero-test costs $\WLPO$ (deciding whether an order parameter vanishes), and the finite-size energy costs $\BISH$ (finite sum).  The same physical system has three different logical costs depending on which quantity is computed.

The ergodic theorem (Paper~25) is the mathematical foundation of statistical mechanics---it justifies identifying time averages with ensemble averages.  The mean ergodic theorem requires countable choice ($\CCax$), subsumed by $\LPO$.  However, the empirical predictions it licenses---finite-time averages to specified precision---are $\BISH$, confirming the dispensability pattern.

\textbf{Fekete's Subadditive Lemma $\equiv$ $\LPO$} (Paper~29). Phase transitions require Fekete's lemma. Fekete's lemma requires $\LPO$. Therefore $\LPO$ is physically instantiated.  This is not merely a sufficiency result but an equivalence: Fekete's lemma can \emph{encode} $\LPO$, so the two are logically interchangeable.

\textbf{Fan Theorem dispensability} (Paper~30). Variational principles use $\FT$ via compactness (e.g., the assertion that a continuous function on a compact set attains its minimum), but the empirical content---the \emph{value} of the minimum, not the existence of a minimizer---is recoverable at $\BISH+\LPO$.

\textbf{Dependent Choice dispensability} (Paper~31). Iterative constructions (ergodic theorems, Picard iteration, sequential compactness) invoke $\DC$ to build infinite sequences step by step, but empirical predictions depend only on finite initial segments, which are $\BISH$.

\section{Quantum Mechanics}

The spectral theorem stratifies cleanly: finite-dimensional quantum mechanics (finite-dimensional Hilbert spaces, matrix mechanics) is entirely $\BISH$; unbounded operators on infinite-dimensional Hilbert spaces cost $\LPO$ because the spectral resolution requires completing an infinite limit.

The Heisenberg uncertainty principle (Paper~6) is an algebraic inequality $\Delta x \, \Delta p \ge \hbar/2$, derivable from the Cauchy--Schwarz inequality in any inner product space: $\BISH$ in finite dimensions, $\LPO$ when the operators are unbounded.  The Born rule (Paper~16) exhibits the same pattern: $|\langle\psi|\varphi\rangle|^2$ is $\BISH$-computable in finite dimensions.  This is the basis for Finding~11---the entire empirical content of quantum mechanics (every Born probability, every expectation value) is $\BISH$-computable.

Bell nonlocality (Papers~11,~21,~27) calibrates at $\LLPO$.  The CHSH inequality violation and the Tsirelson bound are \emph{computable} ($\BISH$)---these are the quantities experimentalists measure.  $\LLPO$ enters only in Bell's theorem itself: the proof that no local hidden variable model can reproduce quantum correlations.  Paper~27 identified the mechanism: $\LLPO$ enters through the Intermediate Value Theorem, which is used to show that a continuous function (the Bell expression as a function of hidden variable parameters) must cross a threshold.  This is the same mechanism that places Kochen--Specker contextuality (Paper~24) at $\LLPO$---hence the Bell $\equiv$ Kochen--Specker finding.

The WKB approximation (Paper~19) places quantum tunneling turning points at $\LLPO$: the classical turning point $E = V(x)$ is an IVT application.  Markov's Principle governs radioactive decay (Paper~22): the assertion ``not-not-decayed implies decayed'' is independent of both $\LPO$ and $\FT$, confirming the hierarchy has genuine branching structure beyond the main spine.

Decoherence (Paper~14) costs $\LPO$: the partial trace over an infinite-dimensional environment requires completing a thermodynamic limit.  This integrates the quantum-to-classical transition into the Fekete universality pattern.

\section{Quantum Field Theory}

Quantum field theory exhibits the sharpest instance of the BISH/LPO boundary: \emph{perturbative} QFT is $\BISH$; \emph{non-perturbative} QFT requires $\LPO$.  This alignment between two independently motivated boundaries---one logical, one physical---is Finding~10.

The Yukawa renormalization group flow (Paper~18) illustrates the full stratification within a single sector: the discrete RG step is $\BISH$ (arithmetic on coupling constants), threshold decoupling (deciding whether the energy scale equals a particle mass) costs $\WLPO$, and the global running over all scales costs $\LPO$ via $\BMC$.  QED one-loop running (Paper~32) follows the same pattern; the Landau pole is $\BISH$ because the one-loop beta function has an explicit closed-form solution.  QCD (Paper~33) mirrors the perturbative structure; the non-perturbative sector (confinement, mass gap) costs $\LPO$ via Fekete, confirming that the most complex gauge theory in the Standard Model reduces to the same logical mechanism as the Ising model.

Scattering amplitudes (Paper~34) delivered the program's strongest pure-$\BISH$ result: tree-level and one-loop predictions---every cross-section, branching ratio, and decay rate the LHC computes---are $\BISH$-computable without any omniscience principle.  Chapter~5 develops these results in full detail.

\section{General Relativity}

General relativity provides some of the program's most conceptually striking calibrations.

The Schwarzschild exterior geometry (Paper~5) is purely $\BISH$: geodesic and curvature computations on the outside region are algebraic operations on metric components, requiring no omniscience principle.  The event horizon (Paper~13) is where non-constructivity enters.  Local horizon detection (deciding whether the metric signature changes at a given radius) costs $\WLPO$; global horizon existence (the assertion that a complete geodesic terminates at finite affine parameter) costs $\LPO$.  The logical boundary of constructive computability and the physical boundary of the black hole coincide---both are the point where finite approximation fails to capture the infinite-extent structure.

Noether conservation laws (Paper~15) introduced an important nuance: charge conservation (from $U(1)$ symmetry) is $\BISH$ because the integrand is non-negative, making the integral's existence constructive.  Energy conservation costs $\LPO$ because the integrand can change sign, requiring $\LPO$ to establish convergence.  Two physically equivalent conservation laws have different logical costs due to a mathematical property (sign-definiteness) unrelated to physics.  This is the program's principal formulation-dependence counterexample, and it informs the open question of whether the calibration tracks physics or formalism.

Bekenstein--Hawking entropy~\cite{Bekenstein1973,Hawking1975} (Paper~17) calibrates at $\BISH$ for the area formula $S = A/(4\ell_P^2)$ (a closed-form expression) and $\LPO$ for the thermodynamic derivation from microstate counting (a completed limit).  The same physical quantity has two mathematical routes to it at different logical costs---another instance of formulation-dependence.

\section{Quantum Information}

Quantum information theory sits at the lower end of the omniscience spine, requiring at most $\WLPO$---strictly below the $\LPO$ ceiling that governs thermodynamics and field theory.

The bidual gap (Paper~2) was the program's first calibration and established the methodology: the existence of non-reflexive Banach spaces---the mathematical setting for quantum state spaces---is equivalent to $\WLPO$.  Paper~26 independently confirmed $\WLPO$-completeness via G\"odel sequences, an arithmetic route entirely independent of the functional analysis proof.  The robustness of this calibration across two independent proofs is evidence that the result tracks mathematical structure rather than proof technique.  Trace-class operators (Paper~7) provide the physical instantiation: density matrices, the mathematical objects representing quantum states in infinite dimensions, inherit the $\WLPO$ cost from the bidual gap.

The CHSH inequality and Tsirelson bound (Papers~11,~21,~27) calibrate at $\LLPO$, strictly below $\WLPO$.  The experimental quantities---violation ratios, correlation functions---are $\BISH$-computable.  $\LLPO$ enters only in Bell's theorem itself: the proof that no local hidden variable model can reproduce quantum correlations.  As discussed in the Quantum Mechanics section above, the mechanism is the Intermediate Value Theorem (Paper~27), which unifies Bell nonlocality and Kochen--Specker contextuality at a single logical cost.

The pattern across quantum information is consistent with the broader program: every \emph{measurable} quantity (expectation values, correlation functions, state tomography outputs) is $\BISH$; non-constructivity enters only in impossibility theorems about classical models of quantum phenomena.

\section{Physical Undecidability}

Physical undecidability is treated at length in Chapter~6, but the calibration-table summary belongs here.

The spectral gap problem (Paper~36) is the most celebrated undecidability result in mathematical physics: Cubitt, Perez-Garcia, and Wolf (2015) proved that deciding whether a many-body Hamiltonian is gapped or gapless is undecidable.  This result generated international headlines as evidence that physics contains ``fundamental unknowability.''  Paper~36 proved that the spectral gap decision is Turing--Weihrauch equivalent to $\LPO$---precisely the same logical principle that governs the Ising phase transition.  The most undecidable thing in physics is exactly as undecidable as a boiling pot of water.

Paper~37 established universality: every undecidability result in physics obtained by computable many-one reduction from the halting problem is Turing--Weihrauch equivalent to $\LPO$.  There is no hierarchy of physical undecidability---the landscape is flat at $\LPO$.  The paper explicitly stratified three further results (Penrose tiling, topological entropy, ground state degeneracy), all at $\LPO$.  A notable exception clarifies the boundary: the ground state energy \emph{density} (Watson--Cubitt 2021) is $\BISH$---computationally hard (exponential time) but logically decidable.  Computational complexity and logical undecidability are fundamentally different.

Paper~38 traced the genealogy to its source: Wang tiling (1961) is the combinatorial foundation on which Cubitt's construction and many other physical undecidability results rest.  The tiling problem itself calibrates at $\LPO$, confirming that physical undecidability is bounded at its source, not merely in its applications.

Paper~39 discovered the one genuine exception: generic intensive observables without a promise gap can reach $\Sigma^0_2$, strictly above $\LPO$ ($\Sigma^0_1$).  The construction uses Robinson aperiodic tilings with perimeter counters encoding a $\Pi^0_1$-complete set.  But this $\Sigma^0_2$ tier is empirically inaccessible: no finite-precision experiment can distinguish ``gap $= 0$'' from ``gap $< \varepsilon$,'' so finite experimental precision enforces an effective promise gap that collapses the decision back to $\LPO$.  The empirical ceiling remains $\BISH+\LPO$; the $\Sigma^0_2$ ceiling is Platonic.  Chapter~6 develops this distinction in full.

\begin{chapterbox}
Systematic calibration across nine physics domains (42~papers) yields a uniform pattern: $\BISH$ for finite computation, $\LPO$ for completed limits. No calibration exceeds $\LPO$. The pattern is universal across statistical mechanics, quantum mechanics, QFT, general relativity, quantum information, and physical undecidability.
\end{chapterbox}

% ======================================================================
%  CHAPTER 4
% ======================================================================
\chapter{The Three Foundational Theorems}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  pillar/.style={rounded corners=3pt, draw=blue!50, fill=blue!8, minimum width=3.5cm, minimum height=2.5cm, text centered, font=\small},
  result/.style={rounded corners=3pt, draw=green!60!black, fill=green!12, minimum width=11.5cm, minimum height=1cm, text centered, font=\small},
]
% Three pillars
\node[pillar] (T1) at (-4.2,1.5) {%
  \begin{tabular}{c}\textbf{Theorem I}\\[2pt]
  \textsc{Necessity}\\[4pt]
  Fekete $\equiv$ $\LPO$\\[2pt]
  {\scriptsize Phase transitions require it}\end{tabular}};

\node[pillar] (T2) at (0,1.5) {%
  \begin{tabular}{c}\textbf{Theorem II}\\[2pt]
  \textsc{Sufficiency}\\[4pt]
  $\FT$ dispensable\\[2pt]
  {\scriptsize Compactness is scaffolding}\end{tabular}};

\node[pillar] (T3) at (4.2,1.5) {%
  \begin{tabular}{c}\textbf{Theorem III}\\[2pt]
  \textsc{Sufficiency}\\[4pt]
  $\DC$ dispensable\\[2pt]
  {\scriptsize Infinite sequences are scaffolding}\end{tabular}};

% Result box
\node[result] (R) at (0,-1) {%
  \textbf{$\BISH+\LPO$ is exact:} ceiling is load-bearing, scaffolding is removable};

% Arrows converging
\draw[-{Stealth}, thick, blue!50] (T1.south) -- (R.north -| T1.south);
\draw[-{Stealth}, thick, blue!50] (T2.south) -- (R.north);
\draw[-{Stealth}, thick, blue!50] (T3.south) -- (R.north -| T3.south);
\end{tikzpicture}
\caption{The three foundational theorems. Theorem~I establishes necessity ($\LPO$ cannot be removed); Theorems~II and~III establish sufficiency ($\FT$ and $\DC$ add no empirical content). Together: $\BISH+\LPO$ is exact.}
\label{fig:three-pillars}
\end{figure}

This chapter presents the three results that transform the empirical calibration pattern into a thesis. Theorem~I proves $\LPO$ is necessary. Theorems~II and~III prove nothing beyond $\LPO$ is needed. Together they establish that $\BISH+\LPO$ is exact.

\section{Theorem I: Fekete's Subadditive Lemma $\equiv$ $\LPO$ (Paper 29)}

\begin{theorem}[Fekete--LPO Equivalence]
Over $\BISH$, Fekete's Subadditive Lemma is equivalent to the Limited Principle of Omniscience.
\end{theorem}

\textbf{Why this matters.} Fekete's Subadditive Lemma is not an obscure technical result. It states: if $(a_n)$ satisfies $a_{m+n} \le a_m + a_n$ for all $m,n \ge 1$, then $\lim a_n/n$ exists and equals $\inf_{n\ge 1} a_n/n$. This lemma is the mathematical engine of statistical mechanics. Free energy, pressure, entropy density, and other thermodynamic potentials are subadditive by construction (the energy of a composite system is at most the sum of the energies of its parts). Fekete's lemma is what guarantees the thermodynamic limit exists.

\textbf{Forward ($\LPO \Rightarrow$ Fekete):} Given $\LPO$ (equivalently $\BMC$), define $c_n = \inf_{k \le n} a_k/k$. This sequence is bounded below (by subadditivity) and monotone non-increasing. By $\BMC$, $L = \lim c_n$ exists. For any $\varepsilon > 0$, choose~$m$ with $a_m/m < L + \varepsilon$; for large $n = qm + r$, subadditivity gives $a_n/n \le a_m/m + O(1/n) < L + 2\varepsilon$. The downward bound $a_n/n \ge L$ follows from the infimum definition.

\textbf{Reverse (Fekete $\Rightarrow$ $\LPO$):} Given a binary sequence $\alpha : \NN \to \{0,1\}$, define $a_n = n - \sum_{k=1}^n \alpha(k)$ (counting zeros in the first $n$ terms). This sequence is subadditive: zeros in a concatenation are at most the sum of zeros in each part. By Fekete (assumed), $L = \lim a_n/n$ exists. If all $\alpha(k)=0$, then $a_n = n$ and $L=1$. If some $\alpha(k_0)=1$, then $a_n \le n-1$ for $n \ge k_0$, so $L \le 1 - 1/k_0 < 1$. Comparing $L$ to~$1$ decides $\LPO$. The encoding is simple but non-obvious---the insight that subadditivity can encode arbitrary binary information was, as far as we know, original to this program.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[
  box/.style={rectangle, draw, minimum width=0.7cm, minimum height=0.7cm, font=\small, text centered},
  lbl/.style={font=\small},
  >=Stealth
]
% Binary sequence row
\node[lbl, left] at (-0.5,2) {$\alpha$:};
\foreach \i/\v in {0/0, 1/0, 2/1, 3/0, 4/0, 5/0, 6/0} {
  \node[box, fill={\ifnum\v=1 blue!20\else green!15\fi}] (a\i) at (\i,2) {$\v$};
}
\node[lbl] at (7.5,2) {$\cdots$};

% Arrow: encoding
\draw[->, thick, gray] (3,1.5) -- (3,0.8) node[midway, right=3pt, font=\scriptsize] {$a_n = n - \sum\alpha(k)$};

% Subadditive sequence row
\node[lbl, left] at (-0.5,0.3) {$a_n/n$:};
\foreach \i/\v in {0/1, 1/1, 2/{2/3}, 3/1, 4/1, 5/1, 6/1} {
  \node[box, fill=green!8] (b\i) at (\i,0.3) {\scriptsize $\v$};
}
\node[lbl] at (7.5,0.3) {$\cdots$};

% Decision: L = lim a_n/n
\node[rectangle, rounded corners=3pt, draw, thick, fill=yellow!10, minimum width=2.5cm, font=\small\bfseries] (L) at (10,1.15) {$L = \lim a_n/n$};
\draw[->, thick] (7.8,1.15) -- (L.west);

% Two branches
\node[rectangle, rounded corners=3pt, draw=green!60!black, fill=green!12, text width=3.8cm, align=center, font=\small] (allzero) at (10,3) {$L = 1$\\[2pt] $\forall n,\; \alpha(n)=0$};
\node[rectangle, rounded corners=3pt, draw=blue!60, fill=blue!10, text width=3.8cm, align=center, font=\small] (someone) at (10,-0.7) {$L < 1$\\[2pt] $\exists n,\; \alpha(n)=1$};

\draw[->, green!60!black, thick] (L.north) -- (allzero.south);
\draw[->, blue!70, thick] (L.south) -- (someone.north);

% Bottom label
\node[font=\small\itshape, gray] at (5,-1.5) {Comparing $L$ to $1$ decides $\LPO$: subadditivity encodes binary information};
\end{tikzpicture}
\caption{The Fekete encoding: a binary sequence~$\alpha$ maps to a subadditive sequence~$(a_n)$ whose normalized limit~$L$ decides $\LPO$. If $\alpha$ is identically zero, $L=1$; if any term is~$1$, the limit drops strictly below~$1$.}
\label{fig:fekete-encoding}
\end{figure}

\textbf{Physical consequence:} Phase transitions are empirically real---ice melts, magnets demagnetize. Their mathematical description requires Fekete's lemma. Fekete's lemma requires $\LPO$. Therefore $\LPO$ is not a mathematical convenience but a physically instantiated principle. Before Paper~29, the program showed $\LPO$ \emph{suffices}. Paper~29 showed $\LPO$ is \emph{necessary}. The ceiling is load-bearing.

\textbf{Lean certification:} The forward direction uses \texttt{Classical.choice} from the $\LPO$ hypothesis (Level~3: intentional classical). The reverse direction compiles with zero classical axioms (Level~2). Approximately 1{,}300 lines of \Lean.

\section{Theorem II: Fan Theorem Dispensability (Paper 30)}

\begin{theorem}[FT Dispensability]
Every empirical prediction in physics derived using the Fan Theorem can be derived without it, using only $\BISH+\LPO$.
\end{theorem}

$\FT$ appears throughout mathematical physics under the guise of compactness: the Heine--Borel theorem, the Bolzano--Weierstrass theorem, the Arzel\`a--Ascoli theorem. When a physicist asserts that a continuous function on a compact set attains its maximum, or extracts a convergent subsequence from a bounded sequence, they invoke $\FT$.

Yet in every calibrated case, $\FT$ is dispensable. The pattern is consistent: variational principles assert ``a minimizer exists'' via compactness, but the Euler--Lagrange equations provide the minimizer directly---a construction, not an existence claim. Subsequence extraction asserts ``a convergent subsequence exists'' via Bolzano--Weierstrass, but the specific physical sequences in thermodynamic limits are bounded and monotone, converging by $\BMC$ ($\equiv \LPO$) without subsequence extraction.

The dispensability reflects a structural fact. $\FT$ and $\LPO$ are logically independent: $\FT$ is about tree searches (navigating well-founded branching structures), while $\LPO$ is about sequence limits (completing bounded monotone sequences). Physics uses limits, not tree searches. Paper~28 (classical mechanics) provides the most instructive example: Newton's formulation is $\BISH$, Lagrange's variational formulation uses $\FT$, but the empirical content---the trajectory---is identical. The variational scaffolding adds mathematical elegance, not physical content.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[
  sbox/.style={rectangle, rounded corners=3pt, minimum width=3.2cm, minimum height=0.8cm, font=\small, align=center},
  arr/.style={->, thick, >=Stealth},
]
% Left path: Classical (FT)
\node[font=\small\bfseries, gray!70] at (-2.5,4.8) {Classical path};
\node[sbox, draw=gray!50, fill=gray!8] (c1) at (-2.5,3.8) {Continuous $f$\\on compact $K$};
\node[sbox, draw=gray!50, fill=gray!8] (c2) at (-2.5,2.2) {Heine--Borel ($\FT$)};
\node[sbox, draw=gray!50, fill=gray!8] (c3) at (-2.5,0.6) {$f$ attains max};
\node[sbox, draw=gray!50, fill=gray!8] (c4) at (-2.5,-1.0) {Prediction};

\draw[arr, gray!60] (c1) -- (c2);
\draw[arr, gray!60] (c2) -- (c3);
\draw[arr, gray!60] (c3) -- (c4);

% Red X on FT node
\node[font=\Large\bfseries, red] at (-0.6,2.2) {$\times$};
\node[font=\scriptsize, red] at (-0.6,1.7) {dispensable};

% Right path: BISH+LPO
\node[font=\small\bfseries, green!50!black] at (3.5,4.8) {$\BISH+\LPO$ path};
\node[sbox, draw=green!60!black, fill=green!8] (d1) at (3.5,3.8) {Bounded monotone\\sequence};
\node[sbox, draw=blue!60, fill=blue!8] (d2) at (3.5,2.2) {$\BMC$ ($\equiv \LPO$)};
\node[sbox, draw=green!60!black, fill=green!8] (d3) at (3.5,0.6) {Limit exists};
\node[sbox, draw=green!60!black, fill=green!8] (d4) at (3.5,-1.0) {Same prediction};

\draw[arr, green!50!black] (d1) -- (d2);
\draw[arr, blue!60] (d2) -- (d3);
\draw[arr, green!50!black] (d3) -- (d4);

% Horizontal equivalence
\draw[<->, dashed, thick, purple!60] (c4.east) -- (d4.west) node[midway, above, font=\scriptsize, purple!60] {identical empirical content};

\end{tikzpicture}
\caption{Two proof paths to the same empirical prediction. The classical path invokes the Fan Theorem (compactness); the $\BISH+\LPO$ path uses Bounded Monotone Convergence directly. The Fan Theorem is scaffolding---removable without loss of physical content.}
\label{fig:ft-bypass}
\end{figure}

\textbf{Lean certification:} Approximately 1{,}300 lines of \Lean. The dispensability is demonstrated by exhibiting $\BISH+\LPO$ proofs for each previously $\FT$-dependent theorem and verifying via \texttt{\#print axioms}.

\section{Theorem III: Dependent Choice Dispensability (Paper 31)}

\begin{theorem}[DC Dispensability]
Every empirical prediction in physics derived using Dependent Choice can be derived without it, using only $\BISH+\LPO$.
\end{theorem}

$\DC$ appears whenever a construction proceeds iteratively: the mean ergodic theorem builds convergent Cesàro averages; martingale convergence builds adapted sequences of conditional expectations; Picard iteration builds successive approximations to ODE solutions. In each case, $\DC$ constructs an infinite object from a local step rule.

The key insight: empirical predictions never depend on the infinite object---they depend on finite initial segments. To predict a measurement at precision~$\varepsilon$, a physicist needs only finitely many iteration steps. Finite iteration is $\BISH$: given $x_0$ and a computable step function, computing $x_0, x_1, \ldots, x_n$ for any fixed~$n$ is finite recursion. The infinite sequence is mathematical scaffolding; the empirical content lives in the finite truncation.

The Weak Law of Large Numbers provides the most transparent example. Its empirical content---the probability bound $P(|S_n/n - \mu| > \varepsilon) < \delta$ for specified $n, \varepsilon, \delta$---is derivable from the Chebyshev--Markov inequality, which is pure $\BISH$ arithmetic. $\DC$ enters only to assert the Strong Law (almost sure convergence)---an ontological claim about an infinite sequence that no finite experiment can verify. Paper~31 proves every empirical prediction from the Strong Law is already available from the Weak Law.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[>=Stealth]
% Finite prefix (green)
\fill[green!12] (0,0) rectangle (7,1.2);
\draw[green!60!black, thick] (0,0) rectangle (7,1.2);

% Steps
\foreach \i/\lab in {0.5/$x_0$, 1.5/$x_1$, 2.5/$x_2$, 4/$\cdots$, 5.5/$x_{n-1}$, 6.5/$x_n$} {
  \node[font=\small] at (\i,0.6) {\lab};
}

% Infinite tail (gray, fading)
\fill[gray!8] (7.3,0) rectangle (12.5,1.2);
\draw[gray!40, thick] (7.3,0) rectangle (12.5,1.2);
\foreach \i/\lab in {8/$x_{n\!+\!1}$, 9.2/$x_{n\!+\!2}$, 10.4/$\cdots$, 11.6/$\infty$} {
  \node[font=\small, gray!60] at (\i,0.6) {\lab};
}

% Red dashed boundary
\draw[red, thick, dashed] (7.15,-0.5) -- (7.15,1.7);
\node[font=\scriptsize\bfseries, red, above] at (7.15,1.7) {empirical horizon ($\varepsilon$)};

% Labels above
\node[font=\small\bfseries, green!50!black] at (3.5,1.7) {$\BISH$: finite iteration};
\node[font=\small\bfseries, gray!60] at (10,1.7) {$\DC$: infinite tail};

% Weak Law (below green)
\node[font=\scriptsize, green!50!black, text width=6.5cm, align=center] at (3.5,-0.7) {Weak Law: $P(|S_n/n - \mu| > \varepsilon) < \delta$\\[2pt]\emph{This is what experiments test}};

% Strong Law (below gray)
\node[font=\scriptsize, gray!60, text width=5cm, align=center] at (10,-0.7) {Strong Law: $S_n/n \to \mu$ a.s.\\[2pt]\emph{No finite experiment verifies this}};
\end{tikzpicture}
\caption{Empirical content lives in the finite prefix ($\BISH$). The infinite continuation requires Dependent Choice but is ontological scaffolding: the Weak Law provides every testable prediction that the Strong Law does.}
\label{fig:finite-truncation}
\end{figure}

\textbf{Lean certification:} Approximately 1{,}400 lines of \Lean. Same verification structure as Paper~30.

\section{The Combined Result}

Theorems~I--III together establish:

$\BISH+\LPO$ is \textbf{necessary}: $\LPO$ cannot be eliminated---phase transitions require it (Theorem~I).

$\BISH+\LPO$ is \textbf{sufficient}: neither $\FT$ (Theorem~II) nor $\DC$ (Theorem~III) adds empirical content beyond $\BISH+\LPO$.

The logical constitution of empirically accessible physics is exactly $\BISH+\LPO$. Not more. Not less. The ceiling ($\LPO$) is load-bearing---you cannot lower it. The scaffolding ($\FT$, $\DC$) is removable---you can discard it without losing any empirical prediction.

\begin{chapterbox}
Three theorems transform the empirical pattern into a thesis. \textbf{Necessity:} Fekete's Subadditive Lemma $\equiv$ $\LPO$ (phase transitions require it). \textbf{Sufficiency:} the Fan Theorem and Dependent Choice are dispensable. Together: $\BISH+\LPO$ is exact---the ceiling is load-bearing and the scaffolding is removable.
\end{chapterbox}

% ======================================================================
%  CHAPTER 5
% ======================================================================
\chapter{The Standard Model at $\BISH+\LPO$}

The Standard Model of particle physics is the most precisely tested physical theory in history. The anomalous magnetic moment of the electron has been measured to 13 significant figures and agrees with theoretical prediction. Cross sections at the Large Hadron Collider are computed to next-to-next-to-leading order and confirmed to percent-level accuracy. If $\BISH+\LPO$ suffices for the Standard Model's predictions, the characterization covers the empirically hardest-tested corner of physics.

Papers~18, 32, 33, and~34 calibrate its four sectors: electroweak theory, QED, QCD, and scattering amplitudes. The result is uniform: the Standard Model lives at $\BISH+\LPO$. Its empirical predictions---the numbers experimentalists compare to data---are $\BISH$ at any fixed order in perturbation theory. $\LPO$ enters only for completed infinite limits: all-orders summation, continuum limits, global coupling existence.

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  row/.style={minimum height=0.7cm, font=\small},
]
% Column headers
\node[font=\small\bfseries] at (-3.5,2.8) {Sector};
\node[font=\small\bfseries, green!60!black] at (0,2.8) {Perturbative ($\BISH$)};
\node[font=\small\bfseries, blue!60] at (4.5,2.8) {Non-perturbative ($\LPO$)};

% Electroweak
\node[row, anchor=east] at (-1.8,2) {Electroweak};
\filldraw[fill=green!12, draw=green!40, rounded corners=2pt] (-1.5,1.7) rectangle (2.5,2.3);
\node[font=\scriptsize] at (0.5,2) {RG step, threshold};
\filldraw[fill=blue!10, draw=blue!30, rounded corners=2pt] (2.7,1.7) rectangle (6.3,2.3);
\node[font=\scriptsize] at (4.5,2) {global running};

% QED
\node[row, anchor=east] at (-1.8,1) {QED};
\filldraw[fill=green!12, draw=green!40, rounded corners=2pt] (-1.5,0.7) rectangle (2.5,1.3);
\node[font=\scriptsize] at (0.5,1) {one-loop, Landau pole};
\filldraw[fill=blue!10, draw=blue!30, rounded corners=2pt] (2.7,0.7) rectangle (6.3,1.3);
\node[font=\scriptsize] at (4.5,1) {all-orders};

% QCD
\node[row, anchor=east] at (-1.8,0) {QCD};
\filldraw[fill=green!12, draw=green!40, rounded corners=2pt] (-1.5,-0.3) rectangle (2.5,0.3);
\node[font=\scriptsize] at (0.5,0) {perturbative sector};
\filldraw[fill=blue!10, draw=blue!30, rounded corners=2pt] (2.7,-0.3) rectangle (6.3,0.3);
\node[font=\scriptsize] at (4.5,0) {confinement, mass gap};

% Scattering amplitudes
\node[row, anchor=east] at (-1.8,-1) {Scattering};
\filldraw[fill=green!15, draw=green!40, rounded corners=2pt] (-1.5,-1.3) rectangle (6.3,-0.7);
\node[font=\scriptsize] at (2.4,-1) {tree-level, one-loop, dim.\ reg., IR cancellation: \textbf{pure} $\BISH$};

% Empirical boundary
\draw[dashed, red, thick] (2.6,2.6) -- (2.6,-0.5);
\node[font=\scriptsize, red, anchor=north] at (2.6,-1.6) {empirical boundary};
\end{tikzpicture}
\caption{Standard Model logical stratification. Every sector is dominantly $\BISH$ (green); $\LPO$ (blue) appears only in non-perturbative completed limits. Scattering amplitudes at fixed order are entirely $\BISH$. The dashed line marks the empirical boundary: everything to the left is what experiments test.}
\label{fig:sm-strat}
\end{figure}

\section{Electroweak Theory (Paper 18)}

The Yukawa coupling between fermions and the Higgs field generates fermion masses through spontaneous symmetry breaking. The one-loop renormalization group equation for the Yukawa coupling $y(\mu)$ involves logarithms and rational functions of known quantities---pure $\BISH$ at any fixed energy scale. The discrete RG step is finite arithmetic.

Threshold decoupling---the physical phenomenon where heavy particles decouple at the scale $\mu = m_f$---involves the Heaviside step function $\Theta(\mu - m_f)$. Deciding whether $\mu$ equals $m_f$ exactly is a zero-test: $\WLPO$. The CKM matrix elements, as completed real numbers characterizing quark mixing at all scales, cost $\LPO$ via the completed limit of the running couplings. The fermion mass hierarchy ($m_t \gg m_e$ by five orders of magnitude) has no additional logical cost---it is an empirical fact about computable constants.

\section{Quantum Electrodynamics (Paper 32)}

The fine structure constant $\alpha(\mu)$ runs with energy scale according to the one-loop beta function. At any fixed scale, computing $\alpha(\mu)$ involves evaluating a logarithm---$\BISH$. The assertion that the running coupling exists globally (defined for all~$\mu$) requires completing the sequence of finite-scale evaluations, which is $\BMC$ ($\equiv \LPO$).

The Landau pole---the energy scale $\Lambda$ at which the one-loop coupling diverges---is $\BISH$, \emph{not} $\LPO$. The reason: the one-loop beta function has an explicit closed-form solution $\Lambda = \mu_0 \exp(3\pi/(\alpha_0 \cdot N_f))$. This is a computable real number requiring no omniscience. Analytic solvability bypasses omniscience; $\LPO$ is the price of \emph{genericity}---asserting convergence without knowing the rate.

Ward identities (gauge invariance constraints) are algebraic identities: verifying them is $\BISH$. The Schwinger anomalous magnetic moment $a_e = \alpha/(2\pi)$---the most precisely confirmed prediction in all of physics, measured to 13 significant figures---is a ratio of computable constants: pure $\BISH$. Higher-order corrections ($\alpha^2$, $\alpha^3$, \ldots) involve finite sums of known integrals at each loop order, each computable. $\LPO$ enters only if one asserts the all-orders convergence of the perturbative series---a claim no experiment tests directly.

\section{Quantum Chromodynamics (Paper 33)}

Perturbative QCD exhibits asymptotic freedom: $\alpha_s(\mu)$ decreases at high energies, opposite to QED. The sign flip in the beta function (due to the non-Abelian gauge group $SU(3)$) does not change the logical structure: the one-loop discrete RG step is $\BISH$, the global coupling trajectory is $\LPO$ via $\BMC$.

Non-perturbative QCD introduces confinement and the mass gap. Finite lattice QCD---computing correlation functions on a finite spacetime grid---is $\BISH$ (finite-dimensional linear algebra). The continuum limit (lattice spacing $a \to 0$) is $\LPO$ via $\BMC$/Fekete. The mass gap assertion $\Delta > 0$ (given $\Delta \ge 0$ and $\neg(\Delta = 0)$) is Markov's Principle, subsumed by $\LPO$.

\emph{Caveat:} The non-perturbative results are conditional on physical axioms. The Yang--Mills existence and mass gap problem (a Clay Millennium Problem) is open. Paper~33 axiomatizes these assumptions as explicit \texttt{axiom} declarations in \Lean\ (Level~4 certification). The calibration says: \emph{if} Yang--Mills theory exists with the required properties, those properties cost at most $\BISH+\LPO$.

\section{Scattering Amplitudes (Paper 34)}

Paper~34 delivered the program's biggest surprise since Paper~29. Tree-level amplitudes are rational functions of Mandelstam variables---pure $\BISH$. After Passarino--Veltman reduction, one-loop integrals reduce to logarithms and dilogarithms ($\mathrm{Li}_2$)---computable special functions, hence $\BISH$. Dimensional regularization is formal Laurent series manipulation in $\varepsilon$---$\BISH$. The Bloch--Nordsieck IR cancellation is algebraic---$\BISH$.

The result is stronger than expected: fixed-order scattering predictions are not merely $\BISH+\LPO$---they are \emph{pure $\BISH$}. No omniscience principle is needed. $\LPO$ enters only when asserting that the perturbation series converges to an all-orders answer, which no experiment tests and which the series (being asymptotic) probably does not support.

\textbf{The sentence for physicists:} every quantity the LHC measures is constructively computable.

\section{Summary}

\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
SM Sector & Perturbative & Non-perturbative & Empirical \\
\midrule
Electroweak & $\BISH$ / $\WLPO$ / $\LPO$ & --- & $\BISH$ \\
QED         & $\BISH$ / $\WLPO$ / $\LPO$ & --- & $\BISH$ \\
QCD         & $\BISH$ / $\WLPO$ / $\LPO$ & $\BISH$ (lattice) / $\LPO$ (cont.) & $\BISH$ \\
Scattering  & $\BISH$ & --- & $\BISH$ \\
\bottomrule
\end{tabular}
\caption{Standard Model calibration summary. The most important column is the last: every experimentally tested prediction is $\BISH$.}
\end{table}

The most precise predictions in all of physics---anomalous magnetic moments, Z-boson mass, scattering cross sections---are pure $\BISH$: finite computations involving computable functions evaluated at computable inputs. Omniscience is needed only for the mathematical framework surrounding these computations, not for the computations themselves.

\begin{chapterbox}
Every Standard Model prediction compared to experimental data is $\BISH$-computable at fixed perturbative order. $\LPO$ enters only for completed limits---all-orders summation, continuum limits---that no experiment directly tests. The most precise predictions in physics are pure finite computation.
\end{chapterbox}

% ======================================================================
%  CHAPTER 6
% ======================================================================
\chapter{The Metatheorem}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  start/.style={ellipse, draw=gray!60, fill=gray!8, minimum width=2.5cm, minimum height=0.7cm, font=\small},
  decision/.style={diamond, draw=blue!50, fill=blue!5, minimum width=2cm, minimum height=1.2cm, font=\scriptsize, align=center, aspect=2.5, inner sep=1pt},
  result/.style={rounded corners=3pt, draw, minimum width=2.2cm, minimum height=0.6cm, font=\small\bfseries, text centered},
  >=Stealth,
]
% Start
\node[start] (S) at (0,3.5) {Physical theorem};

% Decision 1
\node[decision] (D1) at (0,1.8) {Finite composition of\\computable functions?};

% BISH result
\node[result, draw=green!50, fill=green!12] (BISH) at (-4,1.8) {$\BISH$};

% Decision 2
\node[decision] (D2) at (0,-0.3) {Completed limit\\(bounded monotone)?};

% LPO result
\node[result, draw=blue!50, fill=blue!12] (LPO) at (-4,-0.3) {$\LPO$};

% Excluded
\node[result, draw=red!50, fill=red!8] (EX) at (4,-0.3) {Excluded (Ch.~7)};

% Arrows
\draw[->, thick] (S) -- (D1);
\draw[->, thick] (D1) -- (BISH) node[midway, above, font=\scriptsize] {Yes};
\draw[->, thick] (D1) -- (D2) node[midway, right, font=\scriptsize] {No};
\draw[->, thick] (D2) -- (LPO) node[midway, above, font=\scriptsize] {Yes};
\draw[->, thick] (D2) -- (EX) node[midway, above, font=\scriptsize] {No};

% Theorem labels
\node[font=\tiny, green!60!black] at (-4,1.2) {Theorem A};
\node[font=\tiny, blue!60] at (-4,-0.9) {Theorem B};
\node[font=\tiny, red!60] at (4,-0.9) {Theorems C--D};
\end{tikzpicture}
\caption{The metatheorem as a decision flowchart. Every physical theorem follows one of three paths: finite computation ($\BISH$, Theorem~A), completed limit ($\LPO$, Theorem~B), or exclusion (Theorems~C--D). The flowchart has predictive power for new calibrations.}
\label{fig:metatheorem-flow}
\end{figure}

\section{Why the Pattern Holds}

The calibration table (Chapter~3) presents an empirical fact: across 42~physics papers covering every major domain, no calibration exceeds $\LPO$. The hierarchy offers infinitely many levels above $\LPO$---Fan Theorem, Bar Induction, Dependent Choice, full $\LEM$, the Axiom of Choice, large cardinals---yet physics never reaches any of them. A single exception among 42~papers could be a coincidence. Zero exceptions demands an explanation.

Paper~35 provides that explanation: a conservation metatheorem consisting of four sub-theorems that together prove the pattern is a structural consequence of how physical predictions are built. The metatheorem does not merely assert that the calibration holds---it explains \emph{why} it holds, and it predicts that future calibrations will produce the same result. The undecidability arc (Papers~36--39, Chapter~8) provides independent confirmation: even physical \emph{undecidability}---which might have been expected to exceed the $\LPO$ ceiling---calibrates to exactly $\LPO$.

\section{Theorem A: $\BISH$ Conservation}

\begin{theorem}[A: $\BISH$ Conservation]
Any physical prediction expressible as a finite composition of computable functions evaluated at computable inputs is $\BISH$.
\end{theorem}

The proof is straightforward from computable analysis: compositions of computable functions are computable~\cite{Weihrauch2000}. Every function appearing in a physics prediction at finite order is computable: rational functions (polynomial algebra), exponentials and logarithms (Taylor series with computable error bounds), trigonometric functions, dilogarithms and polylogarithms (computable special functions), Bessel functions (power series with computable convergence), hypergeometric functions (ratio test provides computable modulus). The inputs---coupling constants, masses, momenta---are computable real numbers. Therefore the output is computable, hence $\BISH$.

The theorem is ``trivial'' from the computable analysis perspective. Its content is not the proof but the \emph{observation}: physical predictions have the structure of finite compositions of computable functions at computable inputs. Physics could involve non-computable functions (it does not), or non-computable constants (none are known), or operations that destroy computability (such as unrestricted suprema---but physical suprema are always over bounded monotone sequences, handled by $\LPO$, not by Theorem~A).

Theorem~A explains why the vast majority of entries in the calibration table land at $\BISH$: tree-level scattering amplitudes, finite-lattice partition functions, fixed-order perturbative corrections, local differential geometric computations, Born rule probabilities at finite precision, Chebyshev bounds---all are finite compositions of computable functions.

\section{Theorem B: The $\LPO$ Boundary}

\begin{theorem}[B: The $\LPO$ Boundary]
A physical assertion requires $\LPO$ over $\BISH$ if and only if it asserts a completed limit without computable modulus of convergence.
\end{theorem}

Theorem~B consists of three sub-theorems that together classify every physical assertion involving a limit:

\textbf{Sub-theorem B1 (Computable modulus $\to$ $\BISH$).} If a sequence $(a_n)$ converges and its modulus of convergence is computable---there exists a computable function $N(\varepsilon)$ such that $|a_n - L| < \varepsilon$ for all $n \geq N(\varepsilon)$---then the limit $L$ is computable and any prediction depending on $L$ is $\BISH$. This explains why most perturbative calculations are $\BISH$: Taylor series, Fourier series, and iterative solutions typically come with explicit error bounds that provide computable moduli.

\textbf{Sub-theorem B2 (No computable modulus, bounded monotone $\to$ $\LPO$).} If a sequence $(a_n)$ is bounded and monotone but has no computable modulus of convergence, then asserting convergence is $\BMC \equiv \LPO$ by Paper~29's Fekete encoding. This is the mechanism behind \emph{every} $\LPO$ entry: thermodynamic limits, continuum limits, and global coupling existence all involve bounded monotone sequences whose convergence rates are not uniformly computable.

\textbf{Sub-theorem B3 (Limit comparison $\to$ $\WLPO$).} Deciding whether a limit equals a specific value---$M = 0$ (paramagnetic) versus $M > 0$ (ferromagnetic)---is a zero-test. This is $\WLPO$, subsumed by $\LPO$. The proof encodes a binary sequence $\alpha$ into a convergent sequence whose limit is $0$ iff all terms of $\alpha$ are zero.

Together, B1--B3 provide a complete classification: a physical assertion involving a limit is $\BISH$ if the modulus is computable, $\LPO$ if the sequence is bounded and monotone without computable modulus, and $\WLPO$ if the assertion is a zero-test on a limit. No physical assertion in the calibration exceeds these three cases.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[
  dec/.style={diamond, draw=gray!60, fill=gray!5, aspect=2.2, inner sep=1pt, font=\small, align=center},
  res/.style={rectangle, rounded corners=3pt, minimum width=3cm, minimum height=0.8cm, font=\small, align=center},
  arr/.style={->, thick, >=Stealth},
  elbl/.style={font=\scriptsize\itshape, text width=3.2cm, align=center, gray!60},
]
% Start
\node[rectangle, rounded corners=3pt, draw, fill=yellow!10, font=\small\bfseries] (start) at (0,0) {Physical limit};

% Decision 1: computable modulus?
\node[dec] (d1) at (0,-1.8) {Computable\\modulus?};
\draw[arr] (start) -- (d1);

% Yes -> BISH
\node[res, draw=green!60!black, fill=green!12] (bish) at (5,-1.8) {$\BISH$ (B1)};
\draw[arr, green!60!black] (d1) -- node[above, font=\scriptsize] {Yes} (bish);
\node[elbl] at (5,-2.8) {perturbative series,\\Fourier sums};

% Decision 2: bounded monotone?
\node[dec] (d2) at (0,-4.0) {Bounded\\monotone?};
\draw[arr] (d1) -- node[left, font=\scriptsize] {No} (d2);

% Yes -> LPO
\node[res, draw=blue!60, fill=blue!10] (lpo) at (5,-4.0) {$\LPO$ (B2)};
\draw[arr, blue!60] (d2) -- node[above, font=\scriptsize] {Yes} (lpo);
\node[elbl] at (5,-5.0) {thermodynamic limits,\\continuum limits};

% Decision 3: zero-test?
\node[dec] (d3) at (0,-6.2) {Zero-test\\on limit?};
\draw[arr] (d2) -- node[left, font=\scriptsize] {No} (d3);

% Yes -> WLPO
\node[res, draw=cyan!60!black, fill=cyan!10] (wlpo) at (5,-6.2) {$\WLPO$ (B3)};
\draw[arr, cyan!60!black] (d3) -- node[above, font=\scriptsize] {Yes} (wlpo);
\node[elbl] at (5,-7.2) {$M = 0$?, mass gap $= 0$?,\\horizon at $r = 2M$?};

% Subsumption arrow
\draw[->, dashed, gray, thick] (wlpo.north) -- (lpo.south) node[midway, right=3pt, font=\scriptsize, gray] {subsumed};
\end{tikzpicture}
\caption{Theorem~B triage: every physical limit is classified by the convergence properties of the underlying sequence. Computable modulus gives $\BISH$; bounded monotone without computable modulus gives $\LPO$; a zero-test on the limit gives $\WLPO$ (subsumed by $\LPO$). No calibrated physical assertion exceeds these three cases.}
\label{fig:limit-triage}
\end{figure}

\section{Theorem C: Exhaustiveness}

\begin{theorem}[C: Exhaustiveness]
Every calibration result in Papers~2--42 is an instance of Theorem~A ($\BISH$) or Theorem~B ($\WLPO$/$\LLPO$/$\LPO$).
\end{theorem}

Theorem~C is an audit, not a proof. The Lean formalization encodes each result as a classified instance---tagged with its position in the hierarchy---and verifies that the tagging is consistent with the axiom profile. The classification is exhaustive: every theorem in every paper falls into one of the categories ($\BISH$ via computable composition, $\LPO$ via bounded monotone limit, or $\WLPO$/$\LLPO$ via zero-test/sign-decision).

Theorem~C's strength is cumulative. A single paper calibrating to $\LPO$ could be coincidence. Forty-two papers across seven physics domains---statistical mechanics, quantum mechanics, quantum field theory, general relativity, quantum information, classical mechanics, and holographic duality---all calibrating to $\BISH+\LPO$ is a pattern demanding explanation. Theorems~A and~B provide the explanation; Theorem~C certifies that the explanation is exhaustive.

\section{Theorem D: Three Mechanisms}

\begin{theorem}[D: Three Mechanisms]
Every instance of $\LPO$ in the program arises from one of three equivalent mechanisms: Bounded Monotone Convergence, Cauchy completeness without modulus, or supremum existence.
\end{theorem}

The three mechanisms are: \textbf{(M1)} Bounded Monotone Convergence---asserting that a bounded monotone sequence converges; \textbf{(M2)} Cauchy completeness without computable modulus---asserting that a Cauchy sequence has a limit when the modulus of convergence is not computable; \textbf{(M3)} Supremum existence---asserting that a bounded set of reals has a least upper bound. The equivalences $\text{M1} \Leftrightarrow \text{M2} \Leftrightarrow \text{M3}$ over $\BISH$ are standard results in constructive analysis (Bridges and Richman 1987, Ishihara 2006). The theorem's content is the observation that every $\LPO$ instance in the calibration arises from one of these mechanisms: phase transitions via $\BMC$ (M1), continuum limits via Cauchy completeness (M2), and variational bounds via supremum existence (M3).

The sub-$\LPO$ entries follow the same pattern. Every $\WLPO$ entry arises from the \emph{zero-test mechanism}: deciding whether a computed quantity equals zero (bidual gap, threshold decoupling, magnetization vanishing). Every $\LLPO$ entry arises from the \emph{sign-decision mechanism}: deciding the sign of a computed quantity (Bell/CHSH inequality violation, WKB tunneling direction, intermediate value theorem applications). The three-mechanism classification has predictive power: when calibrating a new theorem, one determines its height by identifying which mechanism it invokes.

\section{The Five Steps of a Physicist}

The metatheorem can be restated as an observation about the practice of theoretical physics. A physicist making a prediction follows five steps, each mapping to a specific location in the hierarchy:
\begin{enumerate}[nosep]
\item \textbf{Write a Lagrangian.} $\mathcal{L} = \bar\psi(i\gamma^\mu\partial_\mu - m)\psi - \tfrac{1}{4}F^{\mu\nu}F_{\mu\nu} + \cdots$. Polynomial algebra: $\BISH$.
\item \textbf{Derive equations of motion.} Euler--Lagrange equations via formal differentiation: $\BISH$.
\item \textbf{Solve the equations.} At finite order in perturbation theory, the solution is a finite combination of computable functions (exponentials, logarithms, Bessel functions, hypergeometric functions): $\BISH$ by Theorem~A.
\item \textbf{Take a limit if needed.} Thermodynamic limit ($N \to \infty$), continuum limit ($a \to 0$), or all-orders summation. If the convergence rate is not computable: $\BMC \equiv \LPO$ by Theorem~B2.
\item \textbf{Compare to experiment.} Finite-precision arithmetic---computing $|\text{prediction} - \text{measurement}|$ and checking error bars: $\BISH$.
\end{enumerate}

$\LPO$ enters only at Step~4, and only when the limit lacks a computable modulus. Steps~1, 2, 3, and~5 are always $\BISH$. This is why the characterization holds: the structure of physics-as-practiced---Lagrangian $\to$ equations $\to$ solutions $\to$ limits $\to$ experiment---maps onto the $\BISH$/$\LPO$ boundary with $\LPO$ entering at exactly one point.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[
  stp/.style={rectangle, rounded corners=4pt, minimum height=1cm, font=\small, align=center, text width=1.8cm},
  arr/.style={->, thick, >=Stealth, gray!60},
]
% Five steps
\node[stp, draw=green!60!black, fill=green!12] (s1) at (0,0)    {1.\\Write\\Lagrangian};
\node[stp, draw=green!60!black, fill=green!12] (s2) at (2.8,0)  {2.\\Derive\\equations};
\node[stp, draw=green!60!black, fill=green!12] (s3) at (5.6,0)  {3.\\Solve at\\finite order};
\node[stp, draw=blue!70, fill=blue!12, line width=1.5pt, minimum height=1.15cm] (s4) at (8.4,0)  {4.\\Take a\\limit};
\node[stp, draw=green!60!black, fill=green!12] (s5) at (11.2,0) {5.\\Compare to\\experiment};

% Arrows
\draw[arr] (s1) -- (s2);
\draw[arr] (s2) -- (s3);
\draw[arr] (s3) -- (s4);
\draw[arr] (s4) -- (s5);

% Labels above
\node[font=\scriptsize, green!50!black] at (0,0.95) {$\BISH$};
\node[font=\scriptsize, green!50!black] at (2.8,0.95) {$\BISH$};
\node[font=\scriptsize, green!50!black] at (5.6,0.95) {$\BISH$};
\node[font=\scriptsize\bfseries, blue!70] at (8.4,1.05) {$\LPO$};
\node[font=\scriptsize, green!50!black] at (11.2,0.95) {$\BISH$};

% Annotation below Step 4
\node[font=\scriptsize\itshape, blue!60, text width=2.5cm, align=center] at (8.4,-1.2) {Only entry point for\\non-constructivity};

% Green brace under Steps 1,2,3,5
\draw[decorate, decoration={brace, amplitude=5pt, mirror}, green!50!black]
  (-0.9,-0.8) -- (6.5,-0.8) node[midway, below=7pt, font=\scriptsize, green!50!black] {finite, computable};
\draw[decorate, decoration={brace, amplitude=5pt, mirror}, green!50!black]
  (10.3,-0.8) -- (12.1,-0.8) node[midway, below=7pt, font=\scriptsize, green!50!black] {finite};
\end{tikzpicture}
\caption{The five steps of a physicist as a pipeline. Four steps are $\BISH$ (finite, computable); $\LPO$ enters at exactly one point---the completed limit of Step~4. This is why the $\BISH+\LPO$ characterization holds.}
\label{fig:five-steps}
\end{figure}

The five-step structure also explains why empirical predictions are often \emph{more} constructive than the mathematical framework suggests. A physicist computing a cross section at one loop (Step~3) never reaches Step~4. The $\LPO$-level claims---``the perturbation series converges,'' ``the continuum limit exists''---are assertions \emph{about} the mathematical framework, not predictions compared to experiment. The metatheorem identifies this gap between framework and prediction as the $\BISH$/$\LPO$ boundary.

\begin{chapterbox}
The metatheorem explains why the $\BISH+\LPO$ pattern holds: empirical predictions are finite compositions of computable functions ($\BISH$); the only idealization exceeding finite computation is the completed limit ($\LPO$ via Bounded Monotone Convergence). $\LPO$ enters physics at exactly one point in the five-step workflow: the limit step.
\end{chapterbox}

% ======================================================================
%  CHAPTER 7
% ======================================================================
\chapter{The Boundary --- What Physics Cannot Do}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  >=Stealth,
]
% Accessible region
\filldraw[fill=green!8, draw=green!40, rounded corners=3pt] (-5.5,-2.5) rectangle (-0.3,2.5);
\node[font=\small\bfseries, green!60!black] at (-2.9,2.1) {Empirically accessible};
\node[font=\scriptsize, anchor=west] at (-5.2,1.3) {$\BISH$: finite computation};
\node[font=\scriptsize, anchor=west] at (-5.2,0.6) {$\LLPO$: real comparison};
\node[font=\scriptsize, anchor=west] at (-5.2,-0.1) {$\WLPO$: zero-testing};
\node[font=\scriptsize, anchor=west] at (-5.2,-0.8) {$\LPO$: completed limits};

% Barrier
\fill[red!15] (-0.15,-2.5) rectangle (0.15,2.5);
\draw[red, line width=2pt] (0,-2.5) -- (0,2.5);
\node[font=\scriptsize\bfseries, red, rotate=90] at (0.5,0) {Empirical boundary};

% Inaccessible region
\filldraw[fill=gray!8, draw=gray!40, rounded corners=3pt] (0.7,-2.5) rectangle (5.5,2.5);
\node[font=\small\bfseries, gray!70] at (3.1,2.1) {Inaccessible to physics};
\node[font=\scriptsize, anchor=west] at (1.0,1.3) {1.\ General convergence testing};
\node[font=\scriptsize, anchor=west] at (1.0,0.6) {2.\ Non-$\Delta^0_2$ real numbers};
\node[font=\scriptsize, anchor=west] at (1.0,-0.1) {3.\ Tree-search principles ($\FT$, $\BI$)};
\node[font=\scriptsize, anchor=west] at (1.0,-0.8) {4.\ Set-theoretic combinatorics};
\node[font=\scriptsize, anchor=west] at (1.0,-1.5) {5.\ Full classical logic ($\LEM$)};
\end{tikzpicture}
\caption{The exclusion zone. Physics accesses $\BISH$ through $\LPO$ (left). Five classes of mathematical principles lie beyond the empirical boundary (right). Each exclusion yields a falsifiable prediction.}
\label{fig:exclusion}
\end{figure}

\section{The Arithmetic Hierarchy}

The arithmetic hierarchy classifies mathematical statements by quantifier complexity. At the base level, a \textbf{decidable predicate} $P(n)$ is one for which an algorithm determines truth or falsity in finite time. The simplest non-trivial statements are $\Sigma^0_1$: ``there exists $n$ such that $P(n)$''---a single existential quantifier over a decidable predicate. Dually, $\Pi^0_1$ statements have the form ``for all $n$, $P(n)$.'' $\LPO$ is precisely $\Sigma^0_1$-$\LEM$: the principle that every $\Sigma^0_1$ statement is either true or false.

The next level introduces $\Sigma^0_2$: ``there exists $n$ such that for all $m$, $Q(n,m)$''---two quantifier alternations. The statement ``sequence $(a_n)$ converges'' is $\Pi^0_2$: $\forall \varepsilon\;\exists N\;\forall n,m > N,\; |a_n - a_m| < \varepsilon$. This is why general convergence testing is beyond $\LPO$. The placement:
\begin{center}
\begin{tabular}{@{}ll@{}}
$\Sigma^0_1\colon\; \exists n\; P(n)$ & --- $\LPO$ decides this \\
$\Pi^0_1\colon\; \forall n\; P(n)$ & --- $\LPO$ decides this \\
$\Sigma^0_2\colon\; \exists n\;\forall m\; Q(n,m)$ & --- \textbf{beyond} $\LPO$ \\
$\Pi^0_2\colon\; \forall n\;\exists m\; Q(n,m)$ & --- \textbf{beyond} $\LPO$ \\
Full $\LEM$ & --- incomparably beyond $\LPO$
\end{tabular}
\end{center}

The physical significance is direct. $\LPO$ decides $\Sigma^0_1$ statements---exactly the thermodynamic limit. Everything beyond $\Sigma^0_1$---general convergence testing ($\Pi^0_2$), the finiteness problem ($\Sigma^0_2$), set-theoretic combinatorics (far beyond)---is excluded from empirical predictions. The arithmetic hierarchy provides a precise ruler for measuring logical complexity, and physics uses exactly one notch on that ruler.

\section{Exclusion 1: General Convergence Testing}

The statement ``the sequence $(a_n)$ converges'' is $\Pi^0_2$: it involves two quantifier alternations ($\forall\varepsilon\;\exists N$ followed by $\forall n,m$), placing it strictly beyond $\LPO$. Nature can complete specific classes of limits---$\BMC$ handles bounded monotone sequences, which is exactly $\LPO$---but nature cannot decide general convergence.

The physical consequence is concrete. No experiment can determine whether an arbitrary physical process converges to a steady state. A physicist can observe that measurements are getting closer together, but ``this process converges'' is $\Pi^0_2$. What physics \emph{can} do is assert convergence for bounded monotone processes ($\LPO$/$\BMC$), which covers thermodynamic limits, phase transitions, and continuum limits. This exclusion has a counterintuitive consequence for dynamical systems: the long-time behavior of a generic system---fixed point, periodic orbit, or chaos---is beyond $\BISH+\LPO$ in full generality. Physics handles this by studying specific systems whose dynamics are constrained (dissipative systems converge by $\BMC$, Hamiltonian systems conserve energy by $\BISH$).

\section{Exclusion 2: Non-$\Delta^0_2$ Real Numbers}

The real numbers accessible to $\BISH+\LPO$ are exactly the $\Delta^0_2$ reals---the limit-computable reals. A real $x$ is $\Delta^0_2$ if there exists a computable sequence of rationals $(q_n)$ converging to $x$, though the rate need not be computable. This includes $\pi$, $e$, Euler--Mascheroni $\gamma$, Chaitin's $\Omega$---but not all reals.

The physical prediction: every physical constant---$\alpha$, $m_e$, $\Lambda$, $G$---is $\Delta^0_2$. No physical constant can encode a $\Sigma^0_2$-complete problem. This constrains fine-tuning arguments: the physically relevant sample space is the countable class $\Delta^0_2$, not the uncountable continuum $\RR$. Fine-tuning debates implicitly assume constants are drawn from the full continuum---an unnecessarily large sample space.

\section{Exclusion 3: Tree-Search Principles}

The Fan Theorem and Bar Induction are logically independent of $\LPO$---neither implies the other over $\BISH$. Their exclusion means nature does not perform unrestricted tree searches. $\FT$ asserts that every bar of the binary fan is uniform; $\BI$ asserts well-founded induction for certain tree classes. Both involve navigating branching structures of potentially unbounded depth. Physical processes search bounded trees ($\BISH$) and complete monotone limits ($\LPO$), but do not navigate arbitrary well-founded trees.

The dispensability of $\FT$ (Paper~30) provides evidence: every physical prediction derived via compactness can be re-derived without it. Nature solves optimization problems by satisfying equations ($\BISH$), not by searching over compact sets ($\FT$). Nature takes thermodynamic limits by monotone convergence ($\LPO$), not by extracting convergent subsequences ($\FT$).

\section{Exclusion 4: Set-Theoretic Combinatorics}

$\BISH+\LPO$ is an arithmetic theory---its assertions concern natural numbers, real numbers, and functions between them. The Continuum Hypothesis, large cardinal axioms, the full Axiom of Choice, projective determinacy---the entire landscape of modern set theory---is physically meaningless. No experiment can distinguish a CH-universe from a $\neg$CH-universe. No scattering amplitude depends on inaccessible cardinals.

The relationship is analogous to using a dictionary: the Oxford English Dictionary contains 170{,}000 entries, but a grocery list uses perhaps 30~words. Physics is the grocery list; $\ZFC$ is the dictionary. The full $\AC$ is used routinely in functional analysis (Hamel bases, full Hahn--Banach, non-measurable sets), but the program's calibration shows every empirical prediction can be re-derived without $\AC$, using at most $\BISH+\LPO$.

\section{Exclusion 5: Full Classical Logic}

$\LPO$ is $\Sigma^0_1$-$\LEM$---the law of excluded middle restricted to existential statements over decidable predicates. It is not full $\LEM$. The gap is enormous: the difference between deciding one quantifier alternation and deciding arbitrary logical complexity.

The measurement problem illustrates this vividly. The double-slit experiment asks: ``did the electron go through slit A or slit~B?'' This is $\Sigma^0_1$ (a decision about a detector event) and is $\LPO$-decidable. But the collapse question---``did the quantum state transition from a superposition to an eigenstate?''---is a proposition about an infinite-dimensional Hilbert space, and $\BISH+\LPO$ does not validate $\LEM$ for such propositions. The interpretations of quantum mechanics differ precisely on whether to apply $\LEM$ to these higher-complexity propositions: Copenhagen applies it, Many-Worlds refuses it, and the empirical predictions are identical because they depend only on Born rule probabilities ($\BISH$/$\LPO$).

\section{Falsifiability}

The characterization makes five concrete, testable predictions:
\begin{enumerate}[nosep]
\item No scattering amplitude at any loop order will require deciding a $\Sigma^0_2$ statement.
\item No physical constant will be shown to be non-$\Delta^0_2$.
\item No empirical prediction will require $\FT$ or $\BI$.
\item No empirical prediction will require full $\AC$ or any set-theoretic axiom beyond arithmetic.
\item No finite-precision measurement will require full $\LEM$ (as opposed to $\Sigma^0_1$-$\LEM$).
\end{enumerate}
Each prediction is falsifiable: a single counterexample refutes the characterization. Paper~39 shows that generic intensive observables \emph{without} promise gap can reach $\Sigma^0_2$---but this does not refute the characterization. The empirical ceiling ($\LPO$) is intact because finite experimental precision enforces an effective promise gap that collapses $\Sigma^0_2$ back to $\Sigma^0_1$. The program invites falsification: a laboratory measurement genuinely requiring $\Sigma^0_2$ reasoning would be more interesting than confirmation.

\begin{chapterbox}
Physics cannot access $\Sigma^0_2$ statements, general convergence testing, non-$\Delta^0_2$ real numbers, tree-search principles ($\FT$, $\BI$), or full classical logic. The arithmetic hierarchy provides a precise ruler: physics uses exactly one notch ($\Sigma^0_1 = \LPO$) and no further. Each exclusion yields a falsifiable prediction.
\end{chapterbox}

% ======================================================================
%  CHAPTER 8
% ======================================================================
\chapter{The Genealogy of Physical Undecidability}

\section{Overview}

In 2015, Cubitt, Perez-Garcia, and Wolf~\cite{Cubitt2015} proved that the spectral gap problem is undecidable---a Hamiltonian encoding a Turing machine can be constructed such that whether the system is gapped or gapless is as hard as the halting problem. This was widely interpreted as revealing a fundamental limit to physical knowledge.

The program faced a question: does Cubitt's undecidability break the $\LPO$ ceiling? Papers~36--38 answer: no. Every known physical undecidability result is Turing--Weihrauch equivalent to $\LPO$. ``Undecidability'' in physics is the non-computability of $\LPO$---the same principle governing thermodynamic limits since Boltzmann. Physical undecidability adds zero new logical resources to the program's thesis. Paper~39 then asks the deeper question: is $\LPO$ really a hard ceiling, or are there observables beyond it?

\section{Cubitt's Theorem Is $\LPO$ (Paper 36)}

The stratification of Cubitt--Perez-Garcia--Wolf reveals five layers, corresponding to five theorems in Paper~36:

{\small
\begin{center}
\begin{tabular}{@{}lp{4.5cm}p{4.5cm}l@{}}
\toprule
\textbf{Level} & \textbf{Content} & \textbf{Mechanism} & \textbf{Thm} \\
\midrule
$\BISH$ & Finite-volume spectral gap $\Delta_L$ & Algebraic eigenvalue computation & 1 \\
$\WLPO$ & ``$\Delta = 0$ or $\Delta > 0$?'' & Zero-test on completed real & 4 \\
$\LPO$ & Thermodynamic limit $\Delta = \lim \Delta_L$ & Conditional BMC via branching & 2 \\
$\LPO$ & Each instance: ``is $H(M)$ gapped?'' & $\Sigma^0_1$ decision for specific $M$ & 3 \\
Non-comp. & Uniform $M \mapsto$ gapped/gapless & Halting $=$ $\LPO$ non-computability & 5 \\
\bottomrule
\end{tabular}
\end{center}}

The finite-volume spectral gap is the energy difference between ground state and first excited state of a finite-dimensional Hamiltonian---finite linear algebra, pure $\BISH$. Each specific instance---``is $H(M)$ gapped for this particular Turing machine $M$?''---reduces to ``does $M$ halt?'', a $\Sigma^0_1$ decision decidable by $\LPO$. The undecidability arises only for the \emph{uniform} question: ``given arbitrary $M$, is $H(M)$ gapped?'' This is the halting problem, non-computable---but its non-computability is exactly the non-computability of $\LPO$ applied to arbitrary inputs.

The central result is Theorem~2, the biconditional between the thermodynamic limit and $\LPO$ (\cref{fig:cubitt-pipeline}). In the forward direction: given a binary sequence~$\alpha$, one constructs a Turing machine $M_\alpha$ that halts if and only if $\exists n\, \alpha(n)=1$. The CPgW construction maps $M_\alpha$ to a Hamiltonian $H(M_\alpha)$ whose spectral gap satisfies a \emph{promise dichotomy}: $\Delta \in \{0\} \cup [\gamma, \infty)$. This dichotomy decides halting, hence yields $\LPO$. In the reverse direction: given $\LPO$, one applies a single oracle call to the halting sequence of~$M$, branching into two cases. In the halting case, the CPgW asymptotics guarantee that finite-volume gaps converge to zero at a computable rate; in the non-halting case, the gaps stabilize above~$\gamma$ with a computable Cauchy modulus. Crucially, the finite-volume gap sequence $(\Delta_L)_{L \geq 1}$ is \emph{not monotone}---gaps fluctuate before the halting time. Unlike Paper~29's direct Fekete/$\BMC$ application, one cannot invoke $\BMC$ directly. $\LPO$ resolves halting \emph{first}; only then does monotonicity emerge within each branch. This ``conditional constructivity''---convergence provable in each branch but no uniform modulus across branches without $\LPO$---is a fundamentally different proof architecture from the direct $\BMC$ equivalence governing phase transitions (Paper~29).

The Lean formalization axiomatizes seven properties of the CPgW construction as \emph{bridge lemmas}: encoding computability, the halting-gap equivalence in both directions, asymptotic convergence rates for halting and non-halting branches, the promise gap dichotomy, and the binary-sequence-to-Turing-machine encoding. These seven axioms express precisely what the CRM analysis needs from the CPgW construction---nothing about the internal aperiodic tiling mechanism, only the input-output behaviour of the encoding and the dichotomous convergence structure. Everything above the bridge (the five-theorem stratification) is machine-checked in 655~lines of Lean~4 with zero \texttt{sorry}; everything below (the 140-page CPgW construction) is physics whose full formalization would require a substantial independent effort.

The physical punchline: the spectral gap is undecidable for the same reason phase transitions cost $\LPO$. Both require completing a thermodynamic limit. Cubitt's celebrated result reveals not a new form of unknowability but the familiar face of $\LPO$.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[
  nd/.style={rectangle, rounded corners=3pt, draw, minimum height=0.75cm,
    align=center, font=\small, inner sep=4pt},
  dec/.style={diamond, draw=blue!60, fill=blue!20, aspect=2,
    inner sep=2pt, font=\small, align=center},
  bish/.style={fill=green!15, draw=green!60!black},
  lpo/.style={fill=blue!15, draw=blue!60},
  infra/.style={fill=gray!8, draw=gray!50},
  arr/.style={->, thick, >=Stealth, gray!60},
  ann/.style={font=\scriptsize, text=black!70},
]
% Row 1: encoding chain
\node[nd, infra] (alpha) at (0,0) {$\alpha : \mathbb{N} \to \{0,1\}$};
\node[nd, infra] (M) at (3.8,0) {$M_\alpha$};
\node[nd, bish]  (H) at (7.2,0) {$H(M_\alpha)$};
\node[nd, bish]  (DL) at (10.8,0) {$\Delta_L$};

\draw[arr] (alpha) -- (M) node[midway, above, ann] {encode};
\draw[arr] (M) -- (H) node[midway, above, ann] {CPgW};
\draw[arr] (H) -- (DL) node[midway, above, ann] {Thm~1};

% Non-monotone zigzag annotation
\draw[red!70, thick, decorate,
  decoration={zigzag, amplitude=1.5pt, segment length=4pt}]
  (10.8,-0.65) -- (10.8,-1.35);
\node[red!70, font=\scriptsize\bfseries, right] at (11.0,-1.0) {not monotone!};

% LPO decision point
\node[dec] (lpo) at (7.2,-2.2) {$\LPO$: halts?};
\draw[arr] (DL) -- (10.8,-1.5) -- (7.2,-1.5) -- (lpo)
  node[pos=0.5, above, ann] {$L \to \infty$ (Thm~2)};

% Branches
\node[nd, lpo, text width=2.8cm] (halt) at (3.0,-3.8)
  {$\exists n$: halts\\$\Delta \to 0$\\{\scriptsize (gapless)}};
\node[nd, lpo, text width=2.8cm] (nohalt) at (11.4,-3.8)
  {$\forall n$: no halt\\$\Delta \geq \gamma$\\{\scriptsize (gapped)}};

\draw[arr] (lpo) -- (halt) node[midway, above left, ann] {halting asymptotics};
\draw[arr] (lpo) -- (nohalt) node[midway, above right, ann] {non-halting asymptotics};

% Cauchy modulus annotations
\node[green!50!black, font=\scriptsize] at (3.0,-4.8) {Cauchy modulus: computable rate};
\node[green!50!black, font=\scriptsize] at (11.4,-4.8) {Cauchy modulus: gap stabilises};

% CRM level labels on right
\node[green!50!black, font=\scriptsize\bfseries] at (13.2,0) {$\BISH$};
\node[blue!60, font=\scriptsize\bfseries] at (13.2,-2.2) {$\LPO$};

% Bridge axiom sidebar
\node[font=\scriptsize, text width=2.6cm, align=left,
  draw=gray!40, fill=gray!4, rounded corners=2pt, inner sep=4pt]
  (bridge) at (-2.2,-2.8) {%
  \textbf{Bridge axioms:}\\[2pt]
  1.~encoding\\
  2.~gapped if $\neg$halt\\
  3.~gapless if halt\\
  4.~halt asymptotics\\
  5.~no-halt asymptotics\\
  6.~gap dichotomy\\
  7.~\texttt{tm\_from\_seq}};
\draw[gray!40, dashed, thin] (bridge.north east) -- (M.south west);
\draw[gray!40, dashed, thin] (bridge.east) -- (lpo.west);
\end{tikzpicture}
\caption{The Cubitt--$\LPO$ reduction pipeline. A binary sequence $\alpha$ encodes into a Turing machine~$M_\alpha$, then into a Hamiltonian $H(M_\alpha)$ ($\BISH$). Finite-volume gaps $\Delta_L$ are computable (Theorem~1, green). The sequence $(\Delta_L)$ is not monotone---$\BMC$/Fekete cannot apply directly. $\LPO$ resolves halting first (Theorem~2, blue), producing a Cauchy modulus in each branch. Seven CPgW bridge lemmas (left sidebar) form the physics--logic interface.}
\label{fig:cubitt-pipeline}
\end{figure}

\section{The Undecidability Landscape (Paper 37)}

Paper~37 proves universality: any undecidability result in physics obtained by computable many-one reduction from the halting problem is Turing--Weihrauch equivalent to $\LPO$. Three further results are explicitly stratified:
\begin{enumerate}[nosep]
\item \textbf{Phase diagram uncomputability} (Bausch--Cubitt--Watson~\cite{BauschCubittWatson2021}): the phase boundary is where a thermodynamic potential is non-analytic, requiring a completed limit ($\LPO$).
\item \textbf{1D spectral gap undecidability} (Bausch--Cubitt--Lucia--Perez-Garcia~\cite{BauschCubittLucia2020}): more sophisticated tiling construction, but dimension does not change the logical structure. Still $\LPO$.
\item \textbf{Uncomputable RG flows} (Cubitt--Lucia--Perez-Garcia--Perez-Eceiza 2022): RG fixed points are completed limits of iterative coarse-graining. $\LPO$.
\end{enumerate}

The meta-theorem is structurally simple. The halting problem is $\Sigma^0_1$-complete. $\LPO$ decides all $\Sigma^0_1$ statements. Any computable reduction preserves the classification: $\Sigma^0_1$ input + computable reduction = $\Sigma^0_1$ output. Therefore every such undecidability result lands at exactly $\LPO$.

A notable exception establishes an important distinction: the ground state energy density (Watson--Cubitt 2021) is $\BISH$---computationally \emph{hard} (exponential time) but logically \emph{decidable} (computable). Computational complexity and logical undecidability are fundamentally different. Paper~37: 660~lines of Lean~4, zero \texttt{sorry}.

\section{Wang Tiling --- The Grandfather (Paper 38)}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  nd/.style={rectangle, rounded corners=3pt, draw=black!50, fill=white,
    text width=4.8cm, align=center, font=\small, minimum height=0.7cm},
  lp/.style={rectangle, rounded corners=2pt, draw=red!60, fill=red!8,
    font=\scriptsize\bfseries, inner sep=2pt},
  edge/.style={->,thick,>=stealth,black!50}
]
\node[nd] (W) at (0,0) {Wang Tiling (1961)};
\node[lp,right=3pt of W] {$= \LPO$};

\node[nd] (Be) at (0,-1.2) {Berger (1966): aperiodic tileset};
\node[nd] (Ro) at (0,-2.4) {Robinson (1971): simplified};
\node[nd] (Ka) at (-3,-3.6) {Kanter (1990): Potts model};
\node[lp,right=3pt of Ka] {$= \LPO$};
\node[nd] (Gu) at (-3,-4.8) {Gu--Weedbrook et al.\ (2009): 2D Ising};
\node[lp,right=3pt of Gu] {$= \LPO$};
\node[nd] (Cu) at (0,-6.0) {Cubitt--Perez-Garcia--Wolf (2015): spectral gap};
\node[lp,right=3pt of Cu] {$= \LPO$};

\node[nd,font=\scriptsize,text width=3.2cm] (B1) at (-4.5,-7.3) {Bausch et al.\ (2020):\\1D spectral gap};
\node[lp,below=1pt of B1] {$= \LPO$};
\node[nd,font=\scriptsize,text width=3.2cm] (B2) at (0,-7.3) {Bausch--Cubitt--Watson\\(2021): phase diagrams};
\node[lp,below=1pt of B2] {$= \LPO$};
\node[nd,font=\scriptsize,text width=3.2cm] (B3) at (4.5,-7.3) {Cubitt--Lucia et al.\\(2022): RG flows};
\node[lp,below=1pt of B3] {$= \LPO$};

\draw[edge] (W) -- (Be);
\draw[edge] (Be) -- (Ro);
\draw[edge] (Ro) -- (Ka);
\draw[edge] (Ka) -- (Gu);
\draw[edge] (Ro) -- (Cu);
\draw[edge] (Gu) -- (Cu);
\draw[edge] (Cu) -- (B1);
\draw[edge] (Cu) -- (B2);
\draw[edge] (Cu) -- (B3);

% Vertical bar
\draw[red!60,very thick] (6,-0.3) -- (6,-8.5);
\node[red!60,font=\small\bfseries,rotate=90] at (6.6,-4) {ALL $= \LPO$};
\end{tikzpicture}
\caption{The genealogy of physical undecidability. Every result descends from Wang tiling and inherits exactly $\LPO$---nothing more, nothing less.}
\label{fig:genealogy}
\end{figure}

Every undecidability result in quantum many-body physics descends from a single ancestor: the undecidability of Wang tiling. Wang~\cite{Wang1961} conjectured that if a finite set of tiles can tile the plane, it can do so periodically. Berger~\cite{Berger1966} disproved this by constructing an aperiodic tileset and proved the general tiling problem undecidable. Robinson~\cite{Robinson1971} simplified the construction. The genealogy then branches into physics (\cref{fig:genealogy}).

Paper~38 proves that Wang tiling itself---the ancestor of the entire genealogy---is Turing--Weihrauch equivalent to $\LPO$. The decision problem ``does finite tileset $T$ tile the plane?'' is $\Sigma^0_1$-complete, hence $\LPO$-equivalent. The $\Sigma^0_1$ Ceiling Theorem completes the analysis: to exceed $\LPO$---to reach $\Sigma^0_2$---would require encoding a $\Sigma^0_2$-complete problem (such as the finiteness problem: ``does $M$ halt on infinitely many inputs?''). No existing construction in quantum many-body physics achieves this. Paper~39 will construct exactly such an encoding---but the resulting observable is empirically inaccessible. Paper~38: 573~lines of Lean~4, zero \texttt{sorry}.

\section{What Undecidability Actually Is}

Papers~36--38 together establish a single conclusion: physical undecidability is $\LPO$'s non-computability, inherited from the thermodynamic limit, traceable to Wang tiling.

This is a reclassification. Before this program, spectral gap undecidability was understood as: ``there exist Hamiltonians whose gapped/gapless nature cannot be determined by any algorithm.'' The reclassification: ``the thermodynamic limit is non-computable when applied uniformly; Cubitt et al.\ embedded the halting problem into this non-computability; the result is the non-computability of $\LPO$.'' All physical undecidability is one phenomenon wearing different costumes. The spectral gap, phase diagrams, RG flows, and 1D spectral gap are all instances of the same logical event: encoding a halting problem into a $\Sigma^0_1$ decision via a computable reduction. The diversity of physical contexts masks the uniformity of the logical mechanism.

\begin{chapterbox}
Every known physical undecidability result---spectral gap, phase diagrams, RG flows---is Turing--Weihrauch equivalent to $\LPO$, traceable to a single ancestor: Wang tiling (1961). The most undecidable thing in physics is exactly as undecidable as a boiling pot of water.
\end{chapterbox}

% ======================================================================
%  CHAPTER 9
% ======================================================================
\chapter{Beyond $\LPO$ --- The Thermodynamic Stratification}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  tier/.style={rectangle, rounded corners=3pt, draw, minimum width=9cm,
    minimum height=0.9cm, align=center, font=\small},
  >=stealth
]
\node[tier, fill=green!15]  (bish) at (0,0)   {$\BISH$ --- Finite computation};
\node[tier, fill=blue!20]   (lpo)  at (0,1.5) {$\LPO$ ($\Sigma^0_1$) --- Empirical ceiling};
\node[tier, fill=blue!8, pattern=north east lines, pattern color=gray!20]
                             (gap)  at (0,3.0) {Promise gap $=$ finite experimental precision};
\node[tier, fill=purple!15] (sig2) at (0,4.5) {$\LPO'$ ($\Sigma^0_2$) --- Platonic ceiling};

\draw[->,thick] (bish) -- (lpo);
\draw[->,thick,dashed,gray] (lpo) -- (gap);
\draw[->,thick,dashed,gray] (gap) -- (sig2);

% Side labels
\node[font=\scriptsize, text width=3.5cm, align=left, right] at (5,1.5)
  {All measured quantities\\Extensive observables\\Phase transitions};
\node[font=\scriptsize, text width=3.5cm, align=left, right] at (5,3.0)
  {One quantifier alternation\\$\Sigma^0_1 \to \Sigma^0_2$};
\node[font=\scriptsize, text width=3.5cm, align=left, right] at (5,4.5)
  {Generic intensive observables\\without promise gap};
\end{tikzpicture}
\caption{The two ceilings and the gap between them. Finite experimental precision enforces an effective promise gap that collapses $\Sigma^0_2$ decisions back to $\LPO$.}
\label{fig:two-ceilings}
\end{figure}

\section{The Promise Gap as Logic Mechanism}

Cubitt's construction builds Hamiltonians with a promise gap: $\Delta \in \{0\} \cup [\gamma, \infty)$ for some computable $\gamma > 0$. With the gap, deciding ``is $\Delta = 0$?'' reduces to:
\[
  \exists L \text{ such that } \Delta_L < \gamma/2
\]
This is $\Sigma^0_1$---a single existential quantifier over a decidable predicate. $\LPO$ decides it.

Without the promise gap---for a generic Hamiltonian where $\Delta$ could be any non-negative real---deciding ``is $\Delta = 0$?'' requires:
\[
  \forall m\;\exists L \text{ such that } \Delta_L < 1/m
\]
This is $\Pi^0_2$---a universal-existential quantifier alternation. $\LPO$ cannot decide it; it requires $\LPO'$ ($\Sigma^0_2$-$\LEM$). The promise gap is not a convenience or a technical limitation---it is the \emph{logical mechanism} that determines whether the decision lives at $\Sigma^0_1$ or $\Sigma^0_2$.

\section{The Modified Encoding}

Paper~39 constructs an explicit encoding that reaches $\Sigma^0_2$. The modification uses Robinson aperiodic tilings with perimeter counters: supertiles of increasing scale $k = 0, 1, 2, \ldots$ each run a Turing machine $M$ on input $k$, where $k$ is extracted from the perimeter counter. The encoding achieves:
\begin{itemize}[nosep]
\item \textbf{Gapped} $\longleftrightarrow$ $M$ halts on finitely many inputs
\item \textbf{Gapless} $\longleftrightarrow$ $M$ halts on infinitely many inputs
\end{itemize}
This is the \textbf{Finiteness Problem}---$\Sigma^0_2/\Pi^0_2$-complete, strictly above the halting problem in the arithmetic hierarchy. The construction is verified in 802~lines of Lean~4 with zero \texttt{sorry}.

\section{The Stratification Theorem}

The Thermodynamic Stratification Theorem classifies observables by arithmetic complexity as a function of thermodynamic scaling:

\textbf{Extensive observables} (energy density, free energy, magnetization per site) converge via subadditivity. Fekete's lemma ($\equiv \LPO$, Paper~29) guarantees convergence. Ceiling: $\LPO$ ($\Sigma^0_1$), regardless of promise gap.

\textbf{Intensive observables} (spectral gap, correlation length, mass gap) are determined by infimum-type operations, not averages. Without a promise gap, deciding whether an intensive observable equals a specific value involves a $\Pi^0_2$ statement. Ceiling: $\LPO'$ ($\Sigma^0_2$).

\textbf{Empirical observables}---quantities measured at finite precision---always carry an effective promise gap. A spectrometer with precision $\varepsilon$ collapses the $\Pi^0_2$ decision to $\Sigma^0_1$: ``is there a finite-volume gap below $\varepsilon/2$?'' is decidable by $\LPO$. Ceiling: $\LPO$ ($\Sigma^0_1$).

\begin{center}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Tier} & \textbf{Observables} & \textbf{Ceiling} & \textbf{Mechanism} \\
\midrule
Empirical & All measured quantities & $\LPO$ ($\Sigma^0_1$) & Finite precision $=$ promise gap \\
Extensive-Platonic & Thermodynamic densities & $\LPO$ ($\Sigma^0_1$) & Fekete/$\BMC$ \\
Intensive-Platonic & Gap, correlation length & $\LPO'$ ($\Sigma^0_2$) & No promise gap, infimum-type \\
\bottomrule
\end{tabular}
\end{center}

\section{The Refined Thesis}

The original thesis: the logical constitution of empirical physics is $\BISH+\LPO$. Paper~39 refines this without breaking it.

\textbf{Empirical physics} (predictions compared to finite-precision measurements) remains at $\BISH+\LPO$. The empirical ceiling is confirmed.

\textbf{Platonic physics} (exact mathematical properties of idealized infinite-volume systems) can reach $\BISH+\LPO'$ for intensive observables without promise gap.

The gap between empirical and Platonic physics is precisely the promise gap, and the promise gap is precisely finite experimental precision. The refinement \emph{strengthens} the thesis in three ways: (1)~it identifies the \emph{mechanism} keeping empirical physics at $\Sigma^0_1$ (finite precision enforces promise gaps); (2)~it identifies the \emph{boundary} of the thesis (the $\Sigma^0_2$ tier is real but experimentally inaccessible); (3)~it makes a sharper falsifiable prediction (if a laboratory measurement requires $\Sigma^0_2$ reasoning, the empirical ceiling is broken).

\begin{chapterbox}
Generic intensive observables without promise gap can reach $\Sigma^0_2$---the Platonic ceiling. But finite experimental precision enforces an effective promise gap that collapses every measurement back to $\LPO$. The empirical ceiling holds; the gap between empirical and Platonic physics is precisely the promise gap.
\end{chapterbox}

% ======================================================================
%  CHAPTER 10
% ======================================================================
\chapter{Consequences}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  hub/.style={rounded corners=5pt, draw=blue!50, fill=blue!12, minimum width=3cm, minimum height=1cm, font=\small\bfseries, text centered},
  spoke/.style={rounded corners=3pt, draw=gray!60, fill=gray!5, minimum width=3.2cm, minimum height=0.7cm, font=\scriptsize, text centered},
  >=Stealth,
]
% Central hub
\node[hub] (H) at (0,0) {$\BISH+\LPO$ Thesis};

% Five spokes
\node[spoke, align=center] (QG) at (0,2.5) {Quantum gravity\\predictions $= \BISH+\LPO$};
\node[spoke, align=center] (MP) at (4.5,1.2) {Measurement problem\\$=$ scaffolding disagreement};
\node[spoke, align=center] (CC) at (4,-1.5) {$10^{120}$ conflates\\two logical levels};
\node[spoke, align=center] (PC) at (-4,-1.5) {Physical constants\\are $\Delta^0_2$};
\node[spoke, align=center] (UD) at (-4.5,1.2) {Undecidability\\$= \LPO$ non-computability};

% Arrows with labels
\draw[->, thick, blue!40] (H) -- (QG) node[midway, right, font=\tiny] {predicts};
\draw[->, thick, blue!40] (H) -- (MP) node[midway, above, font=\tiny, sloped] {reclassifies};
\draw[->, thick, blue!40] (H) -- (CC) node[midway, below, font=\tiny, sloped] {dissolves};
\draw[->, thick, blue!40] (H) -- (PC) node[midway, below, font=\tiny, sloped] {constrains};
\draw[->, thick, blue!40] (H) -- (UD) node[midway, above, font=\tiny, sloped] {reclassifies};
\end{tikzpicture}
\caption{Consequence map. The $\BISH+\LPO$ thesis generates five consequences for open problems: predictions for quantum gravity, reclassification of the measurement problem and undecidability, dissolution of the $10^{120}$ discrepancy, and constraints on physical constants.}
\label{fig:consequences}
\end{figure}

\section{For Quantum Gravity}

The characterization predicts that whatever empirical predictions quantum gravity eventually makes, they will be $\BISH+\LPO$. This applies to all candidate theories.

\textbf{String theory.} The string landscape---the space of consistent string vacua---is constructed using Calabi--Yau geometry, which relies heavily on compactness ($\FT$). But compactness is dispensable (Paper~30). Whatever empirical predictions emerge from a specific string vacuum will be finite computations ($\BISH$) or completed limits ($\LPO$). The swampland program, asserting universal constraints of the form ``no consistent theory has property $P$,'' makes $\Pi^0_2$ claims (universal quantification over a space of theories) that may exceed $\BISH+\LPO$---and if so, the program predicts they are logically disconnected from empirical physics.

\textbf{Loop quantum gravity} fits naturally. Spin network states at finite graph size are combinatorial objects ($\BISH$). The continuum limit costs $\LPO$, exactly as in lattice QCD.

\textbf{Holographic entanglement entropy} (Ryu--Takayanagi) is calibrated in detail in Chapter~11. If both sides of AdS/CFT calibrate to $\BISH+\LPO$, the duality is logically consistent in the program's sense. If one side costs more, the program has identified a logical obstruction.

\section{For the Measurement Problem}

Many-Worlds requires $\DC$ to construct the branching tree of the universal wavefunction---each measurement creates new branches, and the infinite tree requires $\DC$. Copenhagen requires $\WLPO$ to assert that a definite outcome occurred (a zero-test on superposition coefficients). Bohmian mechanics requires $\LPO$ to assert existence of the pilot wave trajectory as a completed real-valued function.

The crucial observation: $\DC$ is dispensable (Paper~31). The branching structure is mathematical scaffolding---the empirical predictions (Born rule probabilities) are $\BISH$ and do not depend on whether branches ``exist.'' Similarly, Copenhagen's collapse ($\WLPO$) and Bohm's trajectory ($\LPO$) generate the same $\BISH$-level empirical predictions. The interpretations agree on every number an experimentalist can measure. They disagree about ontological claims at different heights in the hierarchy.

This does not ``solve'' the measurement problem. It reclassifies it: from ``what actually happens during measurement?'' to ``which logically dispensable mathematical framework do you prefer for describing non-empirical aspects of quantum theory?'' The reclassification identifies precisely where the disagreement lives (in scaffolding above $\BISH$) and why it is empirically unresolvable (because the scaffolding is dispensable).

\section{For the Cosmological Constant}

The $10^{120}$ discrepancy is reclassified in Chapter~12. The QFT vacuum energy sum is an infinite series; asserting convergence to a specific value is $\LPO$. The $\BISH$-level content is different: for any finite number of modes, the vacuum energy is finite and computable. Comparing an $\LPO$-level mathematical artifact to a $\BISH$-level measurement and calling the discrepancy a ``prediction failure'' conflates two logical levels. This is not a resolution---the question of why $\Lambda$ has its observed value remains open---but it is a change in character.

\section{For Physical Constants}

Every physical constant---$\alpha \approx 1/137$, $m_e$, $\Lambda$, $G$---is $\Delta^0_2$ (limit-computable). Each admits a finite description: an algorithm that, given access to a halting oracle, converges to the constant's value. No physical constant can encode a $\Sigma^0_2$-complete problem.

This reframes the fine-tuning debate. The standard discussion assumes constants are drawn from the continuum---an uncountable set with measure-theoretic properties. But the program says physical constants are $\Delta^0_2$---a countable class. A fine-tuning argument computing the probability of $\alpha$ by integrating over all reals is integrating over a space overwhelmingly populated by constants that physics cannot access. The physically relevant sample space is $\Delta^0_2$, which is countable and does not support the standard measure-theoretic formulation of fine-tuning. This does not settle the question but constrains the form an answer can take.

\section{For Physical Undecidability}

The undecidability arc (Papers~36--39) changes how ``undecidability'' in physics should be understood. Before this program, spectral gap undecidability was interpreted as revealing a fundamental limit to physical knowledge, comparable to G\"odel's incompleteness theorems. The reclassification: physical undecidability is the non-computability of $\LPO$, the same principle physicists have implicitly used since Boltzmann defined the thermodynamic limit in the 1870s.

The spectral gap of a \emph{specific} Hamiltonian $H(M)$ is not ``unknowable''---it is $\LPO$-decidable. A physicist who can take a thermodynamic limit can decide whether $H(M)$ is gapped. The uniform problem---deciding for arbitrary Hamiltonians simultaneously---is non-computable because $\LPO$ is non-computable when applied to arbitrary binary sequences. But this is the same non-computability that prevents a general convergence oracle. Cubitt's celebrated result reveals not a new frontier of unknowability but the familiar non-computability of a principle physicists have used for 150~years.

The $\Sigma^0_2$ refinement (Paper~39) sharpens this: for generic intensive observables without promise gap, the decision reaches $\Sigma^0_2$. But this tier is Platonic---no finite-precision experiment can distinguish ``gap $= 0$'' from ``gap $< \varepsilon$.'' The empirical ceiling remains $\LPO$ because finite precision enforces an effective promise gap.

\begin{chapterbox}
The $\BISH+\LPO$ thesis predicts that quantum gravity predictions will be $\BISH+\LPO$, that the measurement problem is a disagreement about dispensable scaffolding, that physical constants are $\Delta^0_2$, and that the $10^{120}$ discrepancy conflates two logical levels. Reclassification identifies what kind of problem each is---resolution requires new physics.
\end{chapterbox}

% ======================================================================
%  CHAPTER 11: AdS/CFT (Paper 41)
% ======================================================================
\chapter{The Diagnostic in Action I --- AdS/CFT Correspondence}

\section{From Classification to Diagnosis}

Papers~1--39 established that the logical resources required for all empirical predictions in known physics are exactly $\BISH+\LPO$. Paper~41 transforms the program from a completed classification exercise into an active diagnostic tool, applied to the most important open problem in theoretical physics.

The target is the AdS/CFT correspondence~\cite{Maldacena1999}---specifically, the Ryu--Takayanagi (RT) formula~\cite{RyuTakayanagi2006} for holographic entanglement entropy. The choice is strategic: AdS/CFT is the most active area of theoretical physics, the RT formula is its most cited result, and the Page curve and island formula are its most debated recent developments. The diagnostic question is simple: \emph{does the holographic dictionary preserve axiom cost?} If the correspondence claims that bulk gravitational physics and boundary conformal field theory compute the same physics, the framework can check whether they compute it at the same \emph{logical cost}. If yes, the duality is logically transparent. If no, the axiom gap identifies places where the duality performs non-trivial logical work---candidates for where it might fail or require modification.

\section{Observables vs.\ Decisions}

Before presenting the calibrations, a distinction must be sharpened that refines the program's treatment of phase transitions. There are two logically distinct operations at a phase transition:

\textbf{The observable computation:} computing the numerical value of the order parameter, the free energy, or the entropy as a function of the control parameter. This is a question about a continuous real-valued function.

\textbf{The phase decision:} declaring which phase the system is in---asserting ``the system is in phase~A'' or ``the system is in phase~B'' as a Boolean classification.

These have different axiom costs. The Fekete mechanism (Paper~29) concerns the \emph{existence} of a thermodynamic limit---proving that $f = \lim_{N\to\infty} F_N/N$ exists. The $\LPO$ cost is in computing the limit. The BTZ entanglement phase transition is different: both competing quantities $L_1(\theta)$ and $L_2(\theta)$ are already in hand as explicit, $\BISH$-computable functions. The entropy $S(A) = \min(L_1,L_2)/(4G_N)$ is $\BISH$ via the identity $\min(x,y) = \tfrac{1}{2}(x + y - |x-y|)$, since the absolute value function is uniformly continuous and $\BISH$-computable. The \emph{phase decision}---extracting a Boolean flag declaring whether $L_1 \leq L_2$ or $L_2 \leq L_1$---costs $\LLPO$ when the difference may be zero.

\textbf{Reconciliation with Paper~29:} Fekete costs $\LPO$ because it computes a quantity \emph{defined as a limit}---a real number not yet in hand. The $\min$ costs $\BISH$ because it selects between values already computed as explicit functions. Both mechanisms occur at phase transitions but contribute different axiom costs. This distinction is invisible in systems where competing quantities are themselves defined by limits (e.g., the Ising free energy). The BTZ RT formula is the first calibration where competing quantities are given by closed-form expressions, making the min mechanism visible as a separate, lower-cost operation.

\section{Vacuum AdS$_3$: The Null Result}

\textbf{Bulk:} In the Poincar\'e patch of AdS$_3$, $ds^2 = (\ell^2/z^2)(dz^2 + dx^2)$, the RT geodesic connecting boundary points $(x_1,0)$ and $(x_2,0)$ is a semicircle of radius $R = |x_2-x_1|/2$. Its regularized length is:
\[
  L_{\mathrm{reg}} = 2\ell\log\!\bigl(|x_2-x_1|/\varepsilon\bigr)
\]
An explicit algebraic expression. No variational principle, no compactness, no limit. \textbf{Calibration: $\BISH$.}

\textbf{Boundary:} The Calabrese--Cardy formula~\cite{CalabreseCardy2004} for entanglement entropy of an interval of length $\ell_A$ in a 2d CFT vacuum state:
\[
  S(A) = \frac{c}{3}\log\!\bigl(\ell_A/\varepsilon\bigr)
\]
Derived via the replica trick: (a)~twist operator correlation function for integer $n$---algebraic, $\BISH$; (b)~analytic continuation $n \to 1$---explicit formula, $\BISH$; (c)~differentiation at $n=1$---elementary, $\BISH$. \textbf{Calibration: $\BISH$.}

Both sides yield the same formula under Brown--Henneaux $c = 3\ell/(2G_N)$. The duality is a $\BISH$-to-$\BISH$ map---the holographic dictionary performs no logical work for this prediction. This null result sets the baseline for the thermal case.

\section{Thermal BTZ: The Phase Transition}

In the BTZ black hole with horizon radius $r_+$ and AdS radius $\ell$, the two competing RT geodesics for a boundary interval of angular extent $\theta$ have lengths:
\[
  L_1(\theta) = 2\ell\ln\!\Bigl(\frac{2R}{r_+}\sinh\frac{r_+\theta}{2\ell}\Bigr), \qquad
  L_2(\theta) = 2\ell\ln\!\Bigl(\frac{2R}{r_+}\sinh\frac{r_+(2\pi-\theta)}{2\ell}\Bigr)
\]
Both are explicit compositions of elementary functions: logarithm, hyperbolic sine, multiplication. \textbf{Both are $\BISH$-computable.} The entanglement entropy $S(A) = \min(L_1,L_2)/(4G_N)$ is $\BISH$.

The critical angle $\theta_c$ where $L_1 = L_2$ is determined by $\sinh(r_+\theta/2\ell) = \sinh(r_+(2\pi-\theta)/2\ell)$. Since $\sinh$ is strictly monotone, this gives $\theta_c = \pi$---a trivially computable constant determined by the $\theta \leftrightarrow 2\pi - \theta$ symmetry of the BTZ geometry. No comparison of potentially equal real numbers is needed: $\theta < \pi$ implies $L_1 < L_2$, and $\theta > \pi$ implies $L_1 > L_2$, both decidable by rational approximation. For the BTZ black hole specifically, even the discrete phase classification is $\BISH$.

\textbf{Generic asymptotically AdS black holes:} with matter fields or higher-derivative corrections, the geodesic lengths remain $\BISH$-computable (determined by explicit ODEs), the continuous entropy $\min(L_1,L_2)$ remains $\BISH$, but the critical angle $\theta_c$ may not have a closed-form solution. The phase decision then costs $\LLPO$.

\textbf{Boundary side:} the Hawking--Page transition involves $F = \min(F_{\mathrm{AdS}},F_{\mathrm{BTZ}})$---both $\BISH$-computable. The duality preserves axiom cost exactly: on both sides, the continuous observable is $\BISH$ and the discrete phase classification is $\LLPO$ (or $\BISH$ for BTZ by symmetry).

\section{FLM Quantum Correction}

The Faulkner--Lewkowycz--Maldacena formula~\cite{FLM2013} adds a one-loop correction: $S(A) = \mathrm{Area}(\gamma_A)/(4G_N) + S_{\mathrm{bulk}}(\Sigma_A)$. For a free massive scalar in AdS$_3$, the bulk entanglement entropy is computed via the replica trick on bulk fields.

The heat kernel on Euclidean AdS$_3$ ($\cong \mathbb{H}^3$) has the explicit Camporesi form~\cite{Camporesi1990}:
\[
  K(t,\rho) \propto t^{-3/2}\,\frac{\rho}{\sinh\rho}\,\exp\!\Bigl(-\frac{\rho^2}{4t} - m^2 t\Bigr)
\]
The Sommerfeld method of images produces a sum over geodesics to image points at distances $\rho_n$. The exponential factor $\exp(-\rho_n^2/4t)$ guarantees the sum converges with an explicit, $\BISH$-computable Cauchy modulus. UV regularization via Seeley--DeWitt coefficients and $\zeta$-function regularization both yield $\BISH$-computable results: $\zeta'(0)$ is obtained by algebraic integration-by-parts on an explicit formula.

\textbf{Calibration: $S_{\mathrm{bulk}}$ for a free scalar in the AdS$_3$ vacuum is $\BISH$.} The FLM quantum correction does not increase axiom cost beyond the classical RT term. The $\LPO$ cost would enter for interacting fields (where the mode sum does not close) or non-symmetric backgrounds (where the heat kernel lacks a closed form).

\section{Quantum Extremal Surface}

The Engelhardt--Wall~\cite{EngelhardtWall2015} QES prescription minimizes the generalized entropy:
\[
  S(A) = \min_\gamma\bigl[\mathrm{Area}(\gamma)/(4G_N) + S_{\mathrm{bulk}}(\Sigma_\gamma)\bigr] = \min_\gamma S_{\mathrm{gen}}(\gamma)
\]
Existence of the minimizing surface $\gamma^*$ by the direct method of calculus of variations requires extracting a convergent subsequence from a minimizing sequence---a compactness argument costing $\FT$. But the boundary CFT does not observe the bulk surface. The entropy is the \emph{infimum} of $S_{\mathrm{gen}}$, and constructive mathematics can compute this infimum by evaluating the functional on successive approximations and applying $\BMC$ ($\LPO$). $\FT$ builds the Platonic surface in the unobservable bulk; $\BISH$ computes the observable entropy on the boundary. The Lean formalization (Theorem \texttt{Infimum\_vs\_Minimizer}) makes this separation precise: (a)~the observable infimum $\inf_\gamma S_{\mathrm{gen}}(\gamma)$ requires only $\LPO$ (via $\BMC$); (b)~the geometric minimizer $\gamma^*$ requires $\FT$ (compactness); (c)~$\LPO$ alone does not yield the minimizer. The three claims are independently machine-checked.

\textbf{Perturbative regime:} the QES is obtained by perturbing the classical RT surface via the Jacobi geodesic deviation equation, an ODE sourced by $\nabla S_{\mathrm{bulk}}$. By Picard--Lindel\"of ($\BISH$ for Lipschitz ODEs), the perturbed surface is $\BISH$-constructible. No compactness needed.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[>=Stealth]
% --- Left panel: Infimum (observable) ---
\begin{scope}[shift={(-3.8,0)}]
  \node[font=\small\bfseries, text=blue!70!black] at (1.5, 3.8)
    {Observable: entropy value};
  % Axes
  \draw[->] (0,0) -- (3.2,0) node[right, font=\small] {$\gamma$};
  \draw[->] (0,0) -- (0,3.5) node[left, font=\small] {$S_{\mathrm{gen}}$};
  % Curve (bounded below, descends toward infimum)
  \draw[thick, blue!70!black, smooth, domain=0.7:3, samples=60]
    plot (\x, {0.8 + 2.0/(\x+0.2)});
  % Infimum line
  \draw[dashed, thick, blue!60] (0, 0.8) -- (3.1, 0.8)
    node[right, font=\small] {$\inf$};
  % Arrows showing successive approximation
  \draw[->, thick, blue!50] (2.0, 1.71) -- (2.0, 0.9)
    node[midway, right, font=\tiny] {$\epsilon$};
  \draw[->, thick, blue!50] (1.3, 2.13) -- (1.3, 0.9);
  \draw[->, thick, blue!50] (0.9, 2.62) -- (0.9, 0.9);
  % Label
  \node[draw, rounded corners=2pt, fill=blue!8,
    font=\small\bfseries, text=blue!60!black,
    inner sep=3pt] at (1.5, -0.6) {$\LPO$ (via $\BMC$)};
  \node[font=\tiny, text=black!60, align=center] at (1.5, -1.1)
    {computable value\\boundary observes this};
\end{scope}
% --- Right panel: Minimizer (scaffolding) ---
\begin{scope}[shift={(3.8,0)}]
  \node[font=\small\bfseries, text=red!60!black] at (1.5, 3.8)
    {Scaffolding: bulk surface};
  % Axes
  \draw[->] (0,0) -- (3.2,0) node[right, font=\small] {$\gamma$};
  \draw[->] (0,0) -- (0,3.5) node[left, font=\small] {$S_{\mathrm{gen}}$};
  % Curve (parabola on compact domain with clear minimum)
  \draw[thick, blue!70!black, smooth, domain=0.3:2.9, samples=60]
    plot (\x, {0.8 + 0.7*(\x-1.5)*(\x-1.5)});
  % Minimizer point
  \fill[red!70!black] (1.5, 0.8) circle (3pt);
  \node[font=\small, text=red!60!black, above right] at (1.6, 0.7)
    {$\gamma^*$};
  % Dashed line to axis
  \draw[dashed, gray] (1.5, 0) -- (1.5, 0.8);
  % Compact domain bracket
  \draw[thick, red!50!black, decorate,
    decoration={brace, amplitude=4pt, mirror}]
    (0.3, -0.15) -- (2.9, -0.15)
    node[midway, below=5pt, font=\tiny, text=red!50!black]
    {compact surface space};
  % Label
  \node[draw, rounded corners=2pt, fill=red!8,
    font=\small\bfseries, text=red!60!black,
    inner sep=3pt] at (1.5, -0.6) {$\FT$ (compactness)};
  \node[font=\tiny, text=black!60, align=center] at (1.5, -1.1)
    {geometric surface\\boundary does NOT observe};
\end{scope}
% --- Central separator ---
\draw[very thick, gray!30] (0, -0.3) -- (0, 3.6);
\node[font=\Large, text=red!50!black, rotate=90] at (0, 1.5) {$\neq$};
\end{tikzpicture}
\caption{The scaffolding separation. \textbf{Left:} The observable entropy (infimum of $S_{\mathrm{gen}}$) is computable at $\LPO$ via successive approximation ($\BMC$). The boundary CFT observes this value. \textbf{Right:} The bulk surface $\gamma^*$ that achieves the minimum requires compactness ($\FT$). The boundary never observes it. Holography needs only the left panel.}
\label{fig:scaffolding-sep}
\end{figure}

\textbf{Island formula and Page curve:} $S(A) = \min(S_{\mathrm{island}}, S_{\mathrm{no\text{-}island}})$ is $\BISH$ (minimum of two $\BISH$-computable quantities). The Page curve---the continuous plot of entropy as a function of time---is a $\BISH$-computable function. The discrete Page time decision costs $\LLPO$ for generic parameters.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[>=Stealth]
  % Axes
  \draw[->] (0,0) -- (8.5,0) node[right, font=\small] {time $t$};
  \draw[->] (0,0) -- (0,5.3) node[above, font=\small] {$S(R)$};

  % No-island entropy (rising, then saturates)
  \draw[thick, blue!60, smooth, domain=0:8, samples=80]
    plot (\x, {3.8*(1 - exp(-0.4*\x))});
  \node[font=\small, text=blue!60, above] at (7.5, 3.6)
    {$S_{\text{no-island}}$};

  % Island entropy (starts high, descends)
  \draw[thick, red!60, smooth, domain=0:8, samples=80]
    plot (\x, {3.6 - 0.3*\x + 0.015*\x*\x});
  \node[font=\small, text=red!60] at (7.8, 1.55)
    {$S_{\text{island}}$};

  % Page curve = min of the two (thick green)
  \draw[very thick, green!60!black, smooth, domain=0:3.28, samples=40]
    plot (\x, {min(3.8*(1-exp(-0.4*\x)), 3.6-0.3*\x+0.015*\x*\x)});
  \draw[very thick, green!60!black, smooth, domain=3.28:8, samples=40]
    plot (\x, {min(3.8*(1-exp(-0.4*\x)), 3.6-0.3*\x+0.015*\x*\x)});

  % Crossing dot
  \fill[orange!70!black] (3.28, 2.78) circle (2.5pt);

  % Page time vertical line
  \draw[dashed, thick, orange!70!black] (3.28, 0) -- (3.28, 4.8);
  \node[font=\small\bfseries, text=orange!70!black, above] at (3.28, 4.8)
    {Page time};

  % CRM annotations
  \node[draw, rounded corners=2pt, fill=green!10,
    font=\scriptsize, text=green!50!black, inner sep=2pt]
    at (1.3, 4.5) {$\BISH$: $\min$ identity};

  \node[draw, rounded corners=2pt, fill=green!10,
    font=\scriptsize, text=green!50!black, inner sep=2pt]
    at (5.8, 4.5) {$\BISH$: $\min$ identity};

  % Page time annotation
  \node[draw, rounded corners=2pt, fill=orange!10,
    font=\small, text=orange!60!black, inner sep=3pt]
    at (3.28, -0.7) {$\LLPO$: ``has the transition occurred?''};

  % Green label for Page curve
  \node[font=\small\bfseries, text=green!50!black] at (1.2, 2.6)
    {Page curve};
\end{tikzpicture}
\caption{The Page curve through the CRM lens. The continuous Page curve $S(R) = \min(S_{\text{island}}, S_{\text{no-island}})$ is $\BISH$-computable at every time $t$ via the min identity. The discrete decision ``has the Page time occurred?'' costs $\LLPO$. Information recovery is encoded in a constructively accessible quantity; only the temporal classification requires a (weak) omniscience principle.}
\label{fig:page-curve-ch11}
\end{figure}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  side/.style={rectangle, rounded corners=5pt, draw=#1!60, fill=#1!8,
    minimum width=4.5cm, minimum height=5cm, align=center},
  item/.style={font=\small, text width=3.8cm, align=left},
  iso/.style={<->,very thick,red!60,>=stealth}
]
\node[side=blue] (bulk) at (-3.5,0) {};
\node[font=\bfseries,blue!70] at (-3.5,2.2) {Bulk (AdS)};
\node[item] at (-3.5,1.3) {Geodesic: $\BISH$};
\node[item] at (-3.5,0.5) {Heat kernel: $\BISH$};
\node[item] at (-3.5,-0.3) {QES existence: $\FT$};
\node[item,red!60] at (-3.5,-1.1) {\textit{(scaffolding)}};
\node[item] at (-3.5,-1.9) {Continuum: $\LPO$};

\node[side=green!60!black] (bdy) at (3.5,0) {};
\node[font=\bfseries,green!60!black] at (3.5,2.2) {Boundary (CFT)};
\node[item] at (3.5,1.3) {Calabrese--Cardy: $\BISH$};
\node[item] at (3.5,0.5) {Entropy: $\BISH$};
\node[item] at (3.5,-0.3) {Phase decision: $\LLPO$};
\node[item] at (3.5,-1.1) {};
\node[item] at (3.5,-1.9) {Thermo limit: $\LPO$};

% Axiom-preservation arrows
\draw[iso] (-0.8,1.3) -- (0.8,1.3) node[midway,above,font=\scriptsize,red!60] {$=$};
\draw[iso] (-0.8,0.5) -- (0.8,0.5) node[midway,above,font=\scriptsize,red!60] {$=$};
\draw[iso] (-0.8,-1.9) -- (0.8,-1.9) node[midway,above,font=\scriptsize,red!60] {$=$};

\node[font=\small\bfseries,red!60] at (0,-3) {Holographic dictionary $=$ axiom-preserving map};
\end{tikzpicture}
\caption{The holographic dictionary preserves axiom cost. Bulk and boundary computations carry identical logical resources at every level. The Fan Theorem cost of QES existence is scaffolding---it does not appear on the boundary side.}
\label{fig:holographic}
\end{figure}

\section{Complete Calibration Table}

\begin{center}
{\small
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Computation} & \textbf{Bulk cost} & \textbf{Boundary cost} & \textbf{Duality preserves?} \\
\midrule
Vacuum AdS$_3$ RT & $\BISH$ & $\BISH$ & \checkmark\; (identity) \\
BTZ RT (entropy value) & $\BISH$ & $\BISH$ & \checkmark \\
BTZ RT (phase decision) & $\BISH$ ($\theta_c = \pi$) & $\BISH$ (by symmetry) & \checkmark \\
Generic thermal RT (entropy) & $\BISH$ & $\BISH$ & \checkmark \\
Generic thermal RT (phase) & $\LLPO$ & $\LLPO$ & \checkmark \\
FLM correction (free, vacuum) & $\BISH$ & N/A (bulk only) & --- \\
FLM correction (free, thermal) & $\BISH$ & N/A (bulk only) & --- \\
QES surface existence & $\FT$ (scaffolding) & Not observed & Projected away \\
QES entropy (perturbative) & $\BISH$ & $\BISH$ & \checkmark \\
QES entropy (non-perturbative) & $\LPO$ (via inf/$\BMC$) & $\LPO$ (expected) & \checkmark \\
Island formula (Page curve) & $\BISH$ & $\BISH$ & \checkmark \\
Island formula (Page time) & $\LLPO$ & $\LLPO$ & \checkmark \\
\bottomrule
\end{tabular}}
\end{center}

The table's most striking feature: no entry exceeds $\LPO$. The $\BISH+\LPO$ ceiling holds across the entire holographic dictionary---from the simplest vacuum RT to the quantum-corrected island formula.

\section{What the Diagnostic Reveals}

\textbf{The holographic dictionary is an axiom-preserving map} (\cref{fig:holographic}). For every prediction examined, bulk and boundary computations carry identical axiom cost. The duality maps $\BISH$ to $\BISH$, $\LLPO$ to $\LLPO$, and $\LPO$ to $\LPO$. This is a falsifiable prediction: any future computation where the two sides have different axiom costs would identify a logical obstruction within the correspondence.

\textbf{Holography projects away compactness.} The Fan Theorem cost of bulk geometric existence is invisible to the boundary. The boundary CFT computes the entropy value without ever constructing or observing the bulk surface. The compactness that guarantees the surface exists is scaffolding---necessary for the bulk geometric picture, irrelevant to the boundary observable. This is the holographic principle, restated in the language of constructive reverse mathematics: holography is the projection that eliminates $\FT$.

\textbf{Phase transitions are cheaper than expected.} The observable entropy at a phase transition is $\BISH$---the minimum of two $\BISH$-computable functions. Only the discrete phase classification costs $\LLPO$, and even this cost vanishes for sufficiently symmetric geometries. This refines the program's treatment of phase transitions by distinguishing computing a limit ($\LPO$, Fekete mechanism) from selecting a minimum ($\BISH$, min mechanism).

\textbf{For theorists working on the Page curve:} the physically meaningful axiom cost of the island formula is the saddle-point competition ($\BISH$ for the entropy value; $\LLPO$ for the phase decision). The $\FT$ cost of QES existence is scaffolding---eliminable in the perturbative regime (Jacobi equation, $\BISH$) and projectable away via holography in the non-perturbative regime (boundary infimum, $\LPO$). Computational effort should be directed at characterizing the competition between island and no-island saddles, not at proving existence of the QES via compactness methods.

\begin{chapterbox}
The holographic dictionary is an axiom-preserving map: bulk and boundary computations carry identical logical resources at every level. The Fan Theorem cost of QES existence is scaffolding---holography is the projection that eliminates $\FT$. No entry exceeds $\LPO$.
\end{chapterbox}

% ======================================================================
%  CHAPTER 12: Cosmological Constant (Paper 42)
% ======================================================================
\chapter{The Diagnostic in Action II --- Cosmological Constant}

\section{The Alleged $10^{120}$ Discrepancy}

The cosmological constant problem~\cite{Weinberg1989} is widely described as the worst prediction in the history of physics. The claim: quantum field theory predicts a vacuum energy density of order $M_{\mathrm{Planck}}^4 \approx 10^{71}\;\mathrm{GeV}^4$, while the observed cosmological constant corresponds to $\rho_\Lambda \approx 10^{-47}\;\mathrm{GeV}^4$---a discrepancy of 120~orders of magnitude.

Paper~42 subjects this claim to the axiom calibration framework. The diagnostic question: at what level of the constructive hierarchy does each component of the problem live? Is the ``prediction'' a $\BISH$-computable quantity derived from experimental inputs? Or is it an artifact of a specific mathematical idealization? The problem decomposes into three logically distinct claims with different constructive status.

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  claim/.style={rectangle, rounded corners=5pt, draw=#1!60, fill=#1!10,
    minimum width=12cm, minimum height=1.4cm, text width=11.5cm,
    align=left, font=\small},
  >=stealth
]
\node[claim=red] (C1) at (0,0) {
  \textbf{Claim I:} The $10^{120}$ UV discrepancy
  \hfill $\longrightarrow$ \textbf{DISSOLVED}\\
  Regulator-dependent artifact. Vanishes under dimensional regularization.
};
\node[claim=yellow!80!orange] (C2) at (0,-2) {
  \textbf{Claim II:} Naturalness ($\Lambda \sim M_{\mathrm{Planck}}^4$)
  \hfill $\longrightarrow$ \textbf{RECLASSIFIED}\\
  Bayesian prior, not mathematical derivation. Outside the deductive hierarchy.
};
\node[claim=green!60!black] (C3) at (0,-4) {
  \textbf{Claim III:} 55-digit fine-tuning
  \hfill $\longrightarrow$ \textbf{IDENTIFIED}\\
  $\Lambda_{\mathrm{obs}} = \Lambda_{\mathrm{bare}} + 8\pi G(\rho_{\mathrm{Higgs}} + \rho_{\mathrm{QCD}})$.
  Arithmetic relation between $\LPO$-computable reals.
};
\end{tikzpicture}
\caption{The cosmological constant problem decomposed. The $10^{120}$ discrepancy is dissolved as scaffolding. Naturalness is reclassified as non-mathematical. The genuine constraint is a 55-digit fine-tuning at $\LPO$.}
\label{fig:cc-decomposition}
\end{figure}

\section{The Mode Sum and Its Regulators}

For a free scalar field of mass $m$ on a finite cubic lattice with $N$ sites per dimension, the vacuum energy $E_{\mathrm{vac}} = \tfrac{1}{2}\sum_{\mathbf{k}} \omega_{\mathbf{k}}$ is a finite sum of explicit algebraic expressions. $\BISH$. No controversy.

In the continuum limit ($a \to 0$, $N \to \infty$), the sum becomes an integral $\rho_{\mathrm{vac}} = \int d^3k/(2\pi)^3 \cdot \tfrac{1}{2}\sqrt{k^2 + m^2}$, which diverges quartically. A divergent series does not define a real number---not at $\BISH$, not at $\LPO$, not at any level. The continuum vacuum energy is not a real number at all.

To extract a finite number, one introduces regularization:

\textbf{Cutoff regularization:} restrict $|k| \leq \Lambda_{\mathrm{UV}}$. The integral gives $\rho \sim \Lambda_{\mathrm{UV}}^4/(16\pi^2)$. Setting $\Lambda_{\mathrm{UV}} = M_{\mathrm{Planck}}$ gives $\rho \sim 10^{71}\;\mathrm{GeV}^4$. This is $\BISH$-computable for any specific $\Lambda_{\mathrm{UV}}$, but the hard cutoff breaks diffeomorphism invariance---it is incompatible with general relativity.

\textbf{Dimensional regularization:} compute in $d = 4 - 2\varepsilon$ dimensions. The quartic divergence vanishes identically---purely polynomial divergences evaluate to zero by construction. For a massive field, $\rho \sim m^4\ln(m^2/\mu^2)$. For the top quark, this gives $\rho \sim (100\;\mathrm{GeV})^4 \sim 10^8\;\mathrm{GeV}^4$. The $10^{120}$ discrepancy does not exist in this scheme.

\textbf{$\zeta$-function regularization} agrees with dimensional regularization, not with cutoff regularization.

The $10^{120}$ number is produced by cutoff regularization. A quantity that changes when you change the regulator is not a physical prediction---it is an artifact of mathematical scaffolding. \textbf{Claim~I is dissolved.}

\section{Gravity and the Hollands--Wald Ambiguity}

The standard objection: gravity couples to absolute energy via $G_{\mu\nu} = 8\pi G\, T_{\mu\nu}$, so the regulator-dependence objection fails when gravity enters. This is serious and must be addressed precisely.

The Hollands--Wald axioms~\cite{HollandsWald2005} establish that the renormalized expectation value $\langle T_{\mu\nu}\rangle_{\mathrm{ren}}$ is uniquely determined by physical axioms (locality, covariance, conservation, correct flat-space limit) \emph{up to} a finite number of free coefficients:
\[
  \langle T_{\mu\nu}\rangle_{\mathrm{ren}} = \langle T_{\mu\nu}\rangle_{\mathrm{canonical}} + c_1 g_{\mu\nu} + c_2 G_{\mu\nu} + c_3(\text{curvature terms}) + \cdots
\]
The coefficient $c_1$ is precisely the cosmological constant $\Lambda$. The Hollands--Wald axioms prove---as a mathematical theorem---that QFT in curved spacetime \emph{cannot predict} $\Lambda$. It is a free parameter on the same footing as particle masses. Setting $c_1 = \Lambda_{\mathrm{obs}} - (\text{condensate contributions})$ is arithmetic. $\BISH$.

\section{Naturalness as Non-Mathematical Claim}

The ``naturalness'' argument asserts that $c_1$ should be ``of order'' $M_{\mathrm{Planck}}^4$. This is a claim about the \emph{expected magnitude} of a free parameter---a prior probability distribution over the space of possible values.

The calibration framework identifies this as a claim \emph{outside the constructive hierarchy}. $\BISH$ formalizes deductive mathematics: given axioms, what follows? Naturalness is an inductive claim: given the structure, what values should we expect? The constructive hierarchy calibrates the former and has nothing to say about the latter. \textbf{Claim~II is reclassified}---not dissolved (the framework does not show it is incorrect) but identified as non-mathematical.

\section{The Genuine Constraint: 55-Digit Fine-Tuning}

After dissolving Claim~I and reclassifying Claim~II, what remains? The electroweak phase transition produces a Higgs vacuum condensate with energy density $\rho_{\mathrm{Higgs}} = V(v) = -\mu^4/(4\lambda) \approx -(100\;\mathrm{GeV})^4 \approx -10^8\;\mathrm{GeV}^4$. The QCD chiral phase transition produces a quark condensate $\rho_{\mathrm{QCD}} \sim -\langle\bar{q}q\rangle \cdot m_q \sim -10^{-3}\;\mathrm{GeV}^4$. Both are $\BISH$-computable at tree level.

The fine-tuning equation:
\[
  \Lambda_{\mathrm{obs}} = \Lambda_{\mathrm{bare}} + 8\pi G\,(\rho_{\mathrm{Higgs}} + \rho_{\mathrm{QCD}})
\]
With $\Lambda_{\mathrm{obs}} \approx 10^{-47}\;\mathrm{GeV}^4$ and $\rho_{\mathrm{Higgs}} \approx -10^8\;\mathrm{GeV}^4$, the bare parameter must satisfy $\Lambda_{\mathrm{bare}} \approx 10^8\;\mathrm{GeV}^4$ to 55~decimal places. The \emph{exact} interacting vacuum energies require the thermodynamic limit (Fekete, Paper~29), placing them at $\LPO$. The fine-tuning equation at exact values is an equality between $\LPO$-computable reals---logically mundane, qualitatively identical to every other thermodynamic limit.

\section{The Casimir Effect: What QFT Actually Predicts}

The Casimir force between parallel conducting plates, $F/A = -\pi^2\hbar c/(240 d^4)$, is an energy \emph{difference}---the difference in vacuum energy between the plate configuration and free space. The divergent terms cancel algebraically. The finite remainder converges with a $\BISH$-computable Cauchy modulus. The Casimir force is $\BISH$---no limits, no omniscience. It confirms the pattern: absolute vacuum energies are regulator-dependent; energy differences are regulator-independent and $\BISH$-computable.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[
  box/.style={rectangle, rounded corners=4pt, draw=#1!60, fill=#1!8,
    minimum width=5.5cm, minimum height=2.2cm, text width=5cm,
    align=center, font=\small},
  lbl/.style={font=\small\bfseries, #1}
]
% Left: Absolute vacuum energy
\node[box=red] (abs) at (-3.5,0) {
  \textbf{Absolute vacuum energy}\\[4pt]
  Cutoff: $\sim M_{\mathrm{Planck}}^4$\\
  Dim.\ reg.: $\sim m^4\ln$\\
  $\zeta$-function: $\sim m^4\ln$\\[3pt]
  \textcolor{red!70!black}{\textbf{Regulator-dependent}}
};
\node[lbl=red!70!black, above] at (-3.5,1.35) {SCAFFOLDING};
% Right: Energy differences
\node[box=green!60!black] (diff) at (3.5,0) {
  \textbf{Energy differences}\\[4pt]
  Casimir: $-\pi^2/(240\,d^4)$\\
  All regulators agree\\
  Experimentally verified\\[3pt]
  \textcolor{green!50!black}{\textbf{$\BISH$-computable}}
};
\node[lbl=green!50!black, above] at (3.5,1.35) {PHYSICAL};
% Dividing line
\draw[dashed, thick, gray] (0,-1.5) -- (0,1.5);
\node[font=\scriptsize, gray, rotate=90] at (0.3,0) {diagnostic};
\end{tikzpicture}
\caption{The scaffolding diagnostic applied to vacuum energy. Absolute vacuum energies (left) depend on the regularization scheme and carry no empirical content. Energy \emph{differences} (right) are scheme-independent, $\BISH$-computable, and experimentally verified (Casimir effect).}
\label{fig:scaffolding-diag}
\end{figure}

\section{The RG Running of $\Lambda$}

The cosmological constant runs under the renormalization group: $\mu\, d\Lambda/d\mu = \beta_\Lambda(\mu)$. The beta function is a finite sum of contributions from all particles below scale $\mu$---algebraic functions of masses and couplings. Integrating the RG equation is a first-order ODE with $\BISH$-computable coefficients; by Picard--Lindel\"of, the running $\Lambda(\mu)$ is $\BISH$-computable. $\LPO$ enters only when the QCD coupling becomes strong ($\mu \sim \Lambda_{\mathrm{QCD}} \sim 200\;\mathrm{MeV}$) and the non-perturbative condensate is needed (Fekete, $\LPO$). Above the QCD scale: $\BISH$. Below: $\LPO$. The same stratification found throughout the program.

\section{Summary}

The cosmological constant problem, under constructive reverse mathematics, decomposes cleanly (\cref{fig:cc-decomposition}). The $10^{120}$ narrative is dissolved (regulator artifact). The naturalness argument is reclassified (non-mathematical). The genuine constraint is the 55-digit fine-tuning---an arithmetic relation between $\LPO$-computable reals, identical in logical character to every other thermodynamic limit.

The framework does not explain \emph{why} the cancellation occurs. It shows that ``why?'' is a question about initial conditions or the UV completion of gravity, not about the logical structure of QFT. The $\BISH+\LPO$ ceiling holds. The cosmological constant problem introduces no new logical resources.

\textbf{Scope:} this analysis addresses the ``old'' cosmological constant problem---the fine-tuning of $\Lambda_{\mathrm{bare}}$ against vacuum energy contributions. The ``coincidence problem'' (why $\rho_\Lambda \sim \rho_{\mathrm{matter}}$ today) and the question of time-varying dark energy (preliminary DESI hints~\cite{DESI2024} at $2.5$--$3.9\sigma$) are distinct questions. If dark energy evolves in time, the additional dynamical component would remain $\BISH$ if perturbative (e.g., a slowly rolling scalar field) and would reach $\LPO$ only if a thermodynamic limit were involved (e.g., a condensate phase transition).

\begin{chapterbox}
The $10^{120}$ discrepancy is dissolved (regulator-dependent artifact). Naturalness is reclassified (non-mathematical Bayesian prior). The genuine constraint is 55-digit fine-tuning---an arithmetic relation between $\LPO$-computable reals, logically identical to every other thermodynamic limit. The $\BISH+\LPO$ ceiling holds.
\end{chapterbox}

% ======================================================================
%  CHAPTER 13
% ======================================================================
\chapter{The Formalization}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  level/.style={rounded corners=3pt, minimum height=0.9cm, font=\small, text centered, draw},
]
% Level 1 (top, narrowest)
\node[level, fill=green!15, minimum width=6cm] (L1) at (0,3)
  {\textbf{Level 1:} Fully verified ($\BISH$)};
% Level 2
\node[level, fill=cyan!12, minimum width=8cm] (L2) at (0,1.8)
  {\textbf{Level 2:} Structurally verified (Mathlib classical only)};
% Level 3
\node[level, fill=blue!12, minimum width=10cm] (L3) at (0,0.6)
  {\textbf{Level 3:} Intentional classical ($\LPO$/$\WLPO$/$\LLPO$ hypothesis)};
% Level 4 (bottom, widest)
\node[level, fill=gray!15, minimum width=12cm] (L4) at (0,-0.6)
  {\textbf{Level 4:} Axiomatized physics (bridge axioms)};

% Arrows
\draw[-{Stealth}, thick, green!60!black] (6.5,-0.6) -- (6.5,3)
  node[midway, right, font=\small] {Increasing trust};
\draw[-{Stealth}, thick, blue!60] (-6.5,3) -- (-6.5,-0.6)
  node[midway, left, font=\small] {Increasing scope};
\end{tikzpicture}
\caption{The four certification levels. Level~1 (fully verified) is pure $\BISH$; Level~4 (axiomatized physics) declares physical assumptions as explicit axioms. The pyramid reflects the trade-off: constructive purity increases upward; physical scope increases downward.}
\label{fig:certification}
\end{figure}

\section{Why Formal Verification}

The formal verification in Lean~4~\cite{deMoura2021} is not an optional supplement---it is essential infrastructure. The distinctions between $\BISH$, $\WLPO$, $\LLPO$, and $\LPO$ are too fine for informal mathematical reasoning. These principles are logically close: $\LLPO$ and $\WLPO$ differ in one quantifier placement; $\WLPO$ is strictly weaker than $\LPO$ but implies it for bounded monotone sequences; the boundary between ``constructive proof with $\LPO$ hypothesis'' and ``classical proof using $\LPO$ as a theorem'' is invisible in natural-language mathematics but precisely detectable by a proof assistant.

The LPO-weakening incident (Paper~2) demonstrated the fragility. An AI coding agent, uncomfortable with the meta-classical producer/consumer architecture, replaced a classical metatheoretic argument with object-level $\LPO$. The resulting proof compiled without errors and appeared correct on informal review. But the calibration was silently destroyed: $\LPO$ implies $\WLPO$, so the bidual gap hypothesis became logically redundant, and the theorem---supposed to show the gap has exactly the strength of $\WLPO$---became vacuously true. The error was caught only by examining \texttt{\#print axioms}. Without formal verification, the error would have persisted indefinitely. The lesson: fine distinctions between $\WLPO$ and $\LPO$ are invisible to informal reasoning and mechanically obvious to Lean.

\section{The Producer/Consumer Architecture}

Constructive reverse mathematics proofs operate in a classical metatheory, adopting a ``producer/consumer'' architecture inspired by dependency injection. The \textbf{producer} operates classically: given a physical theorem $T$ and a candidate principle $P$ (e.g., $\LPO$), the producer constructs a concrete artifact---a specific binary sequence, bounded monotone sequence, or real number---using whatever non-constructive reasoning is convenient. The \textbf{consumer} takes this artifact as input and derives $P$ from $T$ using only $\BISH$. The consumer's proof is constructive: it shows that the physical theorem, applied to the producer's artifact, yields the omniscience principle.

The Lean formalization validates this mechanically. Running \texttt{\#print axioms} on the consumer's theorem shows no \texttt{Classical.choice}---the derivation is constructive. Running it on the producer's construction shows \texttt{Classical.choice}---the artifact was built non-constructively. The two components are independently checkable.

\section{Certification Levels}

The program uses four certification levels:

\textbf{Level 1 (Fully verified):} No \texttt{Classical.choice}, no custom axioms. Mechanically certified $\BISH$. \emph{Examples:} finite-lattice partition functions, tree-level scattering amplitudes, Born rule at finite precision.

\textbf{Level 2 (Structurally verified):} \texttt{Classical.choice} only from Mathlib infrastructure ($\RR$ via Cauchy completion). The theorem itself is constructive; the classical content is in the library, not the physics. \emph{Examples:} most calibration theorems involving real-number arithmetic.

\textbf{Level 3 (Intentional classical):} \texttt{Classical.choice} from an explicit $\LPO$/$\WLPO$/$\LLPO$ hypothesis. The non-constructive content \emph{is} the theorem's content---``assuming $\LPO$, we prove $X$.'' \emph{Examples:} Fekete's lemma, thermodynamic limits, coupling constant existence.

\textbf{Level 4 (Axiomatized physics):} Explicit \texttt{axiom} declarations for physical assumptions stated but not proved. \emph{Examples:} Yang--Mills existence and mass gap, string tension positivity.

\section{The Codebase}

The codebase comprises approximately 35{,}000 lines of Lean~4 across 42~paper repositories, organized in five phases:
\begin{itemize}[nosep]
\item \textbf{Phase~I} (Papers~2--28): $\sim$20{,}000 lines---initial calibration establishing the empirical pattern.
\item \textbf{Phase~II} (Papers~29--31): $\sim$4{,}000 lines---three foundational theorems (Fekete~$\equiv$~$\LPO$, $\FT$ dispensability, $\DC$ dispensability).
\item \textbf{Phase~III} (Papers~32--35): $\sim$6{,}000 lines---Standard Model calibration and conservation metatheorem.
\item \textbf{Phase~IV} (Papers~36--39): $\sim$2{,}690 lines---undecidability arc and thermodynamic stratification.
\item \textbf{Phase~V} (Papers~41--42): $\sim$1{,}600 lines---AdS/CFT and cosmological constant diagnostics.
\end{itemize}
All code publicly available on Zenodo with individual DOIs (Appendix~A). All core calibration results compile without \texttt{sorry}.

\begin{chapterbox}
Formal verification in \Lean\ is essential infrastructure: the $\BISH$/$\WLPO$/$\LPO$ distinctions are too fine for informal reasoning. Four certification levels separate fully verified ($\BISH$, Level~1) from infrastructure classical (Level~2), intentional classical ($\LPO$ hypothesis, Level~3), and axiomatized physics (Level~4). Approximately 35{,}000 lines across 42 papers, zero \texttt{sorry}.
\end{chapterbox}

% ======================================================================
%  CHAPTER 14
% ======================================================================
\chapter{What This Program Does Not Do}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  box/.style={rounded corners=3pt, minimum width=5cm, minimum height=0.5cm, font=\small, text centered},
  item/.style={font=\small, anchor=west},
]
% Left column
\node[box, draw=green!60!black, fill=green!8, minimum height=4cm] (left) at (-3.5,0) {};
\node[font=\small\bfseries, green!60!black] at (-3.5,2.3) {What the program does};
\node[item] at (-5.8,1.5) {Characterizes prediction (epistemology)};
\node[item] at (-5.8,0.8) {Calibrates 42 papers in \Lean};
\node[item] at (-5.8,0.1) {Machine-checks every theorem};
\node[item] at (-5.8,-0.6) {Identifies logical costs};
\node[item] at (-5.8,-1.3) {Reclassifies open problems};

% Right column
\node[box, draw=red!60, fill=red!5, minimum height=4cm] (right) at (3.5,0) {};
\node[font=\small\bfseries, red!60] at (3.5,2.3) {What it does not do};
\node[item] at (1.2,1.5) {Characterize generation (ontology)};
\node[item] at (1.2,0.8) {Prove exhaustiveness};
\node[item] at (1.2,0.1) {Guarantee physical correctness};
\node[item] at (1.2,-0.6) {Resolve the measurement problem};
\node[item] at (1.2,-1.3) {Explain why $\Lambda$ has its value};

% Divider
\draw[line width=1.5pt, gray] (0,-2.2) -- (0,2.2);
\node[font=\scriptsize\bfseries, gray, rotate=90] at (0.35,0) {Scope boundary};
\end{tikzpicture}
\caption{The scope of the program. Left: what the program establishes (prediction characterization, machine verification, reclassification). Right: what lies beyond its scope (ontology, physical correctness, problem resolution).}
\label{fig:scope}
\end{figure}

\section{Epistemology vs.\ Ontology}

The program characterizes the logical resources needed to \emph{predict} physical behavior---to compute the numbers that experimentalists compare to measurements. It does not determine the logical resources the universe \emph{uses} to \emph{generate} that behavior. These are fundamentally different questions.

An analogy: chess can be played on a physical board or simulated on a computer. The logical resources needed to \emph{predict} chess outcomes are well-defined computational problems. The resources the physical board ``uses'' to ``generate'' outcomes are entirely different. The program addresses prediction, not generation.

The epistemological reading is robust: $\BISH+\LPO$ suffices for all known empirical predictions. The ontological reading---that nature's computational architecture \emph{is} $\BISH+\LPO$---would require additional philosophical argument that the program does not provide and does not claim to provide. Paper~29's argument (phase transitions require $\LPO$, phase transitions are real, therefore $\LPO$ is physically instantiated) comes closest to an ontological claim, but it is carefully hedged: $\LPO$ is necessary for the mathematical \emph{description}, not necessarily for the physical mechanism.

\section{Completeness}

The calibration is not exhaustive. Significant gaps remain:
\begin{itemize}[nosep]
\item Multi-loop scattering amplitudes beyond one loop (the metatheorem predicts $\BISH$ at each fixed order, but this is unverified in Lean beyond one loop).
\item Non-perturbative QCD observables beyond confinement and the mass gap---hadron spectroscopy, glueball properties, chiral symmetry breaking.
\item Condensed matter physics---topological phases, the fractional quantum Hall effect, spin liquids, topological insulators. The metatheorem predicts $\BISH+\LPO$, but the prediction is untested.
\item Quantum gravity predictions---Ryu--Takayanagi beyond what Chapter~11 covers, cosmological observables, black hole information.
\end{itemize}
The thermodynamic stratification theorem (Chapter~9) predicts that intensive observables in uncalibrated systems may reach $\Sigma^0_2$ at the Platonic level but that empirical predictions will remain at $\LPO$. Confirming or refuting these predictions is the natural next phase.

\section{Physical Correctness}

Lean verifies logical consistency, not physical correctness. The program's bridge lemmas---axioms encoding physical assumptions (e.g., ``the partition function satisfies the transfer matrix recursion'')---are stated as explicit \texttt{axiom} declarations. If a bridge lemma encodes incorrect physics, the Lean proof is logically valid but physically meaningless. Lean guarantees the calibration \emph{follows from} the physics assumptions; it does not guarantee the assumptions are true.

The program assumes standard physics: the Standard Model is correct (at least at tested energies), general relativity describes gravity (at least outside singularities), and statistical mechanics describes thermodynamic systems. The logical characterization is conditional: ``if known physics is correct, then empirical physics $= \BISH+\LPO$.''

\section{Problem Resolution}

The program reclassifies several open problems---the measurement problem, the cosmological constant, physical undecidability---but reclassification is not resolution. The measurement problem, even after reclassification as a disagreement about dispensable scaffolding, still involves a genuine question: why does the macroscopic world \emph{appear} classical? The program says the disagreement is about framework choice, not physical reality, but framework choice still matters for how physicists develop theories.

Similarly, reclassifying the cosmological constant discrepancy as an ``artifact of idealization'' does not explain why $\Lambda$ has its observed value. Reclassifying physical undecidability as ``$\LPO$ non-computability'' does not make the spectral gap computable. The program changes the \emph{character} of these problems---it identifies what kind of problem each is---but solving a reclassified problem still requires new physics or mathematics that the program does not provide.

\begin{chapterbox}
The program characterizes prediction, not generation. Reclassification is not resolution. The calibration is not exhaustive. \Lean\ verifies logical consistency, not physical correctness. Significant gaps remain---and the thesis is empirical: falsifiable by a single counterexample.
\end{chapterbox}

% ======================================================================
%  CHAPTER 15
% ======================================================================
\chapter{Conclusion}

\section{Summary}

Two narrative threads converge on a single conclusion. The calibration thread: across 42~papers, every empirical prediction requires at most $\BISH+\LPO$. Fekete $\equiv \LPO$ proves necessity. $\FT$ and $\DC$ dispensability proves sufficiency. The undecidability thread: every known physical undecidability result is $\LPO$-equivalent, traceable to Wang tiling. The diagnostic thread (Papers~41--42): the framework actively diagnoses the AdS/CFT correspondence and the cosmological constant problem.

$\BISH+\LPO$ is what physics costs, and $\LPO$ is what physics cannot compute uniformly. The cost and the failure are the same principle.

\section{Future Directions}

\textbf{Defensive:} Higher-loop amplitudes, lattice QCD spectroscopy, condensed matter topological phases, multi-loop AdS/CFT.

\textbf{Offensive:} Physical constants as $\Delta^0_2$, measurement problem resolution, quantum gravity predictions.

\textbf{Frontier:} $\Sigma^0_2$ laboratory observables---the most consequential discovery the program could provoke.

\section{Bishop's Legacy}

Errett Bishop published \emph{Foundations of Constructive Analysis} in 1967. He defined $\BISH$ as a philosophical program. The omniscience principles were identified by Bishop and his successors as the precise points where classical mathematics departs from constructive practice. This was purely logical cartography, with no physical motivation.

That this cartography---designed for pure mathematics in the 1960s--1990s---turns out to classify the non-constructive content of physics with perfect precision across 42~papers and 35{,}000 lines of formal verification is the most striking fact about the program. The fit is discovered, not designed. That is what makes it worth reporting.

For the detailed synthesis of the first 29 calibrations and the historical narrative connecting Bishop's constructive analysis to the present program, the reader is referred to Papers~10~\cite{Lee2026P10} and~12~\cite{Lee2026P12}.

\begin{chapterbox}
$\BISH+\LPO$ is the cost physics pays and the ceiling it cannot exceed. The cost and the failure are the same principle. That Bishop's cartography---designed for pure mathematics---classifies empirical physics with perfect precision across 42~papers is the most striking fact about the program.
\end{chapterbox}

% ======================================================================
%  APPENDICES
% ======================================================================
\appendix

\chapter{Complete Calibration Table}

{\small
\begin{longtable}{@{}clp{3.2cm}p{3cm}lp{2cm}@{}}
\toprule
\textbf{Paper} & \textbf{Domain} & \textbf{Title} & \textbf{Key Theorem} & \textbf{Height} & \textbf{DOI (Zenodo)} \\
\midrule
\endhead
\bottomrule
\endfoot
2  & FA  & Bidual gap          & $X \cong X^{**} \leftrightarrow \WLPO$ & $\WLPO$ & 17107493 \\
4  & QM  & Quantum spectra     & Spectral thm calibration & $\LPO$ & 17059483 \\
5  & GR  & Schwarzschild       & Geodesic computation & $\BISH$ & 18489703 \\
6  & QM  & Heisenberg          & $\Delta x\Delta p \ge \hbar/2$ & $\BISH/\LPO$ & 18519836 \\
7  & FA  & Trace-class         & Physical bidual gap & $\WLPO$ & 18527559 \\
8  & SM  & 1D Ising            & Partition / thermo limit & $\BISH/\LPO$ & 18516813 \\
9  & SM  & Ising invariance    & Transfer $\leftrightarrow$ direct & $\LPO$ & 18517570 \\
11 & QI  & CHSH/Tsirelson      & Bell $\leftrightarrow$ $\LLPO$ & $\LLPO$ & 18527676 \\
13 & GR  & Event horizon       & $r=2M$ logical boundary & $\WLPO/\LPO$ & 18529007 \\
14 & QM  & Decoherence         & Partial trace calibration & $\BISH/\LPO$ & 18569068 \\
15 & GR  & Noether             & Local vs.\ global & $\BISH/\LPO$ & 18572494 \\
16 & QM  & Born rule           & $P=|\langle\psi|\varphi\rangle|^2$ & $\BISH/\LPO$ & 18575377 \\
17 & GR  & Bekenstein--Hawking & $S=A/4$ as thermo limit & $\LPO$ & 18597306 \\
18 & QFT & Yukawa RG           & SM one-loop stratification & $\BISH/\WLPO/\LPO$ & 18626839 \\
19 & QM  & WKB tunneling       & Tunneling probability & $\LLPO$ & 18602596 \\
20 & SM  & Ising magnetization & Observable-dependent cost & $\WLPO$ & 18603079 \\
21 & QI  & Bell nonlocality    & Bell $\leftrightarrow$ $\LLPO$ tight & $\LLPO$ & 18603251 \\
22 & QM  & Radioactive decay   & Markov's Principle & $\MP \subset \LPO$ & 18603503 \\
23 & QM  & Measurement problem & Interpretations vs.\ $\DC$ & $\DC$ (disp.) & 18604312 \\
24 & QI  & Kochen--Specker     & Contextuality & $\LLPO$ & 18604317 \\
25 & SM  & Ergodic theorems    & MET $\leftrightarrow$ $\CCax$ & $\CCax \subset \LPO$ & 18615453 \\
26 & FA  & G\"odel sequences   & Bidual detection & $\WLPO$ & 18615457 \\
27 & QI  & IVT in Bell physics & $\LLPO$ mechanism & $\LLPO$ & 18615459 \\
28 & CM  & Classical mechanics  & Newton $\BISH$ / Lagrange $\FT$ & $\BISH/\FT$ (disp.) & 18616620 \\
29 & SM  & Fekete's lemma      & Fekete $\leftrightarrow$ $\LPO$ & $\LPO$ & 18643617 \\
30 & Fnd & FT dispensability   & $\FT$ dispensable & $\BISH+\LPO$ & 18638394 \\
31 & Fnd & DC dispensability   & $\DC$ dispensable & $\BISH+\LPO$ & 18645578 \\
32 & QFT & QED one-loop        & Running coupling & $\BISH/\LPO$ & 18642598 \\
33 & QFT & QCD + confinement   & Mass gap & $\BISH/\LPO$ & 18642610 \\
34 & QFT & Scattering amplitudes & Fixed-order BISH & $\BISH$ & 18642612 \\
35 & Meta & Metatheorem        & Thms A--D & $\LPO$ ceiling & 18642616 \\
36 & Und & Cubitt $\equiv \LPO$ & Spectral gap & $\LPO$ & 18642620 \\
37 & Und & Undecidability landscape & All reductions & $\LPO$ & 18642802 \\
38 & Und & Wang tiling         & Grandfather thm & $\LPO$ & 18642804 \\
39 & Und & Beyond LPO          & Thermo stratification & $\LPO/\Sigma^0_2$ & 18642806 \\
40 & Mono & This monograph       & $\BISH+\LPO$ thesis & --- & 18654773 \\
41 & AdS & AdS/CFT diagnostic  & Holographic dictionary & $\BISH/\LLPO/\LPO$ & 18654780 \\
42 & CC  & Cosmological constant & CC problem decomposed & $\BISH/\LPO$ & 18654789 \\
\end{longtable}
}

Abbreviations: FA = Functional Analysis, QM = Quantum Mechanics, GR = General Relativity, SM = Statistical Mechanics, QFT = Quantum Field Theory, QI = Quantum Information, CM = Classical Mechanics, Fnd = Foundations, Meta = Metatheory, Und = Undecidability, Mono = Monograph, AdS = AdS/CFT, CC = Cosmology. Papers~1 and~3 withdrawn; Papers~10 and~12 are interpretive syntheses (see~\cite{Lee2026P10,Lee2026P12}). All DOIs are under prefix \texttt{10.5281/zenodo.}

\chapter{Lean Code Availability}

Each paper has a self-contained \Lean\ project:
\begin{lstlisting}[language=bash,backgroundcolor=\color{white},numbers=none]
# Build any paper
cd P{N}_{Name}
lake build

# Verify axiom profile
# In any .lean file, add:  #print axioms theorem_name
\end{lstlisting}

All code is available on Zenodo with individual DOIs. The \texttt{lean-toolchain} file pins \texttt{leanprover/lean4:v4.28.0-rc1}.

\medskip\noindent
\textbf{Data availability.}
This monograph and its associated source materials are archived at
\href{https://doi.org/10.5281/zenodo.18654773}{doi:10.5281/zenodo.18654773}.

% ======================================================================
%  APPENDIX C: PROGRAMME SIGNIFICANCE GUIDE
% ======================================================================
\chapter{Program Significance Guide}
\label{app:significance}

This appendix provides a structured guide to the papers of the Constructive Reverse Mathematics and Physics program.  Each paper is classified by its structural role and its principal finding is stated in a single sentence.  The classification reflects the paper's contribution to the program as a whole, not its standalone technical difficulty.

\bigskip\noindent
\textbf{Rating key.}\quad
\begin{itemize}
\item[\FiveStar] Program-defining: the program cannot exist without this result
\item[\FourStar] Major structural contribution
\item[\ThreeStar] Significant domain extension
\item[\TwoStar] Important supporting calibration
\item[\OneStar] Confirmatory or technical
\end{itemize}

% ------------------------------------------------------------------
\section{On the Necessity of Completeness}
% ------------------------------------------------------------------

Individual calibrations in constructive reverse mathematics are, taken singly, unsurprising to specialists.  That finite matrix arithmetic is $\BISH$, that bounded monotone convergence requires $\LPO$, that the extreme value theorem requires $\FT$---these are textbook-level observations in the constructive analysis community, implicit in the work of Bishop~\cite{Bishop1967}, Bridges and Richman~\cite{BridgesRichman1987}, and Ishihara~\cite{Ishihara2006}.  No single entry in the calibration table constitutes a discovery.

\emph{The discovery is the table.}

The $\BISH+\LPO$ ceiling, the universality of Fekete's lemma as the sole mechanism for $\LPO$ entry, the dispensability of the Fan Theorem and Dependent Choice for every empirical prediction examined, the structural alignment between the $\BISH$/$\LPO$ boundary and the perturbative/non-perturbative boundary in quantum field theory---none of these findings is visible from any single calibration, or from any five, or from any ten.  They become visible only when the calibration is carried out systematically across the full landscape of mathematical physics: statistical mechanics, quantum mechanics, quantum field theory, general relativity, quantum information, classical mechanics, electrodynamics, particle physics, quantum gravity, holographic duality, and vacuum energy.

The program required approximately 60 calibrations across 12 domains before the pattern emerged with sufficient clarity to state as a thesis.  This is why the work had not been done before: not because the individual calibrations are difficult---most can be reproduced by a specialist in an afternoon---but because the constructive analysis community and the mathematical physics community do not overlap, and neither community had reason to expect that a systematic survey would reveal a universal pattern.  The present program is, to the author's knowledge, the first such survey, and the patterns it reveals---the ceiling, the universality, the dispensability---are its principal contribution.

% ------------------------------------------------------------------
\section{Principal Findings}
% ------------------------------------------------------------------

Before cataloguing individual papers, we state the eleven findings that emerge from the calibration table taken as a whole.  Each is a factual summary of results already established in the cited papers; none requires speculation.

\subsection*{1.\ The BISH\,+\,LPO Ceiling}

Across approximately 60 calibrations in 12 physical domains, every empirical prediction requires at most $\BISH+\LPO$.  No empirical prediction in the program has required $\LLPO$, $\WLPO$, $\FT$, or $\DC$ as irreducible logical cost.  This is an observed pattern, not an assumed constraint (Paper~35; calibration table in Paper~10~\cite{Lee2026P10}).

\medskip\noindent
\textbf{Why this matters.}\quad
The entire mathematical apparatus of modern physics---Hilbert spaces, differential geometry, functional analysis, quantum field theory, general relativity---uses logical resources vastly exceeding $\BISH+\LPO$.  Yet every number that can be compared to an experimental measurement requires only two ingredients: constructive computation ($\BISH$) and one omniscience principle ($\LPO$).  The gap between the mathematics physicists \emph{use} and the mathematics physics \emph{needs} is enormous, and precisely measurable.  This is the program's central empirical discovery.

\subsection*{2.\ Fan Theorem Dispensability}

Compactness arguments---Arzel\`a--Ascoli, Banach--Alaoglu, the direct method of the calculus of variations, the existence of minimal surfaces---appear throughout mathematical physics but have not been required for any empirical prediction examined.  $\FT$ governs the existence of extremisers, not the values of extrema (Papers~30, 23, 41).

\medskip\noindent
\textbf{Why this matters.}\quad
Compactness is the single most widely used tool in mathematical physics---it underpins the existence theorems for PDEs, the variational principles of mechanics, the convergence theorems of functional analysis, and the entirety of global differential geometry.  Showing that none of this is needed for empirical predictions is a finding of the first magnitude: 150~years of mathematical infrastructure built on compactness is scaffolding over a simpler constructive core.

\subsection*{3.\ Dependent Choice Dispensability}

$\DC$ governs the strong law of large numbers and sequential construction arguments.  Born probabilities are $\BISH$; the frequentist convergence theorem requires $\DC$.  No empirical prediction in the program has required $\DC$ (Papers~31, 25, 16).

\medskip\noindent
\textbf{Why this matters.}\quad
Dependent Choice is the axiom that lets you build infinite sequences one step at a time---it is the engine behind every ``take a sequence of approximations and extract a convergent subsequence'' argument in analysis.  Showing that $\DC$ is dispensable for empirical predictions means that the entire statistical interpretation of quantum mechanics---the bridge from Born probabilities to observed frequencies---is scaffolding.  Physics predicts individual probabilities ($\BISH$); the story about what those probabilities ``mean'' in the long run costs $\DC$ and is logically unnecessary.

\subsection*{4.\ Fekete Universality}

$\LPO$ enters physics through a single mechanism: the thermodynamic limit, mediated by Fekete's subadditive lemma (Fekete $\equiv$ $\LPO$, Paper~29).  This mechanism is responsible for the $\LPO$ cost in every domain where $\LPO$ appears---the Ising phase transition (Paper~8), geodesic completeness (Paper~13), decoherence (Paper~14), Noether conservation laws (Paper~15), QCD confinement (Paper~33), and vacuum condensates (Paper~42).

\medskip\noindent
\textbf{Why this matters.}\quad
Before this program, the non-constructive content of physics appeared to be scattered---a different obstacle in every domain.  Fekete universality reveals that there is exactly one source of non-constructivity across all of physics: the passage from finite to infinite systems.  Phase transitions, black hole horizons, decoherence, confinement, vacuum condensates---superficially unrelated phenomena across statistical mechanics, general relativity, quantum mechanics, and quantum field theory---all share a single logical mechanism.  This is a unification result: the logical structure of physics is far simpler than its mathematical diversity suggests.

\subsection*{5.\ The Scaffolding Diagnostic Works}

The framework correctly separates physically meaningful quantities from mathematical artifacts in cases where the answer is independently known.  Casimir energy differences are $\BISH$ and experimentally verified; absolute vacuum energies are regulator-dependent and physically meaningless (Paper~42).  Holographic entropy values are $\BISH$; minimal surface existence is $\FT$ scaffolding that holography eliminates (Paper~41).

\medskip\noindent
\textbf{Why this matters.}\quad
A diagnostic framework is only as good as its ability to recover known answers.  The scaffolding diagnostic passes its most demanding tests: it correctly classifies Casimir differences as physical ($\BISH$, experimentally verified) and absolute vacuum energies as unphysical (regulator-dependent, no convergence at any constructive level).  It correctly identifies holographic entropy as the physical observable ($\BISH$) and the bulk minimal surface as mathematical scaffolding ($\FT$)---exactly what AdS/CFT practitioners already knew from the boundary theory.  This track record is what justifies applying the diagnostic to open questions where the answer is not yet known.

\subsection*{6.\ Dissolution of the $10^{120}$}

The cosmological constant ``prediction'' fails the scaffolding diagnostic.  The unregularised vacuum energy does not converge to a real number at any level of the constructive hierarchy.  Different regulators produce different finite values.  A regulator-dependent quantity has no empirical content within the framework (Paper~42).

\medskip\noindent
\textbf{Why this matters.}\quad
The 120-order-of-magnitude discrepancy between quantum field theory's ``prediction'' of vacuum energy and observation has been called the worst prediction in the history of physics.  It has driven decades of research into supersymmetry, anthropic landscape arguments, and quintessence models.  The CRM analysis shows that this entire research program may be addressing a non-problem: the quantity that disagrees with observation does not converge to a well-defined real number in the first place.  The ``prediction'' is an artifact of treating a regulator-dependent intermediate step as if it were a physical quantity.  This is not a philosophical opinion---it is a theorem about the convergence properties of the unregularised sum.

\subsection*{7.\ Duality as Axiom-Preserving Map}

Paper~41 is the first test of a physical duality for logical consistency at the level of individual computational steps.  The holographic dictionary preserves axiom cost across the bulk--boundary correspondence for every entry examined.  This is a structural constraint on AdS/CFT that has not previously been articulated.

\medskip\noindent
\textbf{Why this matters.}\quad
Dualities are the most powerful organising principle in modern theoretical physics.  AdS/CFT is the most studied duality of the past quarter-century.  Yet no one had previously asked whether the dictionary preserves logical structure at the level of individual axioms.  Finding that it does reveals a new structural property of holography: it is not merely a map between physical quantities but a map that preserves the constructive content of computations.  This opens the question of whether axiom-preservation is a feature of all physical dualities or a special property of AdS/CFT.

\subsection*{8.\ Physical Undecidability Is Thermodynamic}

The spectral gap undecidability of Cubitt--Perez-Garcia--Wolf~\cite{Cubitt2015}, the most celebrated undecidability result in mathematical physics, reduces to $\LPO$---the same principle governing every thermodynamic limit in the program (Papers~36--39).  Extensive observables at full precision cap at $\LPO$; intensive observables without promise gaps can reach $\LPO'$ ($\Sigma^0_2$).

\medskip\noindent
\textbf{Why this matters.}\quad
The spectral gap result made international headlines as evidence that physics harbours deep undecidability.  The natural reading was that physics is logically wild---potentially reaching arbitrary levels of the arithmetical hierarchy.  The CRM analysis shows the opposite: physical undecidability is tame.  It sits at exactly the same level ($\LPO$) as every thermodynamic limit in statistical mechanics.  The most undecidable thing in physics is precisely as undecidable as a boiling pot of water.

\subsection*{9.\ Bell $\equiv$ Kochen--Specker}

Bell's theorem~\cite{Bell1964} and the Kochen--Specker theorem calibrate at the same level ($\LLPO$) for a structural reason: both reduce to instances of the intermediate value theorem.  $\LLPO$ enters quantum foundations through exactly one door---the IVT---just as $\LPO$ enters thermodynamics through Fekete (Papers~21, 24, 27).

\medskip\noindent
\textbf{Why this matters.}\quad
Bell's theorem (nonlocality) and the Kochen--Specker theorem (contextuality) have been treated as separate pillars of quantum foundations for sixty years.  The CRM analysis reveals that they are the same theorem at the logical level---both are instances of $\LLPO$ mediated by the intermediate value theorem.  This is a genuine unification: two phenomena that appeared conceptually independent are logically identical.

\subsection*{10.\ The Perturbative/Non-Perturbative Boundary Is the BISH/LPO Boundary}

Perturbative QFT---tree-level amplitudes, one-loop corrections, RG running above $\Lambda_\mathrm{QCD}$---is $\BISH$ (algebraic operations on measured parameters).  Non-perturbative QFT---exact condensates, confinement scale, thermodynamic limits of lattice approximations---requires $\LPO$.  The $\BISH$/$\LPO$ boundary corresponds to the perturbative/non-perturbative boundary in QFT (Papers~32--34, 42).

\medskip\noindent
\textbf{Why this matters.}\quad
The perturbative/non-perturbative divide is the deepest methodological boundary in quantum field theory.  The fact that this physical boundary aligns precisely with the $\BISH$/$\LPO$ boundary in the constructive hierarchy is a striking structural alignment.  It means the distinction between ``what we can calculate perturbatively'' and ``what we cannot'' is not merely a practical limitation of current techniques---it is a logical boundary between finite algebraic computation ($\BISH$) and infinite-volume limits ($\LPO$).

\subsection*{11.\ Born Probabilities Are BISH}

The entire empirical content of quantum mechanics---every probability, every expectation value, every uncertainty bound---is $\BISH$-computable.  The measurement problem ($\LPO$), the frequentist interpretation ($\DC$), and the wavefunction collapse postulate are scaffolding over $\BISH$-computable predictions (Papers~6, 11, 16).

\medskip\noindent
\textbf{Why this matters.}\quad
The measurement problem has been called the central unsolved problem of quantum mechanics.  It has generated an entire field of interpretive debate (Copenhagen, many-worlds, pilot wave, decoherence, QBism).  The CRM analysis does not solve the measurement problem, but it shows that the measurement problem is logically orthogonal to empirical predictions.  Every number that can be compared to an experiment is $\BISH$-computable.  The measurement problem lives at $\LPO$.  The interpretive debates are debates about scaffolding: logically real, but empirically inert.

\subsection*{Open Question}

Whether the $\BISH+\LPO$ ceiling reflects a structural feature of physical law or a feature of its current mathematical formulations remains unresolved.  The Noether charge/energy asymmetry (Paper~15) is evidence for formulation-sensitivity; Paper~9's Ising invariance result is evidence against it.  The question of formulation-invariance is the program's principal open problem.

% ------------------------------------------------------------------
\section{Paper-by-Paper Catalogue}
% ------------------------------------------------------------------

\subsection{Program-Defining Results {\normalfont(\FiveStar)}}

\paragraph{Paper 29: Fekete's Subadditive Lemma Is Equivalent to LPO}
\emph{DOI: \texttt{10.5281/zenodo.18643617}.}\quad
The single most important technical result in the program.  Every $\LPO$ calibration in every domain flows through this equivalence.  Without Paper~29, the program has a collection of individual calibrations.  With it, the program has a universal mechanism.
\begin{quote}\itshape
$\LPO$ enters physics through exactly one door---the thermodynamic limit---and Fekete's lemma is the key.
\end{quote}

\paragraph{Paper 8: 1D Ising Model and LPO}
\emph{DOI: \texttt{10.5281/zenodo.18516813}.}\quad
The first complete bidirectional calibration: Ising thermodynamic limit $\leftrightarrow$ $\LPO$, proved in both directions with \Lean\ verification.  This is the template every subsequent $\LPO$ calibration follows.
\begin{quote}\itshape
The phase transition in the simplest statistical mechanical model is equivalent to the limited principle of omniscience.  Not ``requires''---equivalent.
\end{quote}

\paragraph{Paper 35: The Logical Constitution of Empirical Physics}
\emph{DOI: \texttt{10.5281/zenodo.18642616}.}\quad
The capstone theorem: $\BISH+\LPO$ is the complete logical constitution of empirically accessible physics across all domains examined.  This is the paper that states the ceiling as a thesis rather than a pattern.
\begin{quote}\itshape
Two principles---Bishop's constructive mathematics plus one omniscience principle---suffice for every empirical prediction in the Standard Model, general relativity, and quantum information theory.
\end{quote}

\paragraph{Paper 30: Physical Dispensability of the Fan Theorem}
\emph{DOI: \texttt{10.5281/zenodo.18638394}.}\quad
Every compactness argument in mathematical physics---variational existence, Arzel\`a--Ascoli, Banach--Alaoglu---is scaffolding.  No empirical prediction requires $\FT$.
\begin{quote}\itshape
150~years of compactness arguments in physics bought mathematical elegance at zero empirical cost.  The predictions survive without them.
\end{quote}

\paragraph{Paper 31: Physical Dispensability of Dependent Choice}
\emph{DOI: \texttt{10.5281/zenodo.18645578}.}\quad
$\DC$ governs sequential constructions and the strong law of large numbers.  No empirical prediction requires it.
\begin{quote}\itshape
The frequentist interpretation of probability is logically dispensable.  Born probabilities are $\BISH$.  The convergence theorem that justifies interpreting them as frequencies costs $\DC$ and is scaffolding.
\end{quote}

\paragraph{Paper 42: The Worst Prediction in Physics Is Not a Prediction}
\emph{DOI: \texttt{10.5281/zenodo.18654789}.}\quad
The strongest single application paper in the program.  It dissolves a famous problem rather than merely classifying a known result.  The unregularised vacuum energy does not converge to a real number at any level of the constructive hierarchy.  The Casimir effect proves the diagnostic works: energy differences are $\BISH$ and experimentally verified; absolute energies are regulator-dependent and physically meaningless.  The residual fine-tuning is an $\LPO$ equality---logically mundane.
\begin{quote}\itshape
The ``worst prediction in physics'' is not a prediction.  The $10^{120}$ is a property of a calculational choice, not of quantum field theory.
\end{quote}

% ------------------------------------------------------------------
\subsection{Summary and Synthesis Papers {\normalfont(\FiveStar)}}

\paragraph{Paper 10: The Logical Geography of Mathematical Physics}
\emph{DOI: \texttt{10.5281/zenodo.18636180}.}\quad
The calibration table itself---the reference document the entire program points to.  Contains the methodology section and the comprehensive table of approximately 60~entries across 12~domains~\cite{Lee2026P10}.

\paragraph{Paper 40: This Monograph}
\emph{DOI: \texttt{10.5281/zenodo.18654773}.}\quad
The synthesis and defence of the program.  Contains the eleven principal findings, the attack section addressing objections, and the articulation of why the calibration table matters.

\paragraph{Paper 12: The Constructive History of Mathematical Physics {\normalfont(\FourStar)}}
\emph{DOI: \texttt{10.5281/zenodo.18636250}.}\quad
The historical narrative: how non-constructive mathematics entered physics through 19th-century choices and what the calibration table means for the history and philosophy of physics~\cite{Lee2026P12}.

% ------------------------------------------------------------------
\subsection{Major Structural Contributions {\normalfont(\FourStar)}}

\paragraph{Paper 9: Ising Formulation-Invariance}
\emph{DOI: \texttt{10.5281/zenodo.18517570}.}\quad
The first and strongest test of whether the calibration tracks physics or formalism.  Two independent formalisations of the Ising model produce identical axiom profiles.
\begin{quote}\itshape
The logical cost is invariant under change of mathematical representation.  The hierarchy is detecting physics, not notation.
\end{quote}

\paragraph{Paper 36: Stratifying Spectral Gap Undecidability}
\emph{DOI: \texttt{10.5281/zenodo.18642620}.}\quad
The most celebrated undecidability result in mathematical physics reduces to $\LPO$.  The spectral gap problem, which generated international headlines, turns out to sit at precisely the same logical level as the Ising phase transition.
\begin{quote}\itshape
The most undecidable thing in physics is exactly as undecidable as a boiling pot of water.
\end{quote}

\paragraph{Paper 39: Beyond LPO---Thermodynamic Stratification of Physical Undecidability}
\emph{DOI: \texttt{10.5281/zenodo.18642806}.}\quad
Establishes the ceiling on physical undecidability: extensive observables cap at $\LPO$; intensive observables without promise gaps can reach $\LPO'$ ($\Sigma^0_2$).  The arithmetical hierarchy, which in pure mathematics extends without limit, is effectively truncated at the second level when applied to physical systems.
\begin{quote}\itshape
Physics is not just computable at $\BISH+\LPO$---its undecidability is bounded too.
\end{quote}

\paragraph{Paper 21: Bell Nonlocality and LLPO}
\emph{DOI: \texttt{10.5281/zenodo.18603251}.}\quad
Bell's theorem calibrates at $\LLPO$.
\begin{quote}\itshape
Quantum nonlocality---the most philosophically provocative feature of quantum mechanics---costs exactly the lesser limited principle of omniscience, strictly below the thermodynamic limit.
\end{quote}

\paragraph{Paper 24: Kochen--Specker Contextuality and LLPO}
\emph{DOI: \texttt{10.5281/zenodo.18604317}.}\quad
Kochen--Specker calibrates at the same level as Bell, for a structural reason---both reduce to IVT instances.
\begin{quote}\itshape
Bell and Kochen--Specker, treated as conceptually distinct by the foundations community, are logically identical from the CRM perspective.
\end{quote}

\paragraph{Paper 27: IVT as the Mechanism Behind LLPO in Bell Physics}
\emph{DOI: \texttt{10.5281/zenodo.18615459}.}\quad
Identifies the intermediate value theorem as the common mechanism unifying Bell and KS at $\LLPO$.
\begin{quote}\itshape
$\LLPO$ enters quantum foundations through exactly one door---the intermediate value theorem---just as $\LPO$ enters thermodynamics through Fekete.
\end{quote}

\paragraph{Paper 41: Axiom Calibration of AdS/CFT}
\emph{DOI: \texttt{10.5281/zenodo.18654780}.}\quad
The first deployment of the diagnostic on an active research frontier.  Holography preserves axiom cost across the bulk--boundary correspondence and eliminates the $\FT$ cost of bulk geometry.  The finding that the holographic dictionary preserves constructive content is a new structural constraint on AdS/CFT.
\begin{quote}\itshape
The holographic dictionary is an axiom-preserving map.  $\FT$ builds the bulk surface nobody observes; the boundary computes the entropy without it.
\end{quote}

\paragraph{Paper 2: Bidual Gap and WLPO}
\emph{DOI: \texttt{10.5281/zenodo.17107493}.}\quad
The first calibration in the program.  Established the methodology.
\begin{quote}\itshape
The existence of non-reflexive Banach spaces is equivalent to $\WLPO$.  The first entry in what became a 60-row table.
\end{quote}

\paragraph{Paper 26: Bidual Gap Detection Is WLPO-Complete---G\"odel Sequences}
\emph{DOI: \texttt{10.5281/zenodo.18615457}.}\quad
Proves $\WLPO$-completeness via arithmetic, independent of the functional analysis route.  Two independent proofs of the same calibration.

% ------------------------------------------------------------------
\subsection{Significant Domain Extensions {\normalfont(\ThreeStar)}}

\paragraph{Paper 13: Event Horizon as Logical Boundary}
\emph{DOI: \texttt{10.5281/zenodo.18529007}.}\quad
Geodesic completeness costs $\LPO$.  The event horizon is where the constructive hierarchy places its boundary---a striking alignment between a logical concept and a physical concept.
\begin{quote}\itshape
The logical boundary of constructive computability and the physical boundary of a black hole coincide.
\end{quote}

\paragraph{Paper 15: Noether Theorem}
\emph{DOI: \texttt{10.5281/zenodo.18572494}.}\quad
Conservation laws calibrate---but with the charge/energy asymmetry that partially challenges the strong thesis.
\begin{quote}\itshape
The Noether calibration is where the program discovered its own limitation.  Charge and energy have different logical costs due to sign-definiteness, not physics.
\end{quote}

\paragraph{Paper 33: QCD One-Loop Renormalization and Confinement}
\emph{DOI: \texttt{10.5281/zenodo.18642610}.}\quad
Lattice QCD calibrates at $\LPO$ via Fekete.  Confinement---the millennium-prize-adjacent phenomenon---is shown to have exactly the same logical structure as the Ising phase transition.
\begin{quote}\itshape
Confinement is logically identical to the Ising phase transition.  Same mechanism, same principle, same cost.
\end{quote}

\paragraph{Paper 32: QED One-Loop Renormalization and the Landau Pole}
\emph{DOI: \texttt{10.5281/zenodo.18642598}.}\quad
Perturbative QED is $\BISH$.  The Landau pole is $\LPO$.
\begin{quote}\itshape
The perturbative/non-perturbative boundary in QFT aligns with the $\BISH$/$\LPO$ boundary in the constructive hierarchy.
\end{quote}

\paragraph{Paper 34: Scattering Amplitudes Are Constructively Computable}
\emph{DOI: \texttt{10.5281/zenodo.18642612}.}\quad
Tree-level and one-loop amplitudes are $\BISH$.  Every number produced by the LHC's theoretical predictions is constructively computable without any omniscience principle.
\begin{quote}\itshape
Everything the LHC actually measures---cross-sections, branching ratios, decay rates---is $\BISH$-computable.  The Standard Model's empirical content is constructive.
\end{quote}

\paragraph{Paper 37: The Undecidability Landscape Is LPO}
\emph{DOI: \texttt{10.5281/zenodo.18642802}.}\quad
Surveys multiple undecidability results across physics and shows they all reduce to $\LPO$.  The significance is cumulative: any single result reducing to $\LPO$ might be coincidence, but the entire landscape reducing to the same level is structural.

\paragraph{Paper 19: WKB Tunneling and LLPO}
\emph{DOI: \texttt{10.5281/zenodo.18602596}.}\quad
Quantum tunneling turning points calibrate at $\LLPO$ via IVT.
\begin{quote}\itshape
The classical/quantum boundary in WKB---the turning point where classical motion stops and quantum tunneling begins---costs exactly $\LLPO$.
\end{quote}

\paragraph{Paper 23: Fan Theorem and Optimization}
\emph{DOI: \texttt{10.5281/zenodo.18604312}.}\quad
$\FT \leftrightarrow$ compact optimisation.  Establishes what $\FT$ does so that Papers~30--31 can show it is dispensable.
\begin{quote}\itshape
$\FT$ is the logical price of asserting that continuous functions on compact sets attain their extrema.  Physics computes the extremal values without paying this price.
\end{quote}

% ------------------------------------------------------------------
\subsection{Important Supporting Calibrations {\normalfont(\TwoStar)}}

\paragraph{Paper 14: Quantum Decoherence}
\emph{DOI: \texttt{10.5281/zenodo.18569068}.}\quad
Decoherence costs $\LPO$ (thermodynamic limit of the environment).  The quantum-to-classical transition is not logically special---it integrates into the Fekete universality pattern.

\paragraph{Paper 17: Bekenstein--Hawking Formula}
\emph{DOI: \texttt{10.5281/zenodo.18597306}.}\quad
Black hole entropy calibrates at $\BISH$ (area formula) to $\LPO$ (thermodynamic origin).
\begin{quote}\itshape
The most famous formula in quantum gravity is $\BISH$ when you read the area; $\LPO$ when you derive the entropy from microstates.
\end{quote}

\paragraph{Paper 20: Observable-Dependent Logical Cost}
\emph{DOI: \texttt{10.5281/zenodo.18603079}.}\quad
Different observables of the same system have different costs.  Logical cost is observable-dependent, not system-dependent---the correct unit of analysis for the entire program.
\begin{quote}\itshape
The partition function costs $\LPO$; the magnetisation costs $\WLPO$; the finite-size energy costs $\BISH$.  Same Ising model, three different levels.
\end{quote}

\paragraph{Paper 25: Ergodic Theorems Against Countable and Dependent Choice}
\emph{DOI: \texttt{10.5281/zenodo.18615453}.}\quad
Establishes $\DC$ dispensability for ergodic theory.
\begin{quote}\itshape
The ergodic theorem---the foundation of statistical mechanics---requires $\DC$, but the physical predictions it enables don't.
\end{quote}

\paragraph{Paper 28: Classical Mechanics}
\emph{DOI: \texttt{10.5281/zenodo.18616620}.}\quad
Newton (ODE, $\BISH$) vs.\ Lagrange (variational, $\FT$).
\begin{quote}\itshape
The Newtonian and Lagrangian formulations of classical mechanics are constructively stratified---the equations are $\BISH$, the optimisation principle is $\FT$ scaffolding.
\end{quote}

\paragraph{Paper 18: Standard Model Yukawa RG}
\emph{DOI: \texttt{10.5281/zenodo.18626839}.}\quad
Standard Model Yukawa running is $\BISH$.
\begin{quote}\itshape
The mass hierarchy of the Standard Model---why the top quark is heavy and the electron is light---is perturbatively $\BISH$-computable.
\end{quote}

\paragraph{Paper 22: Markov's Principle and Radioactive Decay}
\emph{DOI: \texttt{10.5281/zenodo.18603503}.}\quad
$\MP$ governs ``not-not-decayed implies decayed.''  Independent of the main spine.
\begin{quote}\itshape
Markov's Principle is independent of both $\LPO$ and $\FT$, confirming the hierarchy has genuine branching structure.
\end{quote}

\paragraph{Paper 11: CHSH and Tsirelson Bound}
\emph{DOI: \texttt{10.5281/zenodo.18527676}.}\quad
Bell inequality violations and the Tsirelson bound are $\BISH$.
\begin{quote}\itshape
The experimental verification of quantum nonlocality is fully constructive.  The non-constructive cost ($\LLPO$) enters only in the theorem about why local hidden variables fail, not in the computation of what experiments measure.
\end{quote}

% ------------------------------------------------------------------
\subsection{Confirmatory and Technical {\normalfont(\OneStar)}}

\paragraph{Paper 4: Quantum Spectra Axiom Calibration}
\emph{DOI: \texttt{10.5281/zenodo.17059483}.}\quad
Early calibration confirming $\BISH$ for finite-dimensional quantum mechanics.

\paragraph{Paper 5: Schwarzschild Curvature Verification}
\emph{DOI: \texttt{10.5281/zenodo.18489703}.}\quad
Curvature computation is $\BISH$.  Feeds into Paper~13.

\paragraph{Paper 6: Heisenberg Uncertainty (v2)}
\emph{DOI: \texttt{10.5281/zenodo.18519836}.}\quad
Uncertainty principle is $\BISH$.

\paragraph{Paper 7: Physical Bidual Gap---Trace-Class Operators}
\emph{DOI: \texttt{10.5281/zenodo.18527559}.}\quad
Physical instantiation of Paper~2's abstract result.

\paragraph{Paper 16: Technical Note---Born Rule}
\emph{DOI: \texttt{10.5281/zenodo.18575377}.}\quad
Born probabilities are $\BISH$; frequentist convergence is $\DC$.  Despite its modest framing, this paper establishes the foundationally important separation between empirical content and interpretive superstructure that underpins Finding~11.

\paragraph{Paper 38: Wang Tiling and the Origin of Physical Undecidability}
\emph{DOI: \texttt{10.5281/zenodo.18642804}.}\quad
Tiling undecidability reduces to $\LPO$.  Wang tiling is the combinatorial foundation on which many physical undecidability constructions rest; showing it calibrates at $\LPO$ confirms that physical undecidability is bounded at its source.

% ------------------------------------------------------------------
\subsection{Withdrawn}

Papers~1 and~3 have been withdrawn.

\bigskip\noindent
\textbf{The arc.}\quad
Build the instrument (Papers~2--39).  Defend it (Paper~40).  Deploy it on a duality (Paper~41).  Dissolve a famous problem (Paper~42).

% ======================================================================
%  APPENDIX D: TERMINOLOGY CONCORDANCE
% ======================================================================
\chapter{Terminology Concordance}
\label{app:terminology}

The author is a medical professional, not a specialist in constructive analysis or reverse mathematics.  This program was developed as an outsider's investigation into the logical structure of mathematical physics.  In several cases, the author coined terminology independently before discovering the standard vocabulary---effectively reinventing the wheel, and sometimes giving it a different name.  This concordance maps the program's non-standard terms to their established equivalents, both as a service to readers coming from constructive analysis and as an honest acknowledgment that better names already existed in many cases.  Where the program's term has been retained despite a standard alternative, the reason is noted.

% ------------------------------------------------------------------
\section{Core Terminology}
% ------------------------------------------------------------------

\paragraph{Bidual gap.}
\emph{Standard term:} Non-reflexivity of Banach spaces; failure of the canonical embedding $X\hookrightarrow X^{**}$ to be surjective.\quad
``Bidual gap'' was coined before the author encountered the standard terminology.  ``Non-reflexivity'' is a negation describing what fails; ``bidual gap'' names the positive geometric phenomenon---a measurable distance between $X$ and $X^{**}$.  The non-standard term has been retained in titles for accessibility, but all papers using it include ``non-reflexivity'' in the abstract and keywords to ensure discoverability.

\paragraph{Scaffolding.}
\emph{Standard term:} Dispensable / eliminable (proof-theoretic contexts); artefact (physics contexts).\quad
A mathematical principle is \emph{scaffolding} for a physical prediction if the prediction can be derived without that principle---i.e., the principle is eliminable.  The metaphor is architectural: the structure is used during construction but is not part of the finished building.

\paragraph{Axiom cost.}
\emph{Standard term:} Proof-theoretic strength; reverse-mathematical classification; calibration level.\quad
``Proof-theoretic strength'' suggests a total ordering (stronger/weaker), which is misleading for independent principles such as $\FT$ and $\LPO$.  ``Axiom cost'' conveys the economic metaphor---each theorem has a price in logical resources---which aligns with the program's cost-benefit framing of mathematical idealisation.

\paragraph{Axiom calibration.}
\emph{Standard term:} Reverse-mathematical classification; proof-theoretic calibration.\quad
The physical analogy is deliberate: one calibrates an instrument against known standards before deploying it on unknown targets.  The term ``calibration'' is already used informally in the constructive reverse mathematics literature.

\paragraph{Logical geography.}
\emph{Standard term:} Reverse-mathematical classification; proof-theoretic landscape.\quad
Used in Paper~10's title to signal that the paper is a survey and atlas---a map of the logical terrain of mathematical physics---rather than a proof of a single theorem.

\paragraph{Omniscience spine.}
\emph{Standard term:} Omniscience hierarchy; omniscience principles.\quad
``Hierarchy'' suggests a total order, which is correct for the linear chain $\LLPO < \WLPO < \LPO$ but misleading when $\FT$ and $\DC$ are included (both are independent of the chain).  ``Spine'' suggests the central linear chain with branches---more accurate for the full picture.

\paragraph{Bridge axiom.}
\emph{Standard term:} No exact equivalent.  Closest: physical axiom, empirical hypothesis, domain axiom.\quad
A bridge axiom encodes a single, well-established physical fact as a typed axiom in \Lean.  The \Lean\ type-checker verifies that theorems follow \emph{from} bridge axioms; it does not verify the bridge axioms themselves.  Bridge axioms are the program's interface between formal mathematics and empirical physics.

\paragraph{The diagnostic.}
\emph{Standard term:} No equivalent.\quad
Refers to the complete methodology: formalise a physical result in \Lean, identify bridge axioms encoding the physics, determine the minimal constructive principle beyond $\BISH$ required for the proof, and record the result in the calibration table.

% ------------------------------------------------------------------
\section{Constructive Analysis Terminology}
% ------------------------------------------------------------------

\paragraph{BISH, LPO, WLPO, LLPO.}
Standard throughout.  No discrepancy with the literature.  Bishop's constructive mathematics ($\BISH$) and the Limited Principle of Omniscience and its variants are used in their established senses~\cite{Bishop1967,BridgesRichman1987,Ishihara2006}.

\paragraph{Fan Theorem (FT).}
$\FT$ is sometimes formulated as the extreme value theorem (EVT) for continuous functions on $[0,1]$, which is equivalent to the standard Fan Theorem.  The equivalence $\FT \leftrightarrow \mathrm{EVT}$ should be cited explicitly whenever the EVT formulation is used, to avoid ambiguity about which principle is intended.

\paragraph{Bounded Monotone Convergence (BMC).}
BMC asserts that every bounded monotone sequence of reals converges, without requiring a computable modulus of convergence.  The equivalence $\mathrm{BMC} \leftrightarrow \LPO$ under this formulation is established by Ishihara~\cite{Ishihara2006} and formalised in Paper~29 via Fekete's subadditive lemma.

\paragraph{Dependent Choice (DC).}
Standard.  The program distinguishes $\DC$ (countable dependent choice) from $\DC_\omega$ (dependent choice for natural-number-indexed sequences).

\paragraph{Markov's Principle (MP).}
Standard.  ``If a computation cannot fail to halt, then it halts.''

% ------------------------------------------------------------------
\section{Formalisation Terminology}
% ------------------------------------------------------------------

\paragraph{Bridge axiom assembly.}
A theorem that combines bridge axioms with genuine proofs.  Distinguished from ``genuine proof'' (uses no bridge axioms) in the CRM audit tables accompanying each paper.

\paragraph{CRM audit.}
The table in each paper classifying every theorem as ``genuine proof,'' ``bridge axiom assembly,'' or ``intentional classical.''  Introduced in Paper~10's methodology section~\cite{Lee2026P10}.  This classification is a methodological contribution of the program.

\paragraph{Zero-sorry build.}
\emph{Standard term:} ``Complete formalisation'' or ``fully verified'' (in the \Lean\ community).\quad
The \Lean\ build compiles with no \texttt{sorry}---\Lean's marker for unproved assertions---confirming that every stated theorem follows logically from the declared axioms.

% ------------------------------------------------------------------
\section{Terms with No Non-Standard Usage}
% ------------------------------------------------------------------

The following terms are used in their standard senses throughout the program and require no concordance entry: thermodynamic limit, perturbative/non-perturbative, Page time, Casimir energy, entanglement entropy, holographic dictionary, Ryu--Takayanagi formula, QES (quantum extremal surface), Fekete's subadditive lemma, Picard--Lindel\"of theorem, Weihrauch degree.

% ======================================================================
%  ACKNOWLEDGMENTS
% ======================================================================
\chapter*{Acknowledgments}
\addcontentsline{toc}{chapter}{Acknowledgments}

This formalization was developed using Claude (Anthropic) as a
collaborative tool for \Lean{}~4 code generation, proof strategy
exploration, and \LaTeX{} document preparation.  All mathematical
content was specified by the author.  Every theorem was verified
by the \Lean{}~4 type checker.

The formal verification was performed using \Lean\ and the \Mathlib\ mathematical library. The author is grateful to the \Lean\ and \Mathlib\ communities for creating and maintaining the infrastructure that made this program possible. The program benefited from the open-access preprint infrastructure provided by Zenodo/CERN for archival and DOI assignment.

\medskip\noindent
\textbf{Preliminary status and author background.}
The results presented in this monograph are preliminary.  The author is a medical
professional, not a domain expert in physics or mathematics.  While all formal
claims are machine-checked by the \Lean{} type-checker, the physical
interpretations, bridge axioms, and modeling assumptions require independent
verification by domain experts in the relevant fields.  Until such verification
is completed, this work should be considered preliminary.

\medskip\noindent
Whatever findings of value emerge from this program belong to the
constructive reverse mathematics community and to the legacy of Errett Bishop,
whose perseverance in developing constructive analysis inspired this entire
series.  Any errors are solely the author's.

% ======================================================================
%  AUTHOR NOTE
% ======================================================================
\chapter*{Author Note}
\addcontentsline{toc}{chapter}{Author Note}

Paul Chun-Kit Lee is an attending cardiologist at NYU Langone Hospital---Brooklyn (New York University, New York). His previous work has focused on artificial intelligence and cardiology. This work was conducted independently and is not affiliated with any academic mathematics or physics department. The author has no formal training in mathematical logic or proof theory. The formal verification in \Lean, which certifies every claim mechanically, was adopted precisely because the author's non-standard background demands a standard of evidence that informal argument cannot provide.

% ======================================================================
%  BIBLIOGRAPHY
% ======================================================================
\begin{thebibliography}{99}

\bibitem{Brouwer1907} Brouwer, L.E.J. (1907). \emph{Over de grondslagen der wiskunde.} Dissertation, University of Amsterdam.

\bibitem{vanDalen2005} van Dalen, D. (2005). \emph{Mystic, Geometer, and Intuitionist: The Life of L.E.J.\ Brouwer}, Vol.~2: \emph{Hope and Disillusion.} Oxford University Press.

\bibitem{Bishop1967} Bishop, E. (1967). \emph{Foundations of Constructive Analysis.} McGraw-Hill.

\bibitem{BishopBridges1985} Bishop, E. and Bridges, D. (1985). \emph{Constructive Analysis.} Springer.

\bibitem{BridgesRichman1987} Bridges, D. and Richman, F. (1987). \emph{Varieties of Constructive Mathematics.} Cambridge University Press.

\bibitem{BridgesVita2006} Bridges, D. and V\^{\i}\c{t}\u{a}, L. (2006). \emph{Techniques of Constructive Analysis.} Springer.

\bibitem{Ishihara2006} Ishihara, H. (2006). Reverse mathematics in Bishop's constructive mathematics. \emph{Philosophia Scientiae}, CS 6, 43--59.

\bibitem{Bauer2017} Bauer, A. (2017). Five stages of accepting constructive mathematics. \emph{Bulletin of the American Mathematical Society}, 54(3), 481--498.

\bibitem{Weihrauch2000} Weihrauch, K. (2000). \emph{Computable Analysis: An Introduction.} Springer.

\bibitem{PourElRichards1989} Pour-El, M.B. and Richards, J.I. (1989). \emph{Computability in Analysis and Physics.} Springer.

\bibitem{Simpson2009} Simpson, S.G. (2009). \emph{Subsystems of Second Order Arithmetic} (2nd ed.). Cambridge University Press.

\bibitem{deMoura2021} de Moura, L. and Ullrich, S. (2021). The Lean 4 theorem prover and programming language. In \emph{CADE-28}, LNCS 12699, Springer.

\bibitem{Fekete1923} Fekete, M. (1923). \"Uber die Verteilung der Wurzeln bei gewissen algebraischen Gleichungen. \emph{Mathematische Zeitschrift}, 17, 228--249.

\bibitem{Lee2026P10} Lee, P.C.K. (2026). The Logical Geography of Mathematical Physics: Constructive Calibration from Density Matrices to the Thermodynamic Limit. Paper~10 of the Constructive Reverse Mathematics Program. Zenodo.

\bibitem{Lee2026P12} Lee, P.C.K. (2026). The Map and the Territory: A Constructive History of Mathematical Physics. Paper~12 of the Constructive Reverse Mathematics Program. Zenodo.

\bibitem{Berger1966} Berger, R. (1966). The undecidability of the domino problem. \emph{Memoirs of the AMS}, 66.

\bibitem{Cubitt2015} Cubitt, T.S., Perez-Garcia, D., and Wolf, M.M. (2015). Undecidability of the spectral gap. \emph{Nature}, 528, 207--211.

\bibitem{RyuTakayanagi2006} Ryu, S. and Takayanagi, T. (2006). Holographic derivation of entanglement entropy from the anti--de Sitter space/conformal field theory correspondence. \emph{Physical Review Letters}, 96, 181602.

\bibitem{Maldacena1999} Maldacena, J. (1999). The large-$N$ limit of superconformal field theories and supergravity. \emph{International Journal of Theoretical Physics}, 38, 1113--1133.

\bibitem{HollandsWald2005} Hollands, S. and Wald, R.M. (2005). Conservation of the stress tensor in perturbative interacting quantum field theory in curved spacetimes. \emph{Reviews in Mathematical Physics}, 17, 227--311.

\bibitem{Weinberg1989} Weinberg, S. (1989). The cosmological constant problem. \emph{Reviews of Modern Physics}, 61, 1--23.

% --- Physics references cited in text ---

\bibitem{Bell1964} Bell, J.S. (1964). On the Einstein Podolsky Rosen paradox. \emph{Physics Physique Fizika}, 1(3), 195--200.

\bibitem{Clauser1969} Clauser, J.F., Horne, M.A., Shimony, A., and Holt, R.A. (1969). Proposed experiment to test local hidden-variable theories. \emph{Physical Review Letters}, 23, 880--884.

\bibitem{Bekenstein1973} Bekenstein, J.D. (1973). Black holes and entropy. \emph{Physical Review D}, 7, 2333--2346.

\bibitem{Hawking1975} Hawking, S.W. (1975). Particle creation by black holes. \emph{Communications in Mathematical Physics}, 43, 199--220.

\bibitem{CalabreseCardy2004} Calabrese, P. and Cardy, J. (2004). Entanglement entropy and quantum field theory. \emph{Journal of Statistical Mechanics}, P06002.

\bibitem{FLM2013} Faulkner, T., Lewkowycz, A., and Maldacena, J. (2013). Quantum corrections to holographic entanglement entropy. \emph{Journal of High Energy Physics}, 2013(11), 074.

\bibitem{EngelhardtWall2015} Engelhardt, N. and Wall, A.C. (2015). Quantum extremal surfaces: holographic entanglement entropy beyond the classical regime. \emph{Journal of High Energy Physics}, 2015(1), 073.

\bibitem{Robinson1971} Robinson, R.M. (1971). Undecidability and nonperiodicity for tilings of the plane. \emph{Inventiones Mathematicae}, 12, 177--209.

\bibitem{BauschCubittLucia2020} Bausch, J., Cubitt, T.S., Lucia, A., and Perez-Garcia, D. (2020). Undecidability of the spectral gap in one dimension. \emph{Physical Review X}, 10, 031038.

\bibitem{BauschCubittWatson2021} Bausch, J., Cubitt, T.S., and Watson, J.D. (2021). Uncomputability of phase diagrams. \emph{Nature Communications}, 12, 452.

\bibitem{CubittLucia2022} Cubitt, T.S., Lucia, A., Perez-Garcia, D., and Perez-Eceiza, P. (2022). Undecidable problems in quantum field theory. Preprint.

\bibitem{Wang1961} Wang, H. (1961). Proving theorems by pattern recognition---II. \emph{Bell System Technical Journal}, 40(1), 1--41.

\bibitem{Camporesi1990} Camporesi, R. (1990). Harmonic analysis and propagators on homogeneous spaces. \emph{Physics Reports}, 196(1--2), 1--134.

\bibitem{DESI2024} DESI Collaboration (2024). DESI 2024 VI: Cosmological constraints from the measurements of baryon acoustic oscillations. arXiv:2404.03002.

\bibitem{Hossenfelder2019} Hossenfelder, S. (2019). Screams for explanation: finetuning and naturalness in the foundations of physics. \emph{Synthese}, 198, 3727--3745.

\bibitem{Nobbenhuis2006} Nobbenhuis, S. (2006). Categorizing different approaches to the cosmological constant problem. \emph{Foundations of Physics}, 36, 613--680.

\bibitem{BoussoPolchinski2000} Bousso, R. and Polchinski, J. (2000). Quantization of four-form fluxes and dynamical neutralization of the cosmological constant. \emph{Journal of High Energy Physics}, 2000(6), 006.

\bibitem{Sola2013} Sol\`a, J. (2013). Cosmological constant and vacuum energy: old and new ideas. \emph{Journal of Physics: Conference Series}, 453, 012015.

\end{thebibliography}

\end{document}

